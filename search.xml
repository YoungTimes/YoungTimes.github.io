<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度神经网络识别交通标牌</title>
    <url>/2021/03/15/cnn-sign-detection/</url>
    <content><![CDATA[<p>这里我们实现一个入门级的CNN交通标牌分类网络。</p>
<p><img src="/2021/03/15/cnn-sign-detection/dataset_input_output.png"></p>
<span id="more"></span>

<p>首先导入基础依赖库。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>

<h1 id="数据集-Dataset"><a href="#数据集-Dataset" class="headerlink" title="数据集(Dataset)"></a>数据集(Dataset)</h1><p>数据集(Dataset)中包含43个不同分类的、大小为32x32的RGB图像，分类如下:</p>
<ul>
<li>0 = Speed limit (20km/h)</li>
<li>1 = Speed limit (30km/h)</li>
<li>2 = Speed limit (50km/h)</li>
<li>3 = Speed limit (60km/h)</li>
<li>4 = Speed limit (70km/h)</li>
<li>5 = Speed limit (80km/h)</li>
<li>6 = End of speed limit (80km/h)</li>
<li>7 = Speed limit (100km/h)</li>
<li>8 = Speed limit (120km/h)</li>
<li>9 = No passing</li>
<li>10 = No passing for vehicles over 3.5 metric tons</li>
<li>11 = Right-of-way at the next intersection</li>
<li>12 = Priority road</li>
<li>13 = Yield</li>
<li>14 = Stop</li>
<li>15 = No vehicles</li>
<li>16 = Vehicles over 3.5 metric tons prohibited</li>
<li>17 = No entry</li>
<li>18 = General caution</li>
<li>19 = Dangerous curve to the left</li>
<li>20 = Dangerous curve to the right</li>
<li>21 = Double curve</li>
<li>22 = Bumpy road</li>
<li>23 = Slippery road</li>
<li>24 = Road narrows on the right</li>
<li>25 = Road work</li>
<li>26 = Traffic signals</li>
<li>27 = Pedestrians</li>
<li>28 = Children crossing</li>
<li>29 = Bicycles crossing</li>
<li>30 = Beware of ice/snow</li>
<li>31 = Wild animals crossing</li>
<li>32 = End of all speed and passing limits</li>
<li>33 = Turn right ahead</li>
<li>34 = Turn left ahead</li>
<li>35 = Ahead only</li>
<li>36 = Go straight or right</li>
<li>37 = Go straight or left</li>
<li>38 = Keep right</li>
<li>39 = Keep left</li>
<li>40 = Roundabout mandatory</li>
<li>41 = End of no passing</li>
<li>42 = End of no passing by vehicles over 3.5 metric tons</li>
</ul>
<h2 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h2><p>读取训练集、验证集和测试集:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/train.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> training_data:</span><br><span class="line">    train = pickle.load(training_data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/valid.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> validation_data:</span><br><span class="line">    valid = pickle.load(validation_data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/test.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> testing_data:</span><br><span class="line">    test = pickle.load(testing_data)</span><br></pre></td></tr></table></figure>

<p>查看训练集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train, y_train = train[<span class="string">&quot;features&quot;</span>], train[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>

<p>(34799, 32, 32, 3)</p>
<p>查看验证集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_validation, y_validation = valid[<span class="string">&quot;features&quot;</span>], valid[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_validation.shape)</span><br></pre></td></tr></table></figure>

<p>(4410, 32, 32, 3)</p>
<p>查看测试集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test, y_test = test[<span class="string">&quot;features&quot;</span>], test[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_test.shape)</span><br></pre></td></tr></table></figure>

<p>(12630, 32, 32, 3)</p>
<h2 id="数据集可视化"><a href="#数据集可视化" class="headerlink" title="数据集可视化"></a>数据集可视化</h2><p>随机选取一张图片：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = np.random.randint(<span class="number">1</span>, <span class="built_in">len</span>(X_train))</span><br><span class="line"></span><br><span class="line">plt.imshow(X_train[i])</span><br><span class="line"></span><br><span class="line">y_train[i]</span><br></pre></td></tr></table></figure>

<p>图片展示效果如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/traffic_sign.png"></p>
<p>多看一些数据集的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_grid = <span class="number">5</span></span><br><span class="line">L_grid = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(L_grid, W_grid, figsize = (<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">axes = axes.ravel() <span class="comment"># flaten the 5 x 5 matrix into 25 array</span></span><br><span class="line"></span><br><span class="line">n_training = <span class="built_in">len</span>(X_train) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>, W_grid * L_grid):</span><br><span class="line">    <span class="comment"># Select a random number</span></span><br><span class="line">    index = np.random.randint(<span class="number">0</span>, n_training)</span><br><span class="line">    <span class="comment"># read and display an image with the selected index    </span></span><br><span class="line">    axes[i].imshow(X_train[index])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>图片展示效果如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/traffic_signs.png"></p>
<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>使用之前，先对数据进行一些预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line">X_train, y_train = shuffle(X_train, y_train)</span><br></pre></td></tr></table></figure>

<h2 id="灰度化"><a href="#灰度化" class="headerlink" title="灰度化"></a>灰度化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_gray = np.<span class="built_in">sum</span>(X_train / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_validation_gray = np.<span class="built_in">sum</span>(X_validation / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_test_gray = np.<span class="built_in">sum</span>(X_test / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(X_train_gray.shape)</span><br></pre></td></tr></table></figure>

<p>(34799, 32, 32, 1)</p>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>将所有图像数据归一化到[-1, 1]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_gray_norm = (X_train_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X_validation_gray_norm = (X_validation_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X_test_gray_norm = (X_test_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">print(X_train_gray_norm)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[[-0.52083333]</span><br><span class="line">   [-0.52604167]</span><br><span class="line">   [-0.51822917]</span><br><span class="line">   ...</span><br><span class="line">   [-0.48958333]</span><br><span class="line">   [-0.47916667]</span><br><span class="line">   [-0.46614583]]</span><br><span class="line"></span><br><span class="line">  [[-0.52083333]</span><br><span class="line">   [-0.52083333]</span><br><span class="line">   [-0.52864583]</span><br><span class="line">   ...</span><br><span class="line">   [-0.5       ]</span><br><span class="line">   [-0.48958333]</span><br><span class="line">   [-0.4765625 ]]</span><br><span class="line"></span><br><span class="line">  [[-0.54427083]</span><br><span class="line">   [-0.53385417]</span><br><span class="line">   [-0.53385417]</span><br><span class="line">   ...</span><br><span class="line">   [-0.50520833]</span><br><span class="line">   [-0.47916667]</span><br><span class="line">   [-0.47395833]]</span><br><span class="line"></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = random.randint(<span class="number">1</span>, <span class="built_in">len</span>(X_train_gray))</span><br><span class="line">plt.imshow(X_train_gray[i].squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(X_train[i])</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(X_train_gray_norm[i].squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>  灰度化和Normalization的效果如下，从上到下依次为：灰度图像，原图像、归一化的图像。</p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_origin.png" alt="灰度图"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_gray.png" alt="原图"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_normal.png" alt="标准化"></p>
<h1 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h1><p>  <img src="/2021/03/15/cnn-sign-detection/cnn.png"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/dropout.png"></p>
<h2 id="构建深度神经网络"><a href="#构建深度神经网络" class="headerlink" title="构建深度神经网络"></a>构建深度神经网络</h2><p>使用Keras构建CNN网络模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"></span><br><span class="line">CNN = models.Sequential()</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Conv2D(<span class="number">6</span>, (<span class="number">5</span>, <span class="number">5</span>), activation = <span class="string">&#x27;relu&#x27;</span>, input_shape = (<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)))</span><br><span class="line">CNN.add(layers.AveragePooling2D())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Conv2D(<span class="number">16</span>, (<span class="number">5</span>, <span class="number">5</span>), activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.AveragePooling2D())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Dropout(<span class="number">0.2</span>))</span><br><span class="line">CNN.add(layers.Flatten())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Dense(<span class="number">120</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.Dense(<span class="number">84</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.Dense(<span class="number">43</span>, activation = <span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">CNN.summary()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Model: &quot;sequential_1&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">average_pooling2d_1 (Average (None, 14, 14, 6)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)            (None, 10, 10, 16)        2416      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">average_pooling2d_2 (Average (None, 5, 5, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout (Dropout)            (None, 5, 5, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 400)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense)              (None, 120)               48120     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_4 (Dense)              (None, 84)                10164     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_5 (Dense)              (None, 43)                3655      </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 64,511</span><br><span class="line">Trainable params: 64,511</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>

<h2 id="编译和训练"><a href="#编译和训练" class="headerlink" title="编译和训练"></a>编译和训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CNN.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;Adam&#x27;</span>, loss = <span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">history = CNN.fit(X_train_gray_norm,</span><br><span class="line">                  y_train,</span><br><span class="line">                  batch_size = <span class="number">500</span>,</span><br><span class="line">                  epochs = <span class="number">50</span>,</span><br><span class="line">                  verbose = <span class="number">1</span>,</span><br><span class="line">                  validation_data = (X_validation_gray_norm, y_validation))</span><br></pre></td></tr></table></figure>

<p>进行50个Epoch的训练，训练集Accuracy达到98.64%，验证集的Accuracy达到91.61%。</p>
<p>训练过程如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 136ms&#x2F;step - loss: 3.1861 - accuracy: 0.1649 - val_loss: 2.5817 - val_accuracy: 0.3082</span><br><span class="line">Epoch 2&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 135ms&#x2F;step - loss: 1.6409 - accuracy: 0.5335 - val_loss: 1.2052 - val_accuracy: 0.6458</span><br><span class="line">Epoch 3&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 145ms&#x2F;step - loss: 0.9414 - accuracy: 0.7220 - val_loss: 0.8685 - val_accuracy: 0.7417</span><br><span class="line">Epoch 4&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 147ms&#x2F;step - loss: 0.7074 - accuracy: 0.7915 - val_loss: 0.7434 - val_accuracy: 0.7834</span><br><span class="line">Epoch 5&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 145ms&#x2F;step - loss: 0.5825 - accuracy: 0.8317 - val_loss: 0.6605 - val_accuracy: 0.7955</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Epoch 45&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 115ms&#x2F;step - loss: 0.0509 - accuracy: 0.9844 - val_loss: 0.3022 - val_accuracy: 0.9220</span><br><span class="line">Epoch 46&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 114ms&#x2F;step - loss: 0.0480 - accuracy: 0.9861 - val_loss: 0.2822 - val_accuracy: 0.9254</span><br><span class="line">Epoch 47&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 110ms&#x2F;step - loss: 0.0462 - accuracy: 0.9864 - val_loss: 0.2971 - val_accuracy: 0.9259</span><br><span class="line">Epoch 48&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 111ms&#x2F;step - loss: 0.0460 - accuracy: 0.9860 - val_loss: 0.2665 - val_accuracy: 0.9268</span><br><span class="line">Epoch 49&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 118ms&#x2F;step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.3237 - val_accuracy: 0.9218</span><br><span class="line">Epoch 50&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 112ms&#x2F;step - loss: 0.0452 - accuracy: 0.9864 - val_loss: 0.3150 - val_accuracy: 0.9161</span><br></pre></td></tr></table></figure>

<h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CNN.save(<span class="string">&#x27;traffic_sign_weights.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<h2 id="模型效果评估"><a href="#模型效果评估" class="headerlink" title="模型效果评估"></a>模型效果评估</h2><p><img src="/2021/03/15/cnn-sign-detection/confusion_mask.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">score = CNN.evaluate(X_test_gray_norm, y_test)</span><br><span class="line">print(<span class="string">&#x27;Test Accuracy: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(score[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<p>395/395 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.9123<br>Test Accuracy: 0.9122723937034607</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history.history.keys()</span><br></pre></td></tr></table></figure>

<p>dict_keys([‘loss’, ‘accuracy’, ‘val_loss’, ‘val_accuracy’])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracy = history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">val_accuracy = history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>对训练过程中的Loss进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(accuracy))</span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;ro&#x27;</span>, label = <span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;r&#x27;</span>, label = <span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training, And Validation Loss&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/loss.png"></p>
<p>对训练过程中的Accuracy进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(accuracy))</span><br><span class="line">plt.plot(epochs, accuracy, <span class="string">&#x27;ro&#x27;</span>, label = <span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_accuracy, <span class="string">&#x27;r&#x27;</span>, label = <span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training, And Validation Accuracy&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/accuracy.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predicted_classes = CNN.predict_classes(X_test_gray_norm)</span><br><span class="line">y_true = y_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(y_true, predicted_classes)</span><br><span class="line">plt.figure(figsize = (<span class="number">25</span>, <span class="number">25</span>))</span><br><span class="line">sns.heatmap(cm, annot = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/matrix.png"></p>
<p>在测试集上验证网络效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L = <span class="number">5</span></span><br><span class="line">W = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(L, W, figsize = (<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">axes = axes.ravel()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>, L*W):</span><br><span class="line">    axes[i].imshow(X_test[i])</span><br><span class="line">    axes[i].set_title(<span class="string">&#x27;Prediction = &#123;&#125;\n True = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(predicted_classes[i], y_true[i]))</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplots_adjust(wspace = <span class="number">1</span>) </span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/test_result.png"></p>
<h1 id="实际检测效果验证"><a href="#实际检测效果验证" class="headerlink" title="实际检测效果验证"></a>实际检测效果验证</h1><p>在网上找了两张图片(限速标牌和Stop标牌)试验， 图像中冗余内容越多，检测效果越差。当标牌充满图像时，检测效果还是不错的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> img_to_array</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_to_array</span>(<span class="params">path</span>):</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(path)</span><br><span class="line">    image = image.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    image = img_to_array(image)</span><br><span class="line"></span><br><span class="line">    image = image.reshape([<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prediction</span>(<span class="params">path</span>):</span></span><br><span class="line"></span><br><span class="line">    img = image_to_array(path)</span><br><span class="line"></span><br><span class="line">    plt.imshow(img.squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    img_gray = np.<span class="built_in">sum</span>(img / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    img_norm = (img_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">    print(img_norm.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(img_norm.squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    detection_model = load_model(<span class="string">&#x27;traffic_sign_weights.h5&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    predicted_classes = detection_model.predict_classes(img_norm)</span><br><span class="line"></span><br><span class="line">    print(predicted_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">prediction(<span class="string">&quot;./stop.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">prediction(<span class="string">&quot;./speed_limit_60.jpg&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输入图像如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/stop.jpg"></p>
<p>检测输出内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">14</span>]</span><br></pre></td></tr></table></figure>
<p>14对应Stop Sign的类型。</p>
<p><img src="/2021/03/15/cnn-sign-detection/speed_limit_60.jpg"></p>
<p>检测输出内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<p>3对应60KM/h的限速。</p>
<h1 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h1><p>Coursera - Traffic Sign Classification Using Deep Learning in Python/Keras</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>环境感知</tag>
        <tag>自动驾驶</tag>
      </tags>
  </entry>
  <entry>
    <title>低速自动驾驶车辆的定位与建图</title>
    <url>/2021/03/25/localization-mapping-zhixingzhe/</url>
    <content><![CDATA[<p><img src="/2021/03/25/localization-mapping-zhixingzhe/1.png"></p>
<p>本文是高翔博士关于低速自动驾驶定位建图的相关介绍。</p>
<span id="more"></span>

<p><img src="/2021/03/25/localization-mapping-zhixingzhe/2.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/3.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/4.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/5.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/6.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/7.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/8.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/9.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/10.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/11.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/12.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/13.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/14.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/15.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/16.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/17.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/18.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/19.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/20.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/21.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/22.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/23.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/24.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/25.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/26.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/27.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/28.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/29.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/30.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/31.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/32.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/33.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/34.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/35.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/36.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/37.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/38.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/39.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/40.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/41.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/42.png"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>高精地图</tag>
        <tag>Mobileye</tag>
      </tags>
  </entry>
  <entry>
    <title>Mobileye REM地图</title>
    <url>/2021/03/14/mobileye-rem-map-md/</url>
    <content><![CDATA[<h1 id="为什么需要高精地图"><a href="#为什么需要高精地图" class="headerlink" title="为什么需要高精地图"></a>为什么需要高精地图</h1><p><img src="/2021/03/14/mobileye-rem-map-md/rem_map.png" alt="Mobileye Rem Map"></p>
<p>理论上来讲，可以在车载系统检测和获取所有道路信息(可行驶路径、车道优先级、红绿灯与车道的关联关系、车道与人行横道与红绿灯的关系等)，但是目前的AI能力无法保证实现很高的MTBF(Mean Time Between Failures, 平均无故障时间)，所以需要提前把这些信息都准备好。</p>
<span id="more"></span>

<p><img src="/2021/03/14/mobileye-rem-map-md/hdmap_motivation.png" alt="Motivation Behind HDMap"></p>
<h1 id="高精地图的挑战"><a href="#高精地图的挑战" class="headerlink" title="高精地图的挑战"></a>高精地图的挑战</h1><h2 id="规模化-Scale"><a href="#规模化-Scale" class="headerlink" title="规模化-Scale"></a>规模化-Scale</h2><p>如果自动驾驶车辆只在一个区域、一个城市、或者几个城市运营，那就不存在规模化的问题。但是2025年之后，自动驾驶会在消费者层面全面落地，用户需要驾车到任意想去的地方，在这种场景下，Scale是一个无法规避的问题。</p>
<h2 id="鲜度-Fresh"><a href="#鲜度-Fresh" class="headerlink" title="鲜度-Fresh"></a>鲜度-Fresh</h2><p>理想情况下，地图是在实时更新的。当物理环境发生变化时，需要实时反映到地图上。月级更新、甚至天级更新都是不够的，我们需要做到分钟级，甚至更短。</p>
<h2 id="精度-Accuracy"><a href="#精度-Accuracy" class="headerlink" title="精度-Accuracy"></a>精度-Accuracy</h2><p>车载系统(OnBoard System)检测的车辆和行人需要与高精地图(High Definiation Map)实现厘米级精度的匹配，因此地图的精度至关重要。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_challange.png" alt="高精地图挑战"></p>
<h1 id="通用高精地图制作方法的缺陷"><a href="#通用高精地图制作方法的缺陷" class="headerlink" title="通用高精地图制作方法的缺陷"></a>通用高精地图制作方法的缺陷</h1><p><img src="/2021/03/14/mobileye-rem-map-md/common_approach_map.png" alt="高精地图通用制作方法"></p>
<h2 id="全局坐标系下厘米级精度不是必需的"><a href="#全局坐标系下厘米级精度不是必需的" class="headerlink" title="全局坐标系下厘米级精度不是必需的"></a>全局坐标系下厘米级精度不是必需的</h2><p>AV车辆行驶过程中只关注周围几百米范围即可，所以只要这个范围内的足够准确即可。至于几公里之外的全局精度，Who Care…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_geometric.png"></p>
<h2 id="语义层数据生产难以自动化"><a href="#语义层数据生产难以自动化" class="headerlink" title="语义层数据生产难以自动化"></a>语义层数据生产难以自动化</h2><p><img src="/2021/03/14/mobileye-rem-map-md/semantic_map.png"></p>
<p>如下图所示，没有车道线的双向车道，单从图像观察，难以识别它的Drive Path。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/drivable_path.png"></p>
<p>如下图所示，转向规则千奇百怪：禁止红灯右转，完全停车后允许红灯右转，绿灯禁止左转，绿灯Yield后允许左转…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/priority_map.png"></p>
<p>如下图所示，红绿灯异常复杂，识别车道、人行横道与红绿灯的关联关系难度很大…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_association.png"></p>
<p>如下图所示，除非地图可以表达所有的3D要素，否则很难自动化的计算出车道的最优Stop/Yield Point。但是表达所有的3D信息对于地图来说又是不现实的…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/stop_point.png"></p>
<p>影响车辆行驶速度的因素有很多，道路几何、限速、文化等，难以量化，但它对Smooth Driving体验至关重要…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/smooth_driving.png"></p>
<h1 id="Mobileye如何解决这些问题"><a href="#Mobileye如何解决这些问题" class="headerlink" title="Mobileye如何解决这些问题"></a>Mobileye如何解决这些问题</h1><p>scalability依赖众包数据生成Millions Map Agents；Accuracy不是全局的Accuracy，而是局部的Accuracy，相对于道路上的静态元素位置。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/av_map.png"></p>
<p>REM的处理流程如下，首先从成百上千辆车获取检测信息(没有使用差分GPS，而是使用了普通的GPS)，这些数据传送到云端；每辆车Detection的角度不同，由于遮挡等原因，每辆车检测的landmark也有差异，将这些数据进行Alignment处理，生成高精度的地图数据；最后，Modeling And Semantics负责生成地图的语义数据。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/rem_process.png"></p>
<h2 id="Harvesting"><a href="#Harvesting" class="headerlink" title="Harvesting"></a>Harvesting</h2><p>下图中黄色的框是车辆检测的landmarks和lane marks，同时车辆会尝试检测driving path等语义信息，一辆车可能检测不准确，但是成百上千的过路车辆会让检测结果越来越好。</p>
<p>Mobileye Harvesting的数据量为10K/公里，这些检测的数据会被发送到云端。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/harvesting.png"></p>
<h2 id="Aligning-Drives"><a href="#Aligning-Drives" class="headerlink" title="Aligning Drives"></a>Aligning Drives</h2><p>检测每个RSD中每个元素的6D Pose，然后对齐相同位置的元素，得到厘米度精度的driving path等信息。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_align_drive.png"></p>
<p>由于GPS存在误差，每个车辆检测的道路元素位置都存在噪声，所以只依靠简单的位置求均值是不可行的。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/align_noise.png"></p>
<p>Align之后可以明显的看到两条Driving Path(蓝色)和两侧的道路边界(红色)。对齐的过程是靠几何运算进行。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/path_align.png"></p>
<p>仅仅靠聚类(Clustering)和Spline Fiting得到下图右上角的结果，这个结果不是特别理想。后来通过神经网络生成高精度地图，效果好了很多。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/model_process.png"></p>
<h1 id="为什么语义理解离不开众包"><a href="#为什么语义理解离不开众包" class="headerlink" title="为什么语义理解离不开众包"></a>为什么语义理解离不开众包</h1><p>如下左图所示，通过众包数据可以在没有Lane Marking的道路上获取Driving Path。</p>
<p>如下右图所示，众包数据提供了复杂场景下的所有可通行路径。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_driving_path.png"></p>
<p>如下图所示，通过众包数据可以获得红绿灯与车道的关联关系、Yield Sign的Stop Point、Crosswalk与红绿灯的关联关系等。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_assocation.png"></p>
<p>如下左图所示，通过检测哪个Drive Path的Stop Point比较多，我们可以从众包数据中获取到没有Traffic Sign情况下各个道路的路权优先级。</p>
<p>如下中图所示，我们可以从众包数据学习到在路口其它司机的停车位置。</p>
<p>如下右图所示，从众包数据可以学习到，在无保护左转的场景下车辆的Stop Point。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_more_info.png"></p>
<p>众包数据是获得各个道路Common Speed的唯一高效的方法，Common Speed提供了当道路没有车辆时候AV车的目标行驶速度。采用这种方法可以使得无论在哪个国家、地区，或者不同的道路类型，AV车都可以自然的融入车流。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_speed.png"></p>
<h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>到目前为止，Mobileye与超过6家汽车制造厂商合作，每天可以覆盖800万公里的路网更新。预计到2024年，每天覆盖的路网会达到10亿公里。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/mobileye_situation.png"></p>
<p><strong>说明</strong>： 本文所有内容都来源于Mobileye CEO Amnon Shashua教授在2021 CES的分享。</p>
<p>YouTube链接：<br><a href="https://www.youtube.com/watch?v=B7YNj66GxRA&amp;t=301s">https://www.youtube.com/watch?v=B7YNj66GxRA&amp;t=301s</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>高精地图</tag>
        <tag>Mobileye</tag>
      </tags>
  </entry>
  <entry>
    <title>Waymo-自动驾驶长尾问题挑战</title>
    <url>/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/</url>
    <content><![CDATA[<p>尽管Waymo已经在开放道路上积累超过10 Million Miles，Waymo的工程师们仍然发现有层出不穷的新自动驾驶场景待解决。</p>
<h1 id="自动驾驶长尾场景举例"><a href="#自动驾驶长尾场景举例" class="headerlink" title="自动驾驶长尾场景举例"></a>自动驾驶长尾场景举例</h1><p><strong>场景一</strong>：一个骑自行车的人手中拿着一个Stop Sign标识牌。我们不知道它何时会举起标识牌。无人车必须理解这种场景，即使他举起了Stop Sign标识牌，自动驾驶汽车也不应该停下来。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-07-22-03-14-1024x574.png"></p>
<p><strong>场景二:</strong> 迎面而来的车辆上装载的塑料管子撒了一地，自动驾驶汽车必须学会应对这种突发情况，并且避开它们对无人车行驶的影响。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-07-22-08-44-1024x370.png"></p>
<p><strong>场景三：</strong>由于道路施工等因素，路面布满锥桶。无人车必须正确识别这些场景，在布满路面锥桶的场景下实现合理驾驶。</p>
<p><img src="/gifhome_774x432_10s.gif"></p>
<p><strong>场景四：</strong>路口绿灯，无人车拥有路权，虽然我们的无人车先到达路口，但必须为稍后到达的特种车辆让行。</p>
<p><img src="/gifhome_774x432_8s.gif"></p>
<p><strong>场景五：</strong> 路口绿灯，无人车准备左转，遇到闯红灯高速通过的社会车辆，无人车需要识别这种场景，并及时停车避让违规车辆。</p>
<p><img src="/gifhome_774x432_5s-1.gif"></p>
<h1 id="自动驾驶核心模块-Perception-Prediction和Planning"><a href="#自动驾驶核心模块-Perception-Prediction和Planning" class="headerlink" title="自动驾驶核心模块-Perception, Prediction和Planning"></a>自动驾驶核心模块-Perception, Prediction和Planning</h1><p>Perception、Prediction和Planning模块是自动驾驶的核心模块，每个模块都存在巨大的挑战。</p>
<h2 id="Perception"><a href="#Perception" class="headerlink" title="Perception"></a>Perception</h2><p>Perception输入：传感器(激光雷达)输入信息以及场景的先验信息。</p>
<p>Perception输出：道路交通对象(行人、车辆等)，对道路场景的语义分割和理解。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-08-56-17.png"></p>
<p>Perception本身是一个非常复杂、高难度的问题，它必须能够识别各种形态各异、不同种类的对象。比如下左一图，一群穿着恐龙服的行人，感知必须能够正确识别它们。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-03-50-1024x338.png"></p>
<p>相同的物体在不同的时间、不同的季节它们的外观表现也会有很大的差异，这会对Perception带来巨大挑战。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-12-28-1024x311.png"></p>
<p>各种复杂场景的分割理解难度极高。如下图左一：一个搬着箱子的人；下图左三：骑马的人。Perception必须能够正确的分割识别这些场景，而不会因为遮挡导致出现识别的错误。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-16-52-1024x300.png"></p>
<h2 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h2><p>Perception对检测到的物体进行下一步行为的预测，以辅助自动驾驶车辆进行合理的行为决策。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-29-49-1024x259.png"></p>
<p>Perception要考虑物体的历史行为，比如车辆不会在短时间内实现90度的转弯，因此我们可以假设车辆在短时间内仍然按照当前的朝向和速度前进；要对场景有更高语义层面的理解；要能够关注到不同对象的属性差异和视觉线索，比如车辆大概率是会在车道上行驶上，行人会走斑马线，车辆的朝向能够大概率反应它的意图，如果行人做出停车的手势，大概率是要过马路；要能够解决待预测物体与其它物体的行为交互。</p>
<p>如下图所示，路边有一辆静止的车辆，骑自行车的人在靠近静止车辆时，会侵入无人车车道。Perception模块需要正确理解这些场景，并生成合理的预测曲线。</p>
<p><img src="/gifhome_774x432_5s.gif"></p>
<p>如何能够准确的预测社会车辆的行为仍然是一个存在巨大挑战的开放性问题。</p>
<h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>Planning是Decision Making Machine，它基于Perception和Prediction的输出，规划车辆的行为，并输出Control模块，控制车辆的加减速、刹车等行为。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-51-50.png"></p>
<p>Planning首要考虑的是安全(safe)，其次要考虑驾乘的舒适性(comfortable)，再次要能够与其它交通参与者正确交互，最后要保证乘客送达目的地。如何能够满足这些条件实现良好的Planning效果仍然是一个开放性的问题。</p>
<p><img src="/gifhome_774x432_10s-1-2.gif"></p>
<h1 id="大规模机器学习技术-Machine-Learning-At-Scale"><a href="#大规模机器学习技术-Machine-Learning-At-Scale" class="headerlink" title="大规模机器学习技术(Machine Learning At Scale)"></a>大规模机器学习技术(Machine Learning At Scale)</h1><p>Machine Learning是解决自动驾驶长尾问题的一种有效工具。利用Machine Learning技术可以实现从数据采集、标注、训练、车端部署的闭环循环流程，从而实现Case的不断积累，模型的不断完善。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-22-39-1024x426.png"></p>
<h2 id="Automated-Machine-Learning技术"><a href="#Automated-Machine-Learning技术" class="headerlink" title="Automated Machine Learning技术"></a>Automated Machine Learning技术</h2><p>Waymo使用了Automated Machine Learning技术生成和优化针对无人车的数据模型，极大提升了模型训练的效率。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-37-13-1024x530.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-39-29-1024x521.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-40-27-1024x425.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-43-04-1024x523.png"></p>
<h2 id="机器学习技术的局限-Limits-Of-Machine-Learning"><a href="#机器学习技术的局限-Limits-Of-Machine-Learning" class="headerlink" title="机器学习技术的局限(Limits Of Machine Learning)"></a>机器学习技术的局限(Limits Of Machine Learning)</h2><p>机器学习模型不能解决所有的问题，但我们需要的是一个安全的自动驾驶系统，所以必须有其它措施来补充ML的不足。</p>
<p>首先可以借助于冗余互补的传感器辅助解决这个问题。车辆同时配备了视觉、Lidar、Radar系统，各个系统彼此独立，相互补充，以最大限度保证无人车不会缺失任何信息。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-48-43-1024x599.png"></p>
<p>其次，我们可以采用ML和Non-ML混合系统，利用专家系统来弥补ML的不足。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-56-06.png"></p>
<h1 id="大规模的测试技术-Large-Scale-Testing"><a href="#大规模的测试技术-Large-Scale-Testing" class="headerlink" title="大规模的测试技术(Large Scale Testing)"></a>大规模的测试技术(Large Scale Testing)</h1><p>首先Waymo有庞大的自动驾驶车队，可以支撑大规模的测试。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-16-08-1024x494.png"></p>
<p>有些场景在实际道路上出现的概率很低，为了测试验证这些低频问题，需要自己构建场景，进行结构化测试。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-19-08-1024x471.png"></p>
<p>仿真是一种重要的验证测试手段，可以轻量级安全的构造各种各样的测试场景。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-22-53.png"></p>
<p>自动驾驶仿真必须能够真实模拟车辆和行人的行为。这仅仅依靠简单的规则模型是不够的，我们需要更加复杂的模型，Waymo使用一种Mid-2-Mid的Drive Agent机器学习模型，它接收定位、感知等信息，输出更加拟人化的运动规划。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-40-00-1024x269.png"></p>
<p>Waymo提出的ChauffeurNet将Map、交通规则、道路环境等信息转化为图像信息，从而可以最大限度的利用比较成熟的机器学习模型，最终输出Agent的Trajectory。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-46-38-1024x543.png"></p>
<p>ChauffeurNet可以解决大部分简单场景下的Prediction和Planning问题。</p>
<p><img src="/unnamed.gif"></p>
<p>场景中红色的拖尾是Agent的历史轨迹，绿色是未来2s的预测轨迹。</p>
<p><img src="/unnamed-1.gif"></p>
<p>主车成功的通过路边静止车辆的场景</p>
<p><img src="/unnamed-2.gif"></p>
<p>主车遇到缓慢前行的车辆后减速</p>
<p>当然ChauffeurNet也有其局限性，比如以下复杂场景目前还不能很好的处理。</p>
<p><img src="/gifhome_406x306_10s.gif"></p>
<p>主车由于视距遮挡，直接冲出了路口</p>
<p><img src="/gifhome_406x306_8s.gif"></p>
<p>车辆没有成功完成掉头操作</p>
<h1 id="机器学习难以覆盖的长尾问题挑战"><a href="#机器学习难以覆盖的长尾问题挑战" class="headerlink" title="机器学习难以覆盖的长尾问题挑战"></a>机器学习难以覆盖的长尾问题挑战</h1><p>对自动驾驶测试来讲，最大的挑战在于很难收集到所有Corner Case。如下图所示，是人类驾驶行为分布，要经过非常长时间的积累才能得到一些Corner的驾驶行为Case。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-15-46-53.png"></p>
<p>在自动驾驶网络的神经网络模型中，可能有上千万的参数，如果Corner Case的样本数量太少，就难以保证网络模型能够学会这些Corner场景。</p>
<p>在神经网络模型覆盖长尾Case前，如何来解决长尾Case呢？专家系统是一个选择。专家系统融入专业的知识，通过小批量的样本就可以获得效果比较好的参数。</p>
<p>比如我们计划得到实现一个轨迹优化机器学习模型，在基于运动控制理论和一系列的约束设计好专家模型之后，通过采集历史车辆轨迹，我们就可以调整参数最小化Cost的方法，使得专家系统的轨迹输出尽可能的逼近人类驾驶轨迹。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-15-53-40-1024x576.png"></p>
<p>轨迹优化专家系统的另一种模型是Inverse Reinforcement Learning技术，通过历史驾驶轨迹训练模型参数，使得它的输出尽可能的逼近预期效果。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-02-30.png"></p>
<p>如下图所示，红色的主车，蓝色的是社会车辆。左图的社会车辆更加保守，右侧的社会车辆更加激进。用保守的轨迹训练出的模型表现就趋于保守，用激进的轨迹训练出的模型表现就趋于激进。</p>
<p><img src="/gifhome_774x222_17s.gif"></p>
<h1 id="Smart-Agent对于自动驾驶规模化不可或缺"><a href="#Smart-Agent对于自动驾驶规模化不可或缺" class="headerlink" title="Smart Agent对于自动驾驶规模化不可或缺"></a>Smart Agent对于自动驾驶规模化不可或缺</h1><p>不管是专家系统，还是神经网络，它们都在努力模拟人的驾驶行为，使Agent变得聪明起来，聪明的Agent可以辅助自动驾驶技术快速规模化。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-19-28-1024x319.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-21-58-1024x433.png"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>waymo</tag>
        <tag>自动驾驶长尾问题</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶Mapping-占位栅格图(Occupancy Grid Map)</title>
    <url>/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/</url>
    <content><![CDATA[<p>前面文章《自动驾驶运动规划(Motion Planning)》中提到可以使用占位图(Occupancy Grid Map)表示自动驾驶行驶区域的哪些区域被障碍物(如静止的车辆、路中间的石墩子、树木、路肩等)占用，Motion Planning模块会通过查询占位地图避开这些道路障碍物，避免与它们碰撞，从而达到安全驾驶的目的。</p>
<span id="more"></span>

<h1 id="占位栅格地图-Occupancy-Grid-Map"><a href="#占位栅格地图-Occupancy-Grid-Map" class="headerlink" title="占位栅格地图(Occupancy Grid Map)"></a>占位栅格地图(Occupancy Grid Map)</h1><p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-10-43-36-1024x604.png"></p>
<p>如上图所示，将车辆行驶道路环境用网格(Cell)切分，并且每个网格(Cell)用二值数值0和1填充，0表示该网格(Cell)被占用，1表示该网格(Cell)没有被占用。</p>
<p>$m^{i} \in \{0, 1\}$</p>
<p>由此，可以得到如下所示的一张栅格占位地图。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-10-54-24-1024x607.png"></p>
<p>要制作理想的占位栅格地图必须满足的以下几个假设条件：</p>
<p>1）占位栅格地图是对道路行驶区域中的静态环境(Static Environment)的描述。也就意味着，我们在制图前必须将地面、动态物体(车辆、行人等)从传感器数据中移除掉；</p>
<p>2）每个网格(Cell)与其它的所有网格的状态是相互独立的，即它的状态不受周围其它网格状态的影响；</p>
<p>3）在每个时刻，车辆的位置是精确的、已知的。</p>
<h1 id="概率占位栅格地图-Probabilistic-Occupancy-Grid-Map"><a href="#概率占位栅格地图-Probabilistic-Occupancy-Grid-Map" class="headerlink" title="概率占位栅格地图(Probabilistic Occupancy Grid Map)"></a>概率占位栅格地图(Probabilistic Occupancy Grid Map)</h1><p>在实际的应用中，车辆传感器的数据测量是存在误差的，车辆的定位结果也是存在误差的，动态障碍物的识别也是存在误差的，因此用概率表示一个网格(Cell)被占用的可能性是一个更加可行的方案。每个网格存储一个[0, 1]之间的概率值，这个值越大，表示网格被占用的可能性越大；这个值越小，表示网格被占用的可能性越小。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-11-32-38.png"></p>
<h1 id="概率占位栅格图-Probabilistic-Occupancy-Grid-Map-制图"><a href="#概率占位栅格图-Probabilistic-Occupancy-Grid-Map-制图" class="headerlink" title="概率占位栅格图(Probabilistic Occupancy Grid Map)制图"></a>概率占位栅格图(Probabilistic Occupancy Grid Map)制图</h1><p>栅格地图的每个Cell的概率值计算公式如下：</p>
<p>$bel_t(m^i) = p(m^i (y, x)_{1:t})$</p>
<p>其中$(y, x)_{1:t}$是1到t时刻的车辆位置和传感器测量结果，通过历史信息的累计，可以提升制作的地图的准确性。</p>
<p>如何将1到t时刻的所有传感器测量结果融合起来呢？贝叶斯理论(Bayes Theorem)是一个不错的选择。</p>
<p>$bel_t(m^i) = \eta p(y_t m^i) bel_{t-1}(m^i)$</p>
<p>其中$\eta$是归一化参数, $p(y_t m^i)$是传感器的测量模型。通过贝叶斯理论(Bayes Theorem)将多次传感器测量结果融合到同一个Cell中，从而获得高可信度的网格占用概率。</p>
<h2 id="贝叶斯理论-Bayes-Theorem-更新存在的问题"><a href="#贝叶斯理论-Bayes-Theorem-更新存在的问题" class="headerlink" title="贝叶斯理论(Bayes Theorem)更新存在的问题"></a>贝叶斯理论(Bayes Theorem)更新存在的问题</h2><p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-18-37-34.png"></p>
<p>重复的浮点数乘法运算导致计算结果的数值变得很小而难以精确表达和运算。Logit函数可以把自变量从(0,1)连续单调地映射到正负无穷。logit函数的定义如下：</p>
<p>$f(x) = log {\frac{x}{1 - x}}$</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/20180901104349204.png"></p>
<p>所以我们使用Logit函数替代标准的Bayes更新过程。</p>
<h2 id="贝叶斯更新过程的推导"><a href="#贝叶斯更新过程的推导" class="headerlink" title="贝叶斯更新过程的推导"></a>贝叶斯更新过程的推导</h2><p>贝叶斯理论(Bayes Theorem)更新网格(Cell)占用概率的公式如下：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(y_{t} y_{1: t-1}, m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(y_{t} y_{1: t-1}\right)} \tag{1}<br>$</p>
<p>根据一阶马尔科夫(Markov Assumption)假设，t时刻的状态只与t-1时刻的状态有关，因此公式(1)可写为如下形式：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(y_{t} m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(y_{t} y_{1: t-1}\right)} \tag{2}<br>$</p>
<p>对测量模型应用贝叶斯(Bayes Theorem)更新过程：</p>
<p>$<br>p\left(y_{t} m^{i}\right)=\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right)}{p\left(m^{i}\right)} \tag{3}<br>$</p>
<p>将公式3)代入公式2)，可得：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(m^{i}\right) p\left(y_{t} y_{1: t-1}\right)} \tag{4}<br>$</p>
<p>然后计算1-p的值：</p>
<p>$<br>p\left(\neg m^{i} y_{1: t}\right)=1-p\left(m^{i} y_{1: t}\right)=\frac{p\left(\neg m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i}\right) p\left(y_{t} y_{1: t-1}\right)} \tag{5}<br>$</p>
<p>将p和1-p代入logit函数：</p>
<p>$<br>\operatorname{logit}(p)=\log \left(\frac{p}{1-p}\right)<br>$</p>
<p>$<br>\begin{aligned}<br>\quad \frac{p\left(m^{i} y_{1: t}\right)}{p\left(\neg m^{i} y_{1: t}\right)} &amp; =\frac{\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(m^{i}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}}{\frac{p\left(\neg m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i}\right) p\left(y_{t} y_{1: t-1}\right)}} \\<br>&amp;=\frac{p\left(m^{i} y_{t}\right) p\left(\neg m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i} y_{t}\right) p\left(m^{i}\right) p\left(\neg m^{i} y_{1: t-1}\right)} \\<br>&amp;=\frac{p\left(m^{i} y_{t}\right)\left(1-p\left(m^{i}\right)\right) p\left(m^{i} y_{1: t-1}\right)}{\left(1-p\left(m^{i} y_{t}\right)\right) p\left(m^{i}\right)\left(1-p\left(m^{i} y_{1: t-1}\right)\right)}<br>\end{aligned} \tag{6}<br>$</p>
<p>对公式6）等号两侧取log，进行整理后，得到：</p>
<p>$<br>\operatorname{logit}\left(p\left(m^{i} y_{1: t}\right)\right)=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+\operatorname{logit}\left(p\left(m^{i} y_{1: t-1}\right)\right)-\operatorname{logit}\left(p\left(m^{i}\right)\right)<br>$</p>
<p>于是得到<strong>Bayes更新递推公式</strong>：</p>
<p>$<br>l_{t, i}=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+l_{t-1, i}-l_{0, i}<br>$</p>
<p>其中: $\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)$是Inverse Measurement Model，$l_{t-1, i}$是网格i在t-1时刻的置信度(belif)，$l_{0,i}$是Initial belief。</p>
<p>可以看到，该递推公式应用的关键是Inverse Measurement Model：$p\left(m^{i} y_{t}\right))$，如何计算该值呢？</p>
<h2 id="Inverse-Measurement-Model"><a href="#Inverse-Measurement-Model" class="headerlink" title="Inverse Measurement Model"></a>Inverse Measurement Model</h2><p>占位栅格地图的传感器测量模型为：$p(y_t m^{i})$，表示基于已有的地图Cell概率，叠加传感器测量结果，得到新的占位概率值。</p>
<p>而现在我们要求解的是：$p(m^{i} y_t)$，这也是为什么该公式被成为Inverse Measurement Model的原因。</p>
<p>下面来看看Inverse Measurement Model如何计算？下面以二维激光雷达扫描模型来说明(注意：实际应用的激光雷达是3D的，这里用2D Lidar是为了简化模型，所用理论可以很好推广到3D模型)。</p>
<p><strong>2D Lidar模型</strong></p>
<p>它在2D平面上进行扫描，包含两个参数：Scanner bearing和Scanner rangers。Scanner bearing均匀的分布在[$-{\phi_{max}}^s, {\phi_{max}}^s$]之间，一般的我们可以认为它们均匀分布在360度的各个方向上。Scanner rangers是从Lidar中心到障碍物的距离，Lidar发出激光、接收回波，从而计算出到周围障碍物的距离；为了简化期间，我们也假设Lidar发送激光后立即收到回波，不存在时间延迟。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-06-50-1024x362.png"></p>
<p><strong>Map坐标系&amp;Vehicle坐标系&amp;传感器坐标系</strong></p>
<p>数学模型构建过程中<strong>坐标系</strong>是不可或缺的。这里主要涉及到三个坐标系：Map坐标系、Vehicle坐标系以及传感器坐标系。2D Lidar的测量结果都是相对于自身传感器中心的，即以2D Lidar中心为坐标原点；所有的测量结果最终都要转换到Map坐标系，完成地图制作的计算。</p>
<p>假设2D Lidar在Map坐标系中的姿态为$(x_{1,t}, x_{2,t}, x_{3,t})$，其中$x_{1,t}$和$x_{2,t}$是x和y坐标，$x_{3,t}$是传感器朝向。通过该姿态，可以将2D Lidar测量结果转换到Map坐标系。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-21-57-1024x755.png"></p>
<p><strong>Lidar测量结果与Map Cell关联匹配</strong></p>
<p>如何将2D Lidar模型与Map Cell关联起来呢？如下图所示，第i个Map Cell用$(r^{i}, {\phi}^i)$表示。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-38-07-1024x461.png"></p>
<p>然后通过2D Lidar bearing与Map Cell相对于传感器的方位进行最小误差匹配，得到影响当前Map Cell的激光束。</p>
<p>$k = argmin({\phi}_i - {\phi}_i^s)$</p>
<p>匹配的过程如下：首先定义两个值$\alpha$和$\beta$，各个网格Cell的概率计算如下：</p>
<p>1）如果$r^i &gt; {r_{max}}^s$或者$\phi^i - \phi_k^s &gt; \beta /2$， 表示为探测区域，没有信息，这些区域的概率值一般为0.5，表示不确定是否被占用。</p>
<ol>
<li><p>如果$r_k^s &lt; r_{max}^s$并且$r^i - r_k^s &lt; \alpha / 2$，表示该区域大概率被占用，因此要赋予一个大于0.5的概率值。</p>
</li>
<li><p>如果$r^i - r_k^s &gt; \alpha / 2$，这些网格被占用的概率较低，因此要赋予一个小于0.5的概率值。</p>
</li>
</ol>
<p><img src="/creenshot-from-2020-02-01-12-50-41-1024x462.png"></p>
<p>如下图所示，红色区域为高概率被占用区域，灰色区域为未知区域，其余区域为低概率被占用区域。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-14-59-27-1024x472.png"></p>
<p>至此，有了Inverse Measurement Model，Bayes更新的过程可以正常进行了。</p>
<p><strong>更高效的Inverse Measurement Model计算方法</strong></p>
<p>采用光线跟踪(Ray Tracing)的Bresenham’s Line Algorithm可以大大减少复杂的浮点数计算，提升计算效率。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-20-01.png"></p>
<h1 id="移除Lidar中地面和动态物体"><a href="#移除Lidar中地面和动态物体" class="headerlink" title="移除Lidar中地面和动态物体"></a>移除Lidar中地面和动态物体</h1><p>实际应用中的激光雷达(Lidar)是3D的，会扫描到大量的地面点，这些地面点如果不被移除，按照计算匹配模型，会被当做障碍物处理。所以需要将地面点点云数据从激光雷达点云中移除掉。如何移除呢？一种可行的方法是，通过自动化识别算法从Lidar点云中将地面识别并剔除。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-43-18-1024x652.png"></p>
<p>地面识别的难度是比较高的，因为很多道路路面内外的界限在点云中是不明确的，自动化识别算法会误把道路边界外的区域识别为道路路面，从而导致错误的地图信息等。通过视觉分割算法辅助点云识别可以提升路面的识别率。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-47-38-1024x511.png"></p>
<p>动态物体(行人、车辆等)也需要从点云数据中移除，这依赖于基于点云和图像的感知技术。但同样也存在很多技术难题，比如如何提升识别的准确率，如何将静止的车辆识别出来等等。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-48-41.png"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>本文主要整理自Coursera自动驾驶课程：Motion Planning for Self-Driving Cars第二周课程的学习笔记。</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>Mapping技术</tag>
        <tag>占位栅格地图</tag>
        <tag>概率占位栅格地图</tag>
        <tag>自动驾驶Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶运动规划(Motion Planning)</title>
    <url>/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/</url>
    <content><![CDATA[<h1 id="什么是Motion-Planning"><a href="#什么是Motion-Planning" class="headerlink" title="什么是Motion Planning"></a>什么是Motion Planning</h1><p>Motion Planning是在遵循道路交通规则的前提下，将自动驾驶车辆从当前位置导航到目的地的一种方法。</p>
<p>在实际开放道理场景下，自动驾驶要处理的场景非常繁杂：空旷的道路场景、与行人、障碍物共用道理的场景、空旷的十字路口、繁忙的十字路口、违反交通规则的行人/车辆、正常行驶的车辆/行人等等。场景虽然复杂，但都可以拆解为一系列简单行为(behavior)的组合:</p>
<span id="more"></span>

<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-18-57-56-1024x468.png"></p>
<p>将这些简单的行为(behavior)组合起来，就可以完成复杂的驾驶行为。</p>
<h1 id="Motion-Planning的约束条件-constraints"><a href="#Motion-Planning的约束条件-constraints" class="headerlink" title="Motion Planning的约束条件(constraints)"></a>Motion Planning的约束条件(constraints)</h1><p>Motion Planning是一个复杂的问题，它的执行过程需要满足很多约束条件：</p>
<h2 id="车辆运动学约束"><a href="#车辆运动学约束" class="headerlink" title="车辆运动学约束"></a>车辆运动学约束</h2><p>车辆运动受到运动学约束，比如它不能实现瞬时侧向移动，前驱的车辆必须依赖前轮的转向才能实现变道、转向等操作，在弯道上不能速度过快等等。通常我们采用单车模型(Bicycle Model)对车辆运动进行建模。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-17-52.png"></p>
<h2 id="静态障碍物-Static-Obstacle-约束"><a href="#静态障碍物-Static-Obstacle-约束" class="headerlink" title="静态障碍物(Static Obstacle)约束"></a>静态障碍物(Static Obstacle)约束</h2><p>静态障碍物(Static Obstacle)是道路上静止的车辆、路面中间的石墩子等车辆不可行驶的区域。Motion Planning需要避开这些静态障碍物，避免与它们发生碰撞。解决碰撞的思路大概有两种：</p>
<p>1）将静态障碍物(Static Obstacle)在网格占位图中表示出来，然后检测规划路线是否与静态障碍物区域相交。</p>
<p>2）将车辆的轮廓扩大，比如扩展成一个圆形，然后检测障碍物是否与Circle发生碰撞。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-26-13.png"></p>
<h2 id="动态障碍物约束"><a href="#动态障碍物约束" class="headerlink" title="动态障碍物约束"></a>动态障碍物约束</h2><p>Motion Planning要实时处理行人、车辆等各种运动的障碍物，避免与障碍物发生碰撞事故。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-37-33-1024x584.png"></p>
<h2 id="道路交通规则约束"><a href="#道路交通规则约束" class="headerlink" title="道路交通规则约束"></a>道路交通规则约束</h2><p>车辆在道路上行驶必须要遵守车道线约束规则(比如左转专用道只能左转、实线不能变道、路口必须遵守红绿灯的指示)和各种标志标牌的指示。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-55-29-1024x281.png"></p>
<h1 id="Motion-Planning的优化目标"><a href="#Motion-Planning的优化目标" class="headerlink" title="Motion Planning的优化目标"></a>Motion Planning的优化目标</h1><p>了解Motion Planning的约束条件之后，需要构造目标优化函数，然后最小化目标函数，从而获得在当前环境下的最优运动轨迹。目标函数的种类有很多，下面枚举一些常用的目标函数。</p>
<p>1）关注路径长度(Path Length)，寻求到达目的地的最短路径。</p>
<p>$s_f = \int^{s_f}_{s_i}{\sqrt{1+ (\frac{dy}{dx})^2}dx}$</p>
<p>2）关注通行时间(Travel Time)，寻求到达目的地的最短时间。</p>
<p>$T_f = \int^{s_f}_{0} {\frac{1}{v(s)}ds}$</p>
<p>3）惩罚偏离参考轨迹和参考速度的行为。</p>
<p>$\int^{s_f}_{0} {x(s) - x_{ref}(s)ds}$</p>
<p>$\int^{s_f}_{0} {v(s) - v_{ref}(s)ds}$</p>
<p>4）考虑轨迹平滑性(Smoothness)</p>
<p>$\int^{s_f}_{0} {\dddot{x}(s)^2ds}$</p>
<p>5）考虑曲率约束(Curvature)</p>
<p>$\int^{s_f}_{0} {k(s)^2ds}$</p>
<p>通过组合设计自己的目标优化函数，从而获得较好的Planning效果。</p>
<h1 id="分级运动规划器-Hierarchical-Motion-Planning"><a href="#分级运动规划器-Hierarchical-Motion-Planning" class="headerlink" title="分级运动规划器(Hierarchical Motion Planning)"></a>分级运动规划器(Hierarchical Motion Planning)</h1><p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-37-52-1024x324.png"></p>
<p>Motion Planning是一个异常复杂的问题，所以通常我们把它切分为一系列的子问题(Sub Problem)。比如Mission Planner、Behavior Planner、Local Planner、Vehicle Control等。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-40-07.png"></p>
<h2 id="Mission-Planner"><a href="#Mission-Planner" class="headerlink" title="Mission Planner"></a>Mission Planner</h2><p>Mission Planner关注High-Level的地图级别的规划；通过Graph Based的图搜索算法实现自动驾驶路径的规划。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-51-30.png"></p>
<h2 id="Behavior-Planner"><a href="#Behavior-Planner" class="headerlink" title="Behavior Planner"></a>Behavior Planner</h2><p>Behavior Planner主要关注交通规则、其它道路交通参与者(自行车、行人、社会车辆)等等，决定在在当前场景下应该采取何种操作(如停车让行、加速通过、避让行人等等)。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-01-29.png"></p>
<p>Behavior Planner的实现方式比较常见的有几种：<strong>有限状态机(Finite State Machines)、规则匹配系统(Rule Based System)、强化学习系统(Reinforcement Learning)。</strong></p>
<p>有限状态机中的State是各个行为决策，根据对外界环境的感知和交通规则的约束在各个状态之间转换。比如在路口红绿灯的场景，当路口交通灯为红色不可通行时，车辆会首先切换到Decelerate to Stop状态，然后在路口停止线完全停下来，进入Stop状态，并持续在Stop状态等待，直至交通灯变为绿色允许车辆通行，车辆进入Track Speed状态，继续前行。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-15-18.png"></p>
<p>Rule-Based System是通过一系列的分级的规则匹配来决定下一步的决策行为。比如交通灯绿色-&gt;通行；交通灯红色-&gt;停车等待。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-25-58-1024x174.png"></p>
<p>基于强化学习的Behavior Planner系统如下：</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-31-32-1024x498.png"></p>
<h2 id="Local-Planner"><a href="#Local-Planner" class="headerlink" title="Local Planner"></a>Local Planner</h2><p>Local Planner关注如何生成舒适的、碰撞避免的行驶路径和舒适的运动速度，所以Local Planner又可以拆分为两个子问题：<strong>Path Planner和Velocity Profile Generation</strong>。Path Planner又分为Sampling-Based Planner、Variational Planner和Lattice Planner。</p>
<p>最经典的Sampling-Based Planner算法是Rapidly Exploring Random Tree，RRT算法。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-36-30.png"></p>
<p>Variational Planner根据Cost Function进行优化调整，从而避开障碍物，生成安全的轨迹。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-45-12-1024x464.png"></p>
<p>Lattice Planner将空间搜索限制在对车辆可行的Action Space。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-48-25.png"></p>
<p><strong>Velocity Profile Generation</strong>要考虑到限速、速度的平滑性等。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-52-05-1024x529.png"></p>
<p>Vehicle Control将Planner的规划结果转化为车辆的运动行为。</p>
<h1 id="待阅读材料"><a href="#待阅读材料" class="headerlink" title="待阅读材料"></a>待阅读材料</h1><ul>
<li><p>  P. Polack, F. Altche, B. Dandrea-Novel, and A. D. L. Fortelle, “<a href="https://ieeexplore.ieee.org/abstract/document/7995816">The kinematic bicycle model: A consistent model for planning feasible trajectories for autonomous vehicles</a>” 2017 IEEE Intelligent Vehicles Symposium (IV), 2017. Gives an overview of the kinematic bicycle model.</p>
</li>
<li><p>  S. Karaman and E. Frazzoli, “<a href="http://amav.gatech.edu/sites/default/files/papers/icra2013.Karaman.Frazzoli.submitted.pdf">Sampling-based optimal motion planning for non-holonomic dynamical systems</a>,” 2013 IEEE International Conference on Robotics and Automation, 2013. Introduces the RRT* algorithm as an example of sampling-based planning.</p>
</li>
<li><p>  N. Ratliff, M. Zucker, J. A. Bagnell, and S. Srinivasa, “<a href="https://kilthub.cmu.edu/articles/CHOMP_Gradient_Optimization_Techniques_for_Efficient_Motion_Planning/6552254/1">CHOMP: Gradient optimization techniques for efficient motion planning</a>,” 2009 IEEE International Conference on Robotics and Automation, 2009. Introduces the CHOMP algorithm as an example of applying calculus of variations to planning.</p>
</li>
<li><p>  M. Pivtoraiko, R. A. Knepper, and A. Kelly, “<a href="https://ri.cmu.edu/pub_files/2009/3/ross.pdf">Differentially constrained mobile robot motion planning in state lattices</a>,” Journal of Field Robotics, vol. 26, no. 3, pp. 308-333, 2009. Introduces the state lattice planning method.</p>
</li>
</ul>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1、Course自动驾驶课程： <a href="https://www.coursera.org/learn/motion-planning-self-driving-cars/home/welcome">Motion Planning for Self-Driving Cars</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>Hierarchical Motion Planning</tag>
        <tag>Motion Planner</tag>
        <tag>Motion Planning</tag>
        <tag>Motion Planning Objective Function</tag>
        <tag>分级运动规划器</tag>
        <tag>自动驾驶运动规划</tag>
        <tag>车辆动力学</tag>
        <tag>车辆运动学</tag>
        <tag>运动规划目标函数</tag>
        <tag>运动规划约束</tag>
      </tags>
  </entry>
</search>
