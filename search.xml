<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度神经网络识别交通标牌</title>
    <url>/2021/03/15/cnn-sign-detection/</url>
    <content><![CDATA[<p>这里我们实现一个入门级的CNN交通标牌分类网络。</p>
<p><img src="/2021/03/15/cnn-sign-detection/dataset_input_output.png"></p>
<span id="more"></span>

<p>首先导入基础依赖库。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>

<h1 id="数据集-Dataset"><a href="#数据集-Dataset" class="headerlink" title="数据集(Dataset)"></a>数据集(Dataset)</h1><p>数据集(Dataset)中包含43个不同分类的、大小为32x32的RGB图像，分类如下:</p>
<ul>
<li>0 = Speed limit (20km/h)</li>
<li>1 = Speed limit (30km/h)</li>
<li>2 = Speed limit (50km/h)</li>
<li>3 = Speed limit (60km/h)</li>
<li>4 = Speed limit (70km/h)</li>
<li>5 = Speed limit (80km/h)</li>
<li>6 = End of speed limit (80km/h)</li>
<li>7 = Speed limit (100km/h)</li>
<li>8 = Speed limit (120km/h)</li>
<li>9 = No passing</li>
<li>10 = No passing for vehicles over 3.5 metric tons</li>
<li>11 = Right-of-way at the next intersection</li>
<li>12 = Priority road</li>
<li>13 = Yield</li>
<li>14 = Stop</li>
<li>15 = No vehicles</li>
<li>16 = Vehicles over 3.5 metric tons prohibited</li>
<li>17 = No entry</li>
<li>18 = General caution</li>
<li>19 = Dangerous curve to the left</li>
<li>20 = Dangerous curve to the right</li>
<li>21 = Double curve</li>
<li>22 = Bumpy road</li>
<li>23 = Slippery road</li>
<li>24 = Road narrows on the right</li>
<li>25 = Road work</li>
<li>26 = Traffic signals</li>
<li>27 = Pedestrians</li>
<li>28 = Children crossing</li>
<li>29 = Bicycles crossing</li>
<li>30 = Beware of ice/snow</li>
<li>31 = Wild animals crossing</li>
<li>32 = End of all speed and passing limits</li>
<li>33 = Turn right ahead</li>
<li>34 = Turn left ahead</li>
<li>35 = Ahead only</li>
<li>36 = Go straight or right</li>
<li>37 = Go straight or left</li>
<li>38 = Keep right</li>
<li>39 = Keep left</li>
<li>40 = Roundabout mandatory</li>
<li>41 = End of no passing</li>
<li>42 = End of no passing by vehicles over 3.5 metric tons</li>
</ul>
<h2 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h2><p>读取训练集、验证集和测试集:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/train.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> training_data:</span><br><span class="line">    train = pickle.load(training_data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/valid.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> validation_data:</span><br><span class="line">    valid = pickle.load(validation_data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/test.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> testing_data:</span><br><span class="line">    test = pickle.load(testing_data)</span><br></pre></td></tr></table></figure>

<p>查看训练集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train, y_train = train[<span class="string">&quot;features&quot;</span>], train[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>

<p>(34799, 32, 32, 3)</p>
<p>查看验证集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_validation, y_validation = valid[<span class="string">&quot;features&quot;</span>], valid[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_validation.shape)</span><br></pre></td></tr></table></figure>

<p>(4410, 32, 32, 3)</p>
<p>查看测试集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test, y_test = test[<span class="string">&quot;features&quot;</span>], test[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_test.shape)</span><br></pre></td></tr></table></figure>

<p>(12630, 32, 32, 3)</p>
<h2 id="数据集可视化"><a href="#数据集可视化" class="headerlink" title="数据集可视化"></a>数据集可视化</h2><p>随机选取一张图片：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = np.random.randint(<span class="number">1</span>, <span class="built_in">len</span>(X_train))</span><br><span class="line"></span><br><span class="line">plt.imshow(X_train[i])</span><br><span class="line"></span><br><span class="line">y_train[i]</span><br></pre></td></tr></table></figure>

<p>图片展示效果如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/traffic_sign.png"></p>
<p>多看一些数据集的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_grid = <span class="number">5</span></span><br><span class="line">L_grid = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(L_grid, W_grid, figsize = (<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">axes = axes.ravel() <span class="comment"># flaten the 5 x 5 matrix into 25 array</span></span><br><span class="line"></span><br><span class="line">n_training = <span class="built_in">len</span>(X_train) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>, W_grid * L_grid):</span><br><span class="line">    <span class="comment"># Select a random number</span></span><br><span class="line">    index = np.random.randint(<span class="number">0</span>, n_training)</span><br><span class="line">    <span class="comment"># read and display an image with the selected index    </span></span><br><span class="line">    axes[i].imshow(X_train[index])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>图片展示效果如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/traffic_signs.png"></p>
<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>使用之前，先对数据进行一些预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line">X_train, y_train = shuffle(X_train, y_train)</span><br></pre></td></tr></table></figure>

<h2 id="灰度化"><a href="#灰度化" class="headerlink" title="灰度化"></a>灰度化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_gray = np.<span class="built_in">sum</span>(X_train / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_validation_gray = np.<span class="built_in">sum</span>(X_validation / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_test_gray = np.<span class="built_in">sum</span>(X_test / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(X_train_gray.shape)</span><br></pre></td></tr></table></figure>

<p>(34799, 32, 32, 1)</p>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>将所有图像数据归一化到[-1, 1]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_gray_norm = (X_train_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X_validation_gray_norm = (X_validation_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X_test_gray_norm = (X_test_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">print(X_train_gray_norm)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[[-0.52083333]</span><br><span class="line">   [-0.52604167]</span><br><span class="line">   [-0.51822917]</span><br><span class="line">   ...</span><br><span class="line">   [-0.48958333]</span><br><span class="line">   [-0.47916667]</span><br><span class="line">   [-0.46614583]]</span><br><span class="line"></span><br><span class="line">  [[-0.52083333]</span><br><span class="line">   [-0.52083333]</span><br><span class="line">   [-0.52864583]</span><br><span class="line">   ...</span><br><span class="line">   [-0.5       ]</span><br><span class="line">   [-0.48958333]</span><br><span class="line">   [-0.4765625 ]]</span><br><span class="line"></span><br><span class="line">  [[-0.54427083]</span><br><span class="line">   [-0.53385417]</span><br><span class="line">   [-0.53385417]</span><br><span class="line">   ...</span><br><span class="line">   [-0.50520833]</span><br><span class="line">   [-0.47916667]</span><br><span class="line">   [-0.47395833]]</span><br><span class="line"></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = random.randint(<span class="number">1</span>, <span class="built_in">len</span>(X_train_gray))</span><br><span class="line">plt.imshow(X_train_gray[i].squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(X_train[i])</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(X_train_gray_norm[i].squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>  灰度化和Normalization的效果如下，从上到下依次为：灰度图像，原图像、归一化的图像。</p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_origin.png" alt="灰度图"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_gray.png" alt="原图"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_normal.png" alt="标准化"></p>
<h1 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h1><p>  <img src="/2021/03/15/cnn-sign-detection/cnn.png"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/dropout.png"></p>
<h2 id="构建深度神经网络"><a href="#构建深度神经网络" class="headerlink" title="构建深度神经网络"></a>构建深度神经网络</h2><p>使用Keras构建CNN网络模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"></span><br><span class="line">CNN = models.Sequential()</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Conv2D(<span class="number">6</span>, (<span class="number">5</span>, <span class="number">5</span>), activation = <span class="string">&#x27;relu&#x27;</span>, input_shape = (<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)))</span><br><span class="line">CNN.add(layers.AveragePooling2D())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Conv2D(<span class="number">16</span>, (<span class="number">5</span>, <span class="number">5</span>), activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.AveragePooling2D())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Dropout(<span class="number">0.2</span>))</span><br><span class="line">CNN.add(layers.Flatten())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Dense(<span class="number">120</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.Dense(<span class="number">84</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.Dense(<span class="number">43</span>, activation = <span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">CNN.summary()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Model: &quot;sequential_1&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">average_pooling2d_1 (Average (None, 14, 14, 6)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)            (None, 10, 10, 16)        2416      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">average_pooling2d_2 (Average (None, 5, 5, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout (Dropout)            (None, 5, 5, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 400)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense)              (None, 120)               48120     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_4 (Dense)              (None, 84)                10164     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_5 (Dense)              (None, 43)                3655      </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 64,511</span><br><span class="line">Trainable params: 64,511</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>

<h2 id="编译和训练"><a href="#编译和训练" class="headerlink" title="编译和训练"></a>编译和训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CNN.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;Adam&#x27;</span>, loss = <span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">history = CNN.fit(X_train_gray_norm,</span><br><span class="line">                  y_train,</span><br><span class="line">                  batch_size = <span class="number">500</span>,</span><br><span class="line">                  epochs = <span class="number">50</span>,</span><br><span class="line">                  verbose = <span class="number">1</span>,</span><br><span class="line">                  validation_data = (X_validation_gray_norm, y_validation))</span><br></pre></td></tr></table></figure>

<p>进行50个Epoch的训练，训练集Accuracy达到98.64%，验证集的Accuracy达到91.61%。</p>
<p>训练过程如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 136ms&#x2F;step - loss: 3.1861 - accuracy: 0.1649 - val_loss: 2.5817 - val_accuracy: 0.3082</span><br><span class="line">Epoch 2&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 135ms&#x2F;step - loss: 1.6409 - accuracy: 0.5335 - val_loss: 1.2052 - val_accuracy: 0.6458</span><br><span class="line">Epoch 3&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 145ms&#x2F;step - loss: 0.9414 - accuracy: 0.7220 - val_loss: 0.8685 - val_accuracy: 0.7417</span><br><span class="line">Epoch 4&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 147ms&#x2F;step - loss: 0.7074 - accuracy: 0.7915 - val_loss: 0.7434 - val_accuracy: 0.7834</span><br><span class="line">Epoch 5&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 145ms&#x2F;step - loss: 0.5825 - accuracy: 0.8317 - val_loss: 0.6605 - val_accuracy: 0.7955</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Epoch 45&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 115ms&#x2F;step - loss: 0.0509 - accuracy: 0.9844 - val_loss: 0.3022 - val_accuracy: 0.9220</span><br><span class="line">Epoch 46&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 114ms&#x2F;step - loss: 0.0480 - accuracy: 0.9861 - val_loss: 0.2822 - val_accuracy: 0.9254</span><br><span class="line">Epoch 47&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 110ms&#x2F;step - loss: 0.0462 - accuracy: 0.9864 - val_loss: 0.2971 - val_accuracy: 0.9259</span><br><span class="line">Epoch 48&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 111ms&#x2F;step - loss: 0.0460 - accuracy: 0.9860 - val_loss: 0.2665 - val_accuracy: 0.9268</span><br><span class="line">Epoch 49&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 118ms&#x2F;step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.3237 - val_accuracy: 0.9218</span><br><span class="line">Epoch 50&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 112ms&#x2F;step - loss: 0.0452 - accuracy: 0.9864 - val_loss: 0.3150 - val_accuracy: 0.9161</span><br></pre></td></tr></table></figure>

<h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CNN.save(<span class="string">&#x27;traffic_sign_weights.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<h2 id="模型效果评估"><a href="#模型效果评估" class="headerlink" title="模型效果评估"></a>模型效果评估</h2><p><img src="/2021/03/15/cnn-sign-detection/confusion_mask.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">score = CNN.evaluate(X_test_gray_norm, y_test)</span><br><span class="line">print(<span class="string">&#x27;Test Accuracy: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(score[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<p>395/395 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.9123<br>Test Accuracy: 0.9122723937034607</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history.history.keys()</span><br></pre></td></tr></table></figure>

<p>dict_keys([‘loss’, ‘accuracy’, ‘val_loss’, ‘val_accuracy’])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracy = history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">val_accuracy = history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>对训练过程中的Loss进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(accuracy))</span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;ro&#x27;</span>, label = <span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;r&#x27;</span>, label = <span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training, And Validation Loss&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/loss.png"></p>
<p>对训练过程中的Accuracy进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(accuracy))</span><br><span class="line">plt.plot(epochs, accuracy, <span class="string">&#x27;ro&#x27;</span>, label = <span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_accuracy, <span class="string">&#x27;r&#x27;</span>, label = <span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training, And Validation Accuracy&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/accuracy.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predicted_classes = CNN.predict_classes(X_test_gray_norm)</span><br><span class="line">y_true = y_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(y_true, predicted_classes)</span><br><span class="line">plt.figure(figsize = (<span class="number">25</span>, <span class="number">25</span>))</span><br><span class="line">sns.heatmap(cm, annot = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/matrix.png"></p>
<p>在测试集上验证网络效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L = <span class="number">5</span></span><br><span class="line">W = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(L, W, figsize = (<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">axes = axes.ravel()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>, L*W):</span><br><span class="line">    axes[i].imshow(X_test[i])</span><br><span class="line">    axes[i].set_title(<span class="string">&#x27;Prediction = &#123;&#125;\n True = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(predicted_classes[i], y_true[i]))</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplots_adjust(wspace = <span class="number">1</span>) </span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/test_result.png"></p>
<h1 id="实际检测效果验证"><a href="#实际检测效果验证" class="headerlink" title="实际检测效果验证"></a>实际检测效果验证</h1><p>在网上找了两张图片(限速标牌和Stop标牌)试验， 图像中冗余内容越多，检测效果越差。当标牌充满图像时，检测效果还是不错的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> img_to_array</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_to_array</span>(<span class="params">path</span>):</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(path)</span><br><span class="line">    image = image.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    image = img_to_array(image)</span><br><span class="line"></span><br><span class="line">    image = image.reshape([<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prediction</span>(<span class="params">path</span>):</span></span><br><span class="line"></span><br><span class="line">    img = image_to_array(path)</span><br><span class="line"></span><br><span class="line">    plt.imshow(img.squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    img_gray = np.<span class="built_in">sum</span>(img / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    img_norm = (img_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">    print(img_norm.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(img_norm.squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    detection_model = load_model(<span class="string">&#x27;traffic_sign_weights.h5&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    predicted_classes = detection_model.predict_classes(img_norm)</span><br><span class="line"></span><br><span class="line">    print(predicted_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">prediction(<span class="string">&quot;./stop.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">prediction(<span class="string">&quot;./speed_limit_60.jpg&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输入图像如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/stop.jpg"></p>
<p>检测输出内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">14</span>]</span><br></pre></td></tr></table></figure>
<p>14对应Stop Sign的类型。</p>
<p><img src="/2021/03/15/cnn-sign-detection/speed_limit_60.jpg"></p>
<p>检测输出内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<p>3对应60KM/h的限速。</p>
<h1 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h1><p>Coursera - Traffic Sign Classification Using Deep Learning in Python/Keras</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>环境感知</tag>
        <tag>自动驾驶</tag>
      </tags>
  </entry>
  <entry>
    <title>低速自动驾驶车辆的定位与建图</title>
    <url>/2021/03/25/localization-mapping-zhixingzhe/</url>
    <content><![CDATA[<p><img src="/2021/03/25/localization-mapping-zhixingzhe/1.png"></p>
<p>本文是高翔博士关于低速自动驾驶定位建图的相关介绍。</p>
<span id="more"></span>

<p><img src="/2021/03/25/localization-mapping-zhixingzhe/2.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/3.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/4.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/5.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/6.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/7.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/8.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/9.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/10.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/11.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/12.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/13.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/14.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/15.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/16.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/17.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/18.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/19.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/20.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/21.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/22.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/23.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/24.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/25.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/26.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/27.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/28.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/29.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/30.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/31.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/32.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/33.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/34.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/35.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/36.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/37.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/38.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/39.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/40.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/41.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/42.png"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>高精地图</tag>
        <tag>Mobileye</tag>
      </tags>
  </entry>
  <entry>
    <title>Mobileye REM地图</title>
    <url>/2021/03/14/mobileye-rem-map-md/</url>
    <content><![CDATA[<h1 id="为什么需要高精地图"><a href="#为什么需要高精地图" class="headerlink" title="为什么需要高精地图"></a>为什么需要高精地图</h1><p><img src="/2021/03/14/mobileye-rem-map-md/rem_map.png" alt="Mobileye Rem Map"></p>
<p>理论上来讲，可以在车载系统检测和获取所有道路信息(可行驶路径、车道优先级、红绿灯与车道的关联关系、车道与人行横道与红绿灯的关系等)，但是目前的AI能力无法保证实现很高的MTBF(Mean Time Between Failures, 平均无故障时间)，所以需要提前把这些信息都准备好。</p>
<span id="more"></span>

<p><img src="/2021/03/14/mobileye-rem-map-md/hdmap_motivation.png" alt="Motivation Behind HDMap"></p>
<h1 id="高精地图的挑战"><a href="#高精地图的挑战" class="headerlink" title="高精地图的挑战"></a>高精地图的挑战</h1><h2 id="规模化-Scale"><a href="#规模化-Scale" class="headerlink" title="规模化-Scale"></a>规模化-Scale</h2><p>如果自动驾驶车辆只在一个区域、一个城市、或者几个城市运营，那就不存在规模化的问题。但是2025年之后，自动驾驶会在消费者层面全面落地，用户需要驾车到任意想去的地方，在这种场景下，Scale是一个无法规避的问题。</p>
<h2 id="鲜度-Fresh"><a href="#鲜度-Fresh" class="headerlink" title="鲜度-Fresh"></a>鲜度-Fresh</h2><p>理想情况下，地图是在实时更新的。当物理环境发生变化时，需要实时反映到地图上。月级更新、甚至天级更新都是不够的，我们需要做到分钟级，甚至更短。</p>
<h2 id="精度-Accuracy"><a href="#精度-Accuracy" class="headerlink" title="精度-Accuracy"></a>精度-Accuracy</h2><p>车载系统(OnBoard System)检测的车辆和行人需要与高精地图(High Definiation Map)实现厘米级精度的匹配，因此地图的精度至关重要。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_challange.png" alt="高精地图挑战"></p>
<h1 id="通用高精地图制作方法的缺陷"><a href="#通用高精地图制作方法的缺陷" class="headerlink" title="通用高精地图制作方法的缺陷"></a>通用高精地图制作方法的缺陷</h1><p><img src="/2021/03/14/mobileye-rem-map-md/common_approach_map.png" alt="高精地图通用制作方法"></p>
<h2 id="全局坐标系下厘米级精度不是必需的"><a href="#全局坐标系下厘米级精度不是必需的" class="headerlink" title="全局坐标系下厘米级精度不是必需的"></a>全局坐标系下厘米级精度不是必需的</h2><p>AV车辆行驶过程中只关注周围几百米范围即可，所以只要这个范围内的足够准确即可。至于几公里之外的全局精度，Who Care…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_geometric.png"></p>
<h2 id="语义层数据生产难以自动化"><a href="#语义层数据生产难以自动化" class="headerlink" title="语义层数据生产难以自动化"></a>语义层数据生产难以自动化</h2><p><img src="/2021/03/14/mobileye-rem-map-md/semantic_map.png"></p>
<p>如下图所示，没有车道线的双向车道，单从图像观察，难以识别它的Drive Path。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/drivable_path.png"></p>
<p>如下图所示，转向规则千奇百怪：禁止红灯右转，完全停车后允许红灯右转，绿灯禁止左转，绿灯Yield后允许左转…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/priority_map.png"></p>
<p>如下图所示，红绿灯异常复杂，识别车道、人行横道与红绿灯的关联关系难度很大…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_association.png"></p>
<p>如下图所示，除非地图可以表达所有的3D要素，否则很难自动化的计算出车道的最优Stop/Yield Point。但是表达所有的3D信息对于地图来说又是不现实的…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/stop_point.png"></p>
<p>影响车辆行驶速度的因素有很多，道路几何、限速、文化等，难以量化，但它对Smooth Driving体验至关重要…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/smooth_driving.png"></p>
<h1 id="Mobileye如何解决这些问题"><a href="#Mobileye如何解决这些问题" class="headerlink" title="Mobileye如何解决这些问题"></a>Mobileye如何解决这些问题</h1><p>scalability依赖众包数据生成Millions Map Agents；Accuracy不是全局的Accuracy，而是局部的Accuracy，相对于道路上的静态元素位置。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/av_map.png"></p>
<p>REM的处理流程如下，首先从成百上千辆车获取检测信息(没有使用差分GPS，而是使用了普通的GPS)，这些数据传送到云端；每辆车Detection的角度不同，由于遮挡等原因，每辆车检测的landmark也有差异，将这些数据进行Alignment处理，生成高精度的地图数据；最后，Modeling And Semantics负责生成地图的语义数据。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/rem_process.png"></p>
<h2 id="Harvesting"><a href="#Harvesting" class="headerlink" title="Harvesting"></a>Harvesting</h2><p>下图中黄色的框是车辆检测的landmarks和lane marks，同时车辆会尝试检测driving path等语义信息，一辆车可能检测不准确，但是成百上千的过路车辆会让检测结果越来越好。</p>
<p>Mobileye Harvesting的数据量为10K/公里，这些检测的数据会被发送到云端。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/harvesting.png"></p>
<h2 id="Aligning-Drives"><a href="#Aligning-Drives" class="headerlink" title="Aligning Drives"></a>Aligning Drives</h2><p>检测每个RSD中每个元素的6D Pose，然后对齐相同位置的元素，得到厘米度精度的driving path等信息。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_align_drive.png"></p>
<p>由于GPS存在误差，每个车辆检测的道路元素位置都存在噪声，所以只依靠简单的位置求均值是不可行的。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/align_noise.png"></p>
<p>Align之后可以明显的看到两条Driving Path(蓝色)和两侧的道路边界(红色)。对齐的过程是靠几何运算进行。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/path_align.png"></p>
<p>仅仅靠聚类(Clustering)和Spline Fiting得到下图右上角的结果，这个结果不是特别理想。后来通过神经网络生成高精度地图，效果好了很多。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/model_process.png"></p>
<h1 id="为什么语义理解离不开众包"><a href="#为什么语义理解离不开众包" class="headerlink" title="为什么语义理解离不开众包"></a>为什么语义理解离不开众包</h1><p>如下左图所示，通过众包数据可以在没有Lane Marking的道路上获取Driving Path。</p>
<p>如下右图所示，众包数据提供了复杂场景下的所有可通行路径。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_driving_path.png"></p>
<p>如下图所示，通过众包数据可以获得红绿灯与车道的关联关系、Yield Sign的Stop Point、Crosswalk与红绿灯的关联关系等。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_assocation.png"></p>
<p>如下左图所示，通过检测哪个Drive Path的Stop Point比较多，我们可以从众包数据中获取到没有Traffic Sign情况下各个道路的路权优先级。</p>
<p>如下中图所示，我们可以从众包数据学习到在路口其它司机的停车位置。</p>
<p>如下右图所示，从众包数据可以学习到，在无保护左转的场景下车辆的Stop Point。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_more_info.png"></p>
<p>众包数据是获得各个道路Common Speed的唯一高效的方法，Common Speed提供了当道路没有车辆时候AV车的目标行驶速度。采用这种方法可以使得无论在哪个国家、地区，或者不同的道路类型，AV车都可以自然的融入车流。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_speed.png"></p>
<h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>到目前为止，Mobileye与超过6家汽车制造厂商合作，每天可以覆盖800万公里的路网更新。预计到2024年，每天覆盖的路网会达到10亿公里。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/mobileye_situation.png"></p>
<p><strong>说明</strong>： 本文所有内容都来源于Mobileye CEO Amnon Shashua教授在2021 CES的分享。</p>
<p>YouTube链接：<br><a href="https://www.youtube.com/watch?v=B7YNj66GxRA&amp;t=301s">https://www.youtube.com/watch?v=B7YNj66GxRA&amp;t=301s</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>高精地图</tag>
        <tag>Mobileye</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN预测行人运动轨迹</title>
    <url>/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/</url>
    <content><![CDATA[<blockquote>
<p>最近在研究论文-Social LSTM: Human Trajectory Prediction in Crowded Spaces, 先从最基本的RNN模型入手看看效果。</p>
<p>本文代码已经上传到Github:<br><a href="https://github.com/YoungTimes/GNN/blob/master/Social-LSTM/train.py">https://github.com/YoungTimes/GNN/blob/master/Social-LSTM/train.py</a></p>
</blockquote>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/Screenshot_from_2021-03-28_22-40-11.png"></p>
<span id="more"></span>

<h1 id="行人轨迹数据集"><a href="#行人轨迹数据集" class="headerlink" title="行人轨迹数据集"></a>行人轨迹数据集</h1><p>数据集来源自[1]，每个数据目录包含一个pixel_pos.csv文件，它的文件格式如下:</p>
<p>pixel_pose.csv包含4行，它的列数是所有行人轨迹点的数量。</p>
<p>第一行是所有的Frame Number；</p>
<p>第二行是所有行人的ID；</p>
<p>第三行是所有的y坐标；</p>
<p>第四行是所有的x坐标。</p>
<p>先看下数据集的内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">directory = <span class="string">&quot;./data/eth/univ&quot;</span></span><br><span class="line"></span><br><span class="line">file_path = os.path.join(directory, <span class="string">&#x27;pixel_pos.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data = np.genfromtxt(file_path, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the number of pedestrians in the current dataset</span></span><br><span class="line">pedIDs = np.unique(data[<span class="number">1</span>, :])</span><br><span class="line">numPeds = np.size(pedIDs)</span><br><span class="line"></span><br><span class="line">pedIndexLookup = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ped_index, ped_id <span class="keyword">in</span> <span class="built_in">enumerate</span>(pedIDs):</span><br><span class="line">    pedIndexLookup[ped_id] = ped_index</span><br><span class="line"></span><br><span class="line">frameIDs = np.unique(data[<span class="number">0</span>, :])</span><br><span class="line">numFrames = np.size(frameIDs)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;number of pedestrians is: &#123;&#125;, number of frames is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(numPeds, numFrames))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将data/eth/univ数据场景的数据可视化如下图所示，每个不同颜色的圆点都是一个运动的行人。</p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/315ft-jpfdw.gif"></p>
<h1 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h1><p>RNN模型参考了[1][2][3]，大概分为三层: 输入层(Embedding)、RNN层(LSTM/GRU)、输出层。</p>
<p>Embedding层将坐标(x,y)嵌入到64维的向量空间；输出层输出每个预测点的二维高斯分布参数(包含5个参数:mux, muy, sx, sy, corr), 时刻t的预测坐标点最后通过$({x}^{t}, {y}^{t}) \sim \mathcal{N}(\mu_i^t, \sigma_i^t, \rho_i^t)$获取。</p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-8f7cdefd76e7c3a3bc9260eee986f828_720w.jpg" alt="TensorFlow Tutorial-RNN文本生成"></p>
<p>模型代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">args</span>):</span></span><br><span class="line">    output_size = <span class="number">5</span></span><br><span class="line">    model = tf.keras.Sequential([</span><br><span class="line">        tf.keras.layers.Dense(args.embedding_size, activation = tf.keras.activations.relu,</span><br><span class="line">            batch_input_shape = [args.batch_size, <span class="literal">None</span>, <span class="number">2</span>]),</span><br><span class="line">        tf.keras.layers.GRU(args.rnn_size,</span><br><span class="line">                            return_sequences=<span class="literal">True</span>,</span><br><span class="line">                            stateful=<span class="literal">True</span>,</span><br><span class="line">                            recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>),</span><br><span class="line">        tf.keras.layers.Dense(output_size)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><h2 id="Loss函数"><a href="#Loss函数" class="headerlink" title="Loss函数"></a>Loss函数</h2><p>模型的Loss函数为所有待预测轨迹点的负对数似然估计之和，模型训练的过程就是最小化所有待预测轨迹的Loss的过程。</p>
<p>$$<br>L^{i}=-\sum_{t=T_{obs}+1}^{T_{pred}} \log \left(\mathbb{P}\left(x_{t}^{i}, y_{t}^{i} \mid \sigma_{t}^{i}, \mu_{t}^{i}, \rho_{t}^{i}\right)\right)<br>$$</p>
<p>loss代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lossfunc</span>(<span class="params">z_mux, z_muy, z_sx, z_sy, z_corr, x_data, y_data</span>):</span></span><br><span class="line"></span><br><span class="line">    result0 = tf_2d_normal(x_data, y_data, z_mux, z_muy, z_sx, z_sy, z_corr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For numerical stability purposes</span></span><br><span class="line">    epsilon = <span class="number">1e-20</span></span><br><span class="line"></span><br><span class="line">    result1 = -tf.math.log(tf.math.maximum(result0, epsilon))  <span class="comment"># Numerical stability</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sum up all log probabilities for each data point</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum(result1)</span><br></pre></td></tr></table></figure>
<h2 id="轨迹预测效果的Metric"><a href="#轨迹预测效果的Metric" class="headerlink" title="轨迹预测效果的Metric"></a>轨迹预测效果的Metric</h2><p>轨迹预测效果的衡量指标为:<strong>Average Displacement Error</strong>和<strong>Final Displacement Error</strong>。</p>
<p>Average Displacement Error = 所有预测轨迹点与GroundTruth对应轨迹点的空间距离之和/预测轨迹点个数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mean_error</span>(<span class="params">pred_traj, true_traj, observed_length</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The data structure to store all errors</span></span><br><span class="line">    error = np.zeros(<span class="built_in">len</span>(true_traj) - observed_length)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(observed_length, <span class="built_in">len</span>(true_traj)):</span><br><span class="line">        <span class="comment"># The predicted position</span></span><br><span class="line">        pred_pos = pred_traj[i, :]</span><br><span class="line">        <span class="comment"># The true position</span></span><br><span class="line">        true_pos = true_traj[i, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The euclidean distance is the error</span></span><br><span class="line">        error[i-observed_length] = np.linalg.norm(true_pos - pred_pos)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.mean(error)</span><br></pre></td></tr></table></figure>

<p>Final Displacement Error = 最后一个预测轨迹点与GroundTruth对应轨迹点的空间距离；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_final_error</span>(<span class="params">pred_traj, true_traj</span>):</span></span><br><span class="line">    error = np.linalg.norm(pred_traj[-<span class="number">1</span>, :] - true_traj[-<span class="number">1</span>, :])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> error</span><br></pre></td></tr></table></figure>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">args</span>):</span></span><br><span class="line">    datasets = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    data_loader = DataLoader(args.batch_size, args.seq_length, datasets, forcePreProcess=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    model = build_model(args)</span><br><span class="line"></span><br><span class="line">    optimizer = tf.keras.optimizers.RMSprop(args.learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(args.num_epochs):</span><br><span class="line">        data_loader.reset_batch_pointer()</span><br><span class="line">        model.reset_states()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(data_loader.num_batches):</span><br><span class="line">            start = time.time()</span><br><span class="line"></span><br><span class="line">             x, y = data_loader.next_batch()</span><br><span class="line"></span><br><span class="line">            base_pos = np.array([[e_x[<span class="number">0</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(e_x))] <span class="keyword">for</span> e_x <span class="keyword">in</span> x])</span><br><span class="line"></span><br><span class="line">            x_offset = x - base_pos</span><br><span class="line">            y_offset = y - base_pos</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                tensor_x = tf.convert_to_tensor(x_offset, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">                logits = model(tensor_x)</span><br><span class="line"></span><br><span class="line">                [o_mux, o_muy, o_sx, o_sy, o_corr] = get_coef(logits)</span><br><span class="line"></span><br><span class="line">                tensor_y = tf.convert_to_tensor(y_offset, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">                [x_data, y_data] = tf.split(tensor_y, <span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Compute the loss function</span></span><br><span class="line">                loss = get_lossfunc(o_mux, o_muy, o_sx, o_sy, o_corr, x_data, y_data)</span><br><span class="line"></span><br><span class="line">                mean_error, final_error = calc_prediction_error(o_mux, o_muy, o_sx, o_sy, o_corr, tensor_y, args)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Compute the cost</span></span><br><span class="line">                loss = tf.math.divide(loss, (args.batch_size * args.seq_length))</span><br><span class="line"></span><br><span class="line">                grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">                optimizer.lr.assign(args.learning_rate * (args.decay_rate ** e))</span><br><span class="line">                optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.trainable_variables))</span><br></pre></td></tr></table></figure>

<p>training过程:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">5195&#x2F;5200 (epoch 199), train\_loss &#x3D; -5.898, time&#x2F;batch &#x3D; 0.583, mean error &#x3D; 0.019682016324174278, final\_error &#x3D; 0.019766580550931393</span><br><span class="line">5196&#x2F;5200 (epoch 199), train\_loss &#x3D; -5.707, time&#x2F;batch &#x3D; 0.583, mean error &#x3D; 0.01821616107675557, final\_error &#x3D; 0.018569258209317922</span><br><span class="line">5197&#x2F;5200 (epoch 199), train\_loss &#x3D; -5.726, time&#x2F;batch &#x3D; 0.581, mean error &#x3D; 0.021631291888964673, final\_error &#x3D; 0.024468516283668577</span><br><span class="line">5198&#x2F;5200 (epoch 199), train\_loss &#x3D; -6.308, time&#x2F;batch &#x3D; 0.595, mean error &#x3D; 0.02178817719841997, final\_error &#x3D; 0.024148114868439735</span><br><span class="line">5199&#x2F;5200 (epoch 199), train\_loss &#x3D; -2.924, time&#x2F;batch &#x3D; 0.603, mean error &#x3D; 0.035233428867844245, final\_error &#x3D; 0.036289180340245364</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-79fae3377250006a5521fb82983b7e62_720w.jpg"></p>
<p>最后放一些测试效果的图吧，绿色是Ground Truth Trajectory，红色是Prediction Trajectory。</p>
<p>最终效果中，预测Trajectory与Ground Truth Trajectory的绝对偏差并不大，因为行人的运动速度通常不会太快。但最终的预测趋势与真实的运动意图个人感觉还比较大，不确定是模型的问题，还是行人运动预测难度比较大，单凭LSTM很难搞定。后面再尝试下Social LSTM，看看效果。</p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-2c6eca62a722134c41825e200e8bfe51_720w.jpg"></p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-7464b730afec00a99d8d4cc9738819a1_720w.jpg"></p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-a0b714e1f20cc24717b90d795fc605ea_720w.jpg"></p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-fd8e403ac5d3c4a411d8e2d2f497368e_720w.jpg"></p>
<h1 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h1><p>1.<a href="https://github.com/xuerenlv/social-lstm-tf">https://github.com/xuerenlv/social-lstm-tf</a><br>2.<a href="https://github.com/quancore/social-lstm">https://github.com/quancore/social-lstm</a><br>3.<a href="https://www.tensorflow.org/tutorials/text/text/_generation?hl=zh-cn">https://www.tensorflow.org/tutorials/text/text\_generation?hl=zh-cn</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>RNN</tag>
        <tag>行人轨迹预测</tag>
        <tag>轨迹预测</tag>
      </tags>
  </entry>
  <entry>
    <title>Waymo-自动驾驶长尾问题挑战</title>
    <url>/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/</url>
    <content><![CDATA[<p>尽管Waymo已经在开放道路上积累超过10 Million Miles，Waymo的工程师们仍然发现有层出不穷的新自动驾驶场景待解决。</p>
<h1 id="自动驾驶长尾场景举例"><a href="#自动驾驶长尾场景举例" class="headerlink" title="自动驾驶长尾场景举例"></a>自动驾驶长尾场景举例</h1><p><strong>场景一</strong>：一个骑自行车的人手中拿着一个Stop Sign标识牌。我们不知道它何时会举起标识牌。无人车必须理解这种场景，即使他举起了Stop Sign标识牌，自动驾驶汽车也不应该停下来。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-07-22-03-14-1024x574.png"></p>
<span id="more"></span>

<p><strong>场景二:</strong> 迎面而来的车辆上装载的塑料管子撒了一地，自动驾驶汽车必须学会应对这种突发情况，并且避开它们对无人车行驶的影响。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-07-22-08-44-1024x370.png"></p>
<p><strong>场景三：</strong>由于道路施工等因素，路面布满锥桶。无人车必须正确识别这些场景，在布满路面锥桶的场景下实现合理驾驶。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_10s.gif"></p>
<p><strong>场景四：</strong>路口绿灯，无人车拥有路权，虽然我们的无人车先到达路口，但必须为稍后到达的特种车辆让行。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_8s.gif"></p>
<p><strong>场景五：</strong> 路口绿灯，无人车准备左转，遇到闯红灯高速通过的社会车辆，无人车需要识别这种场景，并及时停车避让违规车辆。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_5s-1.gif"></p>
<h1 id="自动驾驶核心模块-Perception-Prediction和Planning"><a href="#自动驾驶核心模块-Perception-Prediction和Planning" class="headerlink" title="自动驾驶核心模块-Perception, Prediction和Planning"></a>自动驾驶核心模块-Perception, Prediction和Planning</h1><p>Perception、Prediction和Planning模块是自动驾驶的核心模块，每个模块都存在巨大的挑战。</p>
<h2 id="Perception"><a href="#Perception" class="headerlink" title="Perception"></a>Perception</h2><p>Perception输入：传感器(激光雷达)输入信息以及场景的先验信息。</p>
<p>Perception输出：道路交通对象(行人、车辆等)，对道路场景的语义分割和理解。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-08-56-17.png"></p>
<p>Perception本身是一个非常复杂、高难度的问题，它必须能够识别各种形态各异、不同种类的对象。比如下左一图，一群穿着恐龙服的行人，感知必须能够正确识别它们。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-03-50-1024x338.png"></p>
<p>相同的物体在不同的时间、不同的季节它们的外观表现也会有很大的差异，这会对Perception带来巨大挑战。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-12-28-1024x311.png"></p>
<p>各种复杂场景的分割理解难度极高。如下图左一：一个搬着箱子的人；下图左三：骑马的人。Perception必须能够正确的分割识别这些场景，而不会因为遮挡导致出现识别的错误。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-16-52-1024x300.png"></p>
<h2 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h2><p>Perception对检测到的物体进行下一步行为的预测，以辅助自动驾驶车辆进行合理的行为决策。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-29-49-1024x259.png"></p>
<p>Perception要考虑物体的历史行为，比如车辆不会在短时间内实现90度的转弯，因此我们可以假设车辆在短时间内仍然按照当前的朝向和速度前进；要对场景有更高语义层面的理解；要能够关注到不同对象的属性差异和视觉线索，比如车辆大概率是会在车道上行驶上，行人会走斑马线，车辆的朝向能够大概率反应它的意图，如果行人做出停车的手势，大概率是要过马路；要能够解决待预测物体与其它物体的行为交互。</p>
<p>如下图所示，路边有一辆静止的车辆，骑自行车的人在靠近静止车辆时，会侵入无人车车道。Perception模块需要正确理解这些场景，并生成合理的预测曲线。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_5s.gif"></p>
<p>如何能够准确的预测社会车辆的行为仍然是一个存在巨大挑战的开放性问题。</p>
<h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>Planning是Decision Making Machine，它基于Perception和Prediction的输出，规划车辆的行为，并输出Control模块，控制车辆的加减速、刹车等行为。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-51-50.png"></p>
<p>Planning首要考虑的是安全(safe)，其次要考虑驾乘的舒适性(comfortable)，再次要能够与其它交通参与者正确交互，最后要保证乘客送达目的地。如何能够满足这些条件实现良好的Planning效果仍然是一个开放性的问题。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_10s-1-2.gif"></p>
<h1 id="大规模机器学习技术-Machine-Learning-At-Scale"><a href="#大规模机器学习技术-Machine-Learning-At-Scale" class="headerlink" title="大规模机器学习技术(Machine Learning At Scale)"></a>大规模机器学习技术(Machine Learning At Scale)</h1><p>Machine Learning是解决自动驾驶长尾问题的一种有效工具。利用Machine Learning技术可以实现从数据采集、标注、训练、车端部署的闭环循环流程，从而实现Case的不断积累，模型的不断完善。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-22-39-1024x426.png"></p>
<h2 id="Automated-Machine-Learning技术"><a href="#Automated-Machine-Learning技术" class="headerlink" title="Automated Machine Learning技术"></a>Automated Machine Learning技术</h2><p>Waymo使用了Automated Machine Learning技术生成和优化针对无人车的数据模型，极大提升了模型训练的效率。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-37-13-1024x530.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-39-29-1024x521.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-40-27-1024x425.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-43-04-1024x523.png"></p>
<h2 id="机器学习技术的局限-Limits-Of-Machine-Learning"><a href="#机器学习技术的局限-Limits-Of-Machine-Learning" class="headerlink" title="机器学习技术的局限(Limits Of Machine Learning)"></a>机器学习技术的局限(Limits Of Machine Learning)</h2><p>机器学习模型不能解决所有的问题，但我们需要的是一个安全的自动驾驶系统，所以必须有其它措施来补充ML的不足。</p>
<p>首先可以借助于冗余互补的传感器辅助解决这个问题。车辆同时配备了视觉、Lidar、Radar系统，各个系统彼此独立，相互补充，以最大限度保证无人车不会缺失任何信息。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-48-43-1024x599.png"></p>
<p>其次，我们可以采用ML和Non-ML混合系统，利用专家系统来弥补ML的不足。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-56-06.png"></p>
<h1 id="大规模的测试技术-Large-Scale-Testing"><a href="#大规模的测试技术-Large-Scale-Testing" class="headerlink" title="大规模的测试技术(Large Scale Testing)"></a>大规模的测试技术(Large Scale Testing)</h1><p>首先Waymo有庞大的自动驾驶车队，可以支撑大规模的测试。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-16-08-1024x494.png"></p>
<p>有些场景在实际道路上出现的概率很低，为了测试验证这些低频问题，需要自己构建场景，进行结构化测试。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-19-08-1024x471.png"></p>
<p>仿真是一种重要的验证测试手段，可以轻量级安全的构造各种各样的测试场景。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-22-53.png"></p>
<p>自动驾驶仿真必须能够真实模拟车辆和行人的行为。这仅仅依靠简单的规则模型是不够的，我们需要更加复杂的模型，Waymo使用一种Mid-2-Mid的Drive Agent机器学习模型，它接收定位、感知等信息，输出更加拟人化的运动规划。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-40-00-1024x269.png"></p>
<p>Waymo提出的ChauffeurNet将Map、交通规则、道路环境等信息转化为图像信息，从而可以最大限度的利用比较成熟的机器学习模型，最终输出Agent的Trajectory。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-46-38-1024x543.png"></p>
<p>ChauffeurNet可以解决大部分简单场景下的Prediction和Planning问题。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/unnamed.gif"></p>
<p>场景中红色的拖尾是Agent的历史轨迹，绿色是未来2s的预测轨迹。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/unnamed-1.gif"></p>
<p>主车成功的通过路边静止车辆的场景</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/unnamed-2.gif"></p>
<p>主车遇到缓慢前行的车辆后减速</p>
<p>当然ChauffeurNet也有其局限性，比如以下复杂场景目前还不能很好的处理。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_406x306_10s.gif"></p>
<p>主车由于视距遮挡，直接冲出了路口</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_406x306_8s.gif"></p>
<p>车辆没有成功完成掉头操作</p>
<h1 id="机器学习难以覆盖的长尾问题挑战"><a href="#机器学习难以覆盖的长尾问题挑战" class="headerlink" title="机器学习难以覆盖的长尾问题挑战"></a>机器学习难以覆盖的长尾问题挑战</h1><p>对自动驾驶测试来讲，最大的挑战在于很难收集到所有Corner Case。如下图所示，是人类驾驶行为分布，要经过非常长时间的积累才能得到一些Corner的驾驶行为Case。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-15-46-53.png"></p>
<p>在自动驾驶网络的神经网络模型中，可能有上千万的参数，如果Corner Case的样本数量太少，就难以保证网络模型能够学会这些Corner场景。</p>
<p>在神经网络模型覆盖长尾Case前，如何来解决长尾Case呢？专家系统是一个选择。专家系统融入专业的知识，通过小批量的样本就可以获得效果比较好的参数。</p>
<p>比如我们计划得到实现一个轨迹优化机器学习模型，在基于运动控制理论和一系列的约束设计好专家模型之后，通过采集历史车辆轨迹，我们就可以调整参数最小化Cost的方法，使得专家系统的轨迹输出尽可能的逼近人类驾驶轨迹。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-15-53-40-1024x576.png"></p>
<p>轨迹优化专家系统的另一种模型是Inverse Reinforcement Learning技术，通过历史驾驶轨迹训练模型参数，使得它的输出尽可能的逼近预期效果。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-02-30.png"></p>
<p>如下图所示，红色的主车，蓝色的是社会车辆。左图的社会车辆更加保守，右侧的社会车辆更加激进。用保守的轨迹训练出的模型表现就趋于保守，用激进的轨迹训练出的模型表现就趋于激进。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x222_17s.gif"></p>
<h1 id="Smart-Agent对于自动驾驶规模化不可或缺"><a href="#Smart-Agent对于自动驾驶规模化不可或缺" class="headerlink" title="Smart Agent对于自动驾驶规模化不可或缺"></a>Smart Agent对于自动驾驶规模化不可或缺</h1><p>不管是专家系统，还是神经网络，它们都在努力模拟人的驾驶行为，使Agent变得聪明起来，聪明的Agent可以辅助自动驾驶技术快速规模化。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-19-28-1024x319.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-21-58-1024x433.png"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>waymo</tag>
        <tag>自动驾驶长尾问题</tag>
      </tags>
  </entry>
  <entry>
    <title>未知环境下的Lidar概率占位栅格图(Occupancy Grid Map) Python代码实现</title>
    <url>/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>前面文章&lt;&lt;自动驾驶Mapping-占位栅格图(Occupancy Grid Map)&gt;&gt;中介绍了概率占位栅格地图(Probabilistic Occupancy Grid)的原理，并推导了如何利用贝叶斯理论(Bayes Theorem)更新生成概率占位栅格地图。下面看看如何用Python代码实现未知环境中的运动车辆上安装的激光雷达(lidar)生成概率占位栅格图。</p>
<h4 id="１、构建环境地图和车辆运动模型"><a href="#１、构建环境地图和车辆运动模型" class="headerlink" title="１、构建环境地图和车辆运动模型"></a>１、构建环境地图和车辆运动模型</h4><p>在生成栅格地图之前，首先需要构造一个用于车辆运动的环境地图(这个地图是用于仿真的真值，对于车辆来说是未知的环境)。我们用０和１值来构造Ｍ*N的环境地图，０表示可行驶区域，１表示占用区域。</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">M = <span class="number">50</span></span><br><span class="line">N = <span class="number">60</span></span><br><span class="line">true_map = np.zeros((M, N))</span><br><span class="line">true_map[<span class="number">0</span>:<span class="number">10</span>, <span class="number">0</span>:<span class="number">10</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">30</span>:<span class="number">35</span>, <span class="number">40</span>:<span class="number">45</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">3</span>:<span class="number">6</span>,<span class="number">40</span>:<span class="number">60</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">20</span>:<span class="number">30</span>,<span class="number">25</span>:<span class="number">29</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">40</span>:<span class="number">50</span>,<span class="number">5</span>:<span class="number">25</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>然后构建车辆的运动模型。这里实现了一个简单的运动模型：车辆遇到障碍物或者到达地图边界之前，沿一个方向一直行驶；遇到障碍物或者到达地图边界之后，调整方向继续行驶。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initializing the robot&#x27;s location.</span></span><br><span class="line">x_0 = [<span class="number">30</span>, <span class="number">30</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># The sequence of robot motions.</span></span><br><span class="line">u = np.array([[<span class="number">3</span>, <span class="number">0</span>, -<span class="number">3</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, -<span class="number">3</span>]])</span><br><span class="line">u_i = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the vector of states for our simulation.</span></span><br><span class="line">x = np.zeros((<span class="number">3</span>, <span class="built_in">len</span>(time_steps)))</span><br><span class="line">x[:, <span class="number">0</span>] = x_0</span><br><span class="line"> </span><br><span class="line"><span class="keyword">while</span>(Some Conditon...) :</span><br><span class="line">　　<span class="comment"># Perform robot motion.</span></span><br><span class="line">    move = np.add(x[<span class="number">0</span>:<span class="number">2</span>, t-<span class="number">1</span>], u[:, u_i]) </span><br><span class="line">    <span class="comment"># If we hit the map boundaries, or a collision would occur, remain still.</span></span><br><span class="line">    <span class="keyword">if</span> (move[<span class="number">0</span>] &gt;= M - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &gt;= N - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">0</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> true_map[<span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">0</span>])), <span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">1</span>]))] == <span class="number">1</span>:</span><br><span class="line">        x[:, t] = x[:, t-<span class="number">1</span>]</span><br><span class="line">        u_i = (u_i + <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x[<span class="number">0</span>:<span class="number">2</span>, t] = move</span><br></pre></td></tr></table></figure>

<p>车辆的运动效果如下所示：</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/i2bis-0v5o3.gif"></p>
<p>最后要构建激光雷达(Lidar)的旋转模型。这里假设在车辆运动过程中，激光雷达(lidar)以0.3/Step的速度持续旋转，对周围的环境进行扫描。</p>
<p>$$<br>x[2, t] = (x[2, t-1] + w[t]) % (2 * math.pi)<br>$$</p>
<h4 id="２、生成激光雷达-Lidar-测量数据"><a href="#２、生成激光雷达-Lidar-测量数据" class="headerlink" title="２、生成激光雷达(Lidar)测量数据"></a>２、生成激光雷达(Lidar)测量数据</h4><p>有了地图和车辆运动模型，我们看看如何生成运动车辆上的激光雷达(lidar)扫描数据。</p>
<p>首先，我们需要搞清楚激光雷达的外参和内参，并以此推导出激光雷达(lidar)在Map坐标系下的姿态(x, y, $\theta$)和激光雷达(lidar)的激光束的水平和垂直角度分布(激光束的水平和垂直角度分布跟激光雷达自身的硬件属性相关，一般可以从Lidar产品说明书中获取)。</p>
<p>其次，我们需要知道激光雷达(Lidar)的最大扫描范围，超出该范围的区域不能被当前位置的Lidar扫描到，因而是定义为未知区域。最大扫描范围其实也是跟激光雷达自身属性相关的参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parameters for the sensor model.</span></span><br><span class="line">meas_phi = np.arange(-<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.05</span>)</span><br><span class="line">rmax = <span class="number">30</span> <span class="comment"># Max beam range.</span></span><br><span class="line">alpha = <span class="number">1</span> <span class="comment"># Width of an obstacle (distance about measurement to fill in).</span></span><br><span class="line">beta = <span class="number">0.05</span> <span class="comment"># Angular width of a beam.</span></span><br></pre></td></tr></table></figure>

<p>基于已知环境地图、车辆位置、Lidar激光束分布和Lidar最大扫描范围获取Lidar扫描数据的详细的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ranges</span>(<span class="params">true_map, X, meas_phi, rmax</span>):</span></span><br><span class="line">    (M, N) = np.shape(true_map)</span><br><span class="line">    x = X[<span class="number">0</span>]</span><br><span class="line">    y = X[<span class="number">1</span>]</span><br><span class="line">    theta = X[<span class="number">2</span>]</span><br><span class="line">    meas_r = rmax * np.ones(meas_phi.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Iterate for each measurement bearing.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(meas_phi)):</span><br><span class="line">        <span class="comment"># Iterate over each unit step up to and including rmax.</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, rmax+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Determine the coordinates of the cell.</span></span><br><span class="line">            xi = <span class="built_in">int</span>(<span class="built_in">round</span>(x + r * math.cos(theta + meas_phi[i])))</span><br><span class="line">            yi = <span class="built_in">int</span>(<span class="built_in">round</span>(y + r * math.sin(theta + meas_phi[i])))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If not in the map, set measurement there and stop going further.</span></span><br><span class="line">            <span class="keyword">if</span> (xi &lt;= <span class="number">0</span> <span class="keyword">or</span> xi &gt;= M-<span class="number">1</span> <span class="keyword">or</span> yi &lt;= <span class="number">0</span> <span class="keyword">or</span> yi &gt;= N-<span class="number">1</span>):</span><br><span class="line">                meas_r[i] = r</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># If in the map, but hitting an obstacle, set the measurement range</span></span><br><span class="line">            <span class="comment"># and stop ray tracing.</span></span><br><span class="line">            <span class="keyword">elif</span> true_map[<span class="built_in">int</span>(<span class="built_in">round</span>(xi)), <span class="built_in">int</span>(<span class="built_in">round</span>(yi))] == <span class="number">1</span>:</span><br><span class="line">                meas_r[i] = r</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> meas_r</span><br></pre></td></tr></table></figure>

<h4 id="３、计算Inverse-Scanner-Model"><a href="#３、计算Inverse-Scanner-Model" class="headerlink" title="３、计算Inverse Scanner Model"></a>３、计算Inverse Scanner Model</h4><p>获取激光雷达(Lidar)的测量数据之后，下一步就是将其关联匹配到地图的Map Cell上。主要流程是：</p>
<p>1）将 Lidar bearing与Map Cell相对于传感器的方位进行最小误差匹配，得到影响当前Map Cell的激光束；</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/Screenshot-from-2020-02-01-12-38-07-1-1024x461.png"></p>
<p>匹配的代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = math.sqrt((i - x)**<span class="number">2</span> + (j - y)**<span class="number">2</span>)</span><br><span class="line">phi = (math.atan2(j - y, i - x) - theta + math.pi) % (<span class="number">2</span> * math.pi) - math.pi</span><br><span class="line">            </span><br><span class="line"><span class="comment"># Find the range measurement associated with the relative bearing.</span></span><br><span class="line">k = np.argmin(np.<span class="built_in">abs</span>(np.subtract(phi, meas_phi)))</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>计算每个Cell被占用的概率。计算完成之后，得到三种不同类型的区域：未探测区域、障碍物区域和非障碍物区域，并赋给它们不同的占用概率。这里将未探测区域的占用概率设为0.5，表示不确定是否占用；障碍物区域占用概率等于0.7，表示大概率被占用；可行驶区域占用概率0.3，表示小概率被占用。</li>
</ol>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/Screenshot-from-2020-02-01-14-59-27-1-1024x472.png"></p>
<p>完整的Inverse Scanner Model的实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inverse_scanner</span>(<span class="params">num_rows, num_cols, x, y, theta, meas_phi, meas_r, rmax, alpha, beta</span>):</span></span><br><span class="line">    m = np.zeros((M, N))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_rows):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_cols):</span><br><span class="line">            <span class="comment"># Find range and bearing relative to the input state (x, y, theta).</span></span><br><span class="line">            r = math.sqrt((i - x)**<span class="number">2</span> + (j - y)**<span class="number">2</span>)</span><br><span class="line">            phi = (math.atan2(j - y, i - x) - theta + math.pi) % (<span class="number">2</span> * math.pi) - math.pi</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Find the range measurement associated with the relative bearing.</span></span><br><span class="line">            k = np.argmin(np.<span class="built_in">abs</span>(np.subtract(phi, meas_phi)))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the range is greater than the maximum sensor range, or behind our range</span></span><br><span class="line">            <span class="comment"># measurement, or is outside of the field of view of the sensor, then no</span></span><br><span class="line">            <span class="comment"># new information is available.</span></span><br><span class="line">            <span class="keyword">if</span> (r &gt; <span class="built_in">min</span>(rmax, meas_r[k] + alpha / <span class="number">2.0</span>)) <span class="keyword">or</span> (<span class="built_in">abs</span>(phi - meas_phi[k]) &gt; beta / <span class="number">2.0</span>):</span><br><span class="line">                m[i, j] = <span class="number">0.5</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the range measurement lied within this cell, it is likely to be an object.</span></span><br><span class="line">            <span class="keyword">elif</span> (meas_r[k] &lt; rmax) <span class="keyword">and</span> (<span class="built_in">abs</span>(r - meas_r[k]) &lt; alpha / <span class="number">2.0</span>):</span><br><span class="line">                m[i, j] = <span class="number">0.7</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the cell is in front of the range measurement, it is likely to be empty.</span></span><br><span class="line">            <span class="keyword">elif</span> r &lt; meas_r[k]:</span><br><span class="line">                m[i, j] = <span class="number">0.3</span></span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> m</span><br></pre></td></tr></table></figure>

<p>Inverse Scanner Model的测量结果如下图所示：</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/cv1uj-nhp9b.gif"></p>
<h4 id="４、生成概率占位栅格地图-Probabilistic-Occupancy-Grid"><a href="#４、生成概率占位栅格地图-Probabilistic-Occupancy-Grid" class="headerlink" title="４、生成概率占位栅格地图(Probabilistic Occupancy Grid)"></a>４、生成概率占位栅格地图(Probabilistic Occupancy Grid)</h4><p>生成概率占位地图的过程就是循环对激光雷达(lidar)的测量结果应用Inverse Scanner Model，然后更新各个Map Cell的Log Odds的过程详细推导过程参见：&lt;&lt;自动驾驶Mapping-占位栅格图(Occupancy Grid Map)&gt;&gt;:</p>
<p>$l_{t, i}=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+l_{t-1, i}-l_{0, i}$</p>
<p>其中: $\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)$是Inverse Measurement Model，$l_{t-1, i}$是网格i在t-1时刻的置信度(belif)，$l_{0,i}$是Initial belief。</p>
<p>最后，将log odds还原为真实概率，得到每个网格的占位概率值。</p>
<p>$p = e^{l_{t}} / (1 + e^{1_{t}})$</p>
<p>生成概率占位地图的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">meas_rs = []</span><br><span class="line">meas_r = get_ranges(true_map, x[:, <span class="number">0</span>], meas_phi, rmax)</span><br><span class="line">meas_rs.append(meas_r)</span><br><span class="line">invmods = []</span><br><span class="line">invmod = inverse_scanner(M, N, x[<span class="number">0</span>, <span class="number">0</span>], x[<span class="number">1</span>, <span class="number">0</span>], x[<span class="number">2</span>, <span class="number">0</span>], meas_phi, meas_r, rmax, alpha, beta)</span><br><span class="line">invmods.append(invmod)</span><br><span class="line">ms = []</span><br><span class="line">ms.append(m)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Main simulation loop.</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(time_steps)):</span><br><span class="line">    <span class="comment"># Perform robot motion.</span></span><br><span class="line">    move = np.add(x[<span class="number">0</span>:<span class="number">2</span>, t-<span class="number">1</span>], u[:, u_i]) </span><br><span class="line">    <span class="comment"># If we hit the map boundaries, or a collision would occur, remain still.</span></span><br><span class="line">    <span class="keyword">if</span> (move[<span class="number">0</span>] &gt;= M - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &gt;= N - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">0</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> true_map[<span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">0</span>])), <span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">1</span>]))] == <span class="number">1</span>:</span><br><span class="line">        x[:, t] = x[:, t-<span class="number">1</span>]</span><br><span class="line">        u_i = (u_i + <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x[<span class="number">0</span>:<span class="number">2</span>, t] = move</span><br><span class="line">    x[<span class="number">2</span>, t] = (x[<span class="number">2</span>, t-<span class="number">1</span>] + w[t]) % (<span class="number">2</span> * math.pi)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Gather the measurement range data, which we will convert to occupancy probabilities</span></span><br><span class="line">    meas_r = get_ranges(true_map, x[:, t], meas_phi, rmax)</span><br><span class="line">    meas_rs.append(meas_r)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Given our range measurements and our robot location, apply inverse scanner model</span></span><br><span class="line">    invmod = inverse_scanner(M, N, x[<span class="number">0</span>, t], x[<span class="number">1</span>, t], x[<span class="number">2</span>, t], meas_phi, meas_r, rmax, alpha, beta)</span><br><span class="line">    invmods.append(invmod)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate and update the log odds of our occupancy grid, given our measured occupancy probabilities from the inverse model.</span></span><br><span class="line">    L = np.log(np.divide(invmod, np.subtract(<span class="number">1</span>, invmod))) + L - L0</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate a grid of probabilities from the log odds.</span></span><br><span class="line">    m = np.divide(np.exp(L), np.add(<span class="number">1</span>, np.exp(L)))</span><br><span class="line">    ms.append(m)</span><br></pre></td></tr></table></figure>

<p>生成概率占用地图的过程如下：</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/ut3rg-81b5y.gif"></p>
<p>最终生成的概率占用栅格地图如下图所示。可以看看它基本反应了真实的实际车辆运行环境。</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/Screenshot-from-2020-02-13-07-53-05.png"></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>Coursera自动驾驶课程：Motion Planning for Self-Driving Cars的Weekly Assignment: Occupancy Grid Generation</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>Occupancy Grid Map</tag>
        <tag>概率占位栅格地图</tag>
        <tag>自动驾驶Planning</tag>
        <tag>自动驾驶路径规划</tag>
      </tags>
  </entry>
  <entry>
    <title>机器人动态规划(Dynamic Programming)入门</title>
    <url>/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="什么是动态规划"><a href="#什么是动态规划" class="headerlink" title="什么是动态规划"></a>什么是动态规划</h1><p>CS专业出身的人大抵没有人不知道动态规划(Dynamic Programming)的，该算法的本质就是把复杂的大问题分解成相互重叠的简单子问题，将子问题的最优解层层组合起来，就得到了复杂大问题的最优解。</p>
<p>能用动态规划解决的问题必须满足两个条件：一是最优子结构。即问题的最优解所包含的子问题的解也是最优的；二是子问题相互重叠。即是当使用递归进行自顶向下的求解时,每次产生的子问题不总是新的问题,而是已经被重复计算过的问题。<br>最典型的经常被拿来讲解Dynamic Programming的例子就是斐波那契数列(Fibonacci sequence)，它的数学定义如下:</p>
<span id="more"></span>

<p>$$<br>\begin{aligned} <br>F(0) &amp; = 0,\\<br>F(1) &amp; = 1,\\<br>F(n) &amp; = F(n-1) + F(n-2),\\<br>\end{aligned}<br>$$</p>
<p>斐波那契数列(Fibonacci sequence)计算的Python实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span>(<span class="params">n</span>):</span></span><br><span class="line">    f = [<span class="number">0</span>] * (n + <span class="number">1</span>)</span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    f[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">        f[i] = f[i - <span class="number">1</span>] + f[i - <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> f[n]</span><br></pre></td></tr></table></figure>

<h1 id="动态规划算法在自动驾驶中的应用"><a href="#动态规划算法在自动驾驶中的应用" class="headerlink" title="动态规划算法在自动驾驶中的应用"></a>动态规划算法在自动驾驶中的应用</h1><p>在如下的自动驾驶场景中，无人车在位置A处进行右转，目标是达到位置G处。理想的驾驶路径是: </p>
<p>(位置A处右转)-&gt;(进入车道C)-&gt;(变道进入车道B)-&gt;(左转到达目标位置G)</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_map_scence-1024x841.png"></p>
<p>自动驾驶场景。</p>
<p>但是由于环境是随机的，我们的无人车在实际上路时，可能遇到各种情况。比如当我们计划从车道C变道进入车道B时，发现左侧被一辆大卡车挡住了；如果停下来等大卡车驶过之后再变道，会被车道C上无人车后方的司机拼命用大喇叭催你，无奈之下，我们只好放弃左转，继续直行，再寻求其它路径达到目的地。</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_stuck_scence-1024x863.png"></p>
<p>目标车道被阻塞的场景</p>
<p>所以最后的行驶路径可能就变成了:</p>
<p>(位置A处右转)-&gt;(进入车道C，直行)-&gt;(通过路口，进入车道D)-&gt;(连续右转，达到位置E)-&gt;(直行到达目标位置G)</p>
<p>这里可以看到，我们需要一种方法，使得在无人车放弃车道C到车道B的变道时继续前进时，能够快速找到下一条可通行路径。动态规划(Dynamic Programming)可以用来解决这类问题，它可以给出从任意一个位置出发到达目的地的最优路径。</p>
<h2 id="简化的问题"><a href="#简化的问题" class="headerlink" title="简化的问题"></a>简化的问题</h2><p>为了应用动态规划(Dynamic Programming)算法，我们首先看下简化版的问题。如下图所示，我们将道路区域按照空间进行网格划分，带阴影线的网格表示不可通行区域，G表示目标位置。</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_grid.png"></p>
<p>图中的蓝色箭头表示车辆在该位置的规划策略，也是我们要求解的目标，可以认为我们的目标就是通过Policy函数，将位置(x，y)映射到车辆的运动Action上。简化起见，我们假设车辆只有四个运动Action：向上、向下、向左、向右。</p>
<p>$$Policy(x, y) = Action\{left, right, up, down\}$$</p>
<p>如何将(x,y)转化为具体的Action呢？ 为了计算Action，我们首先需要为每个网格计算Value值。Value的大小与该网格距离目标位置的最短距离成正比。有了Value值之后，Action的方向就是从Value值大的网格指向Value值小的网格。</p>
<h2 id="网格Value的计算"><a href="#网格Value的计算" class="headerlink" title="网格Value的计算"></a>网格Value的计算</h2><p>每个Cell的Value的Value Function定义如下：</p>
<p>$f(x, y) = min_{(x^{\prime}, y^{\prime})} f(x^{\prime}, y^{\prime})$ + Cost</p>
<p>$(x^{\prime}, y^{\prime})$的取值为: $(x-1, y)$,$(x, y-1)$, $(x+1, y)$, $(x, y+1)$，即它的左、上、右、下四个方向的Cell； cost为Cell之间的Cost，这里取Cost为步长1。</p>
<p>可以看到，这是一个典型的动态规划的问题。我们看下如何使用动态规划(Dynamic Programming)算法求解每个Cell的Value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Map Represention</span></span><br><span class="line">grid = [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">goal = [<span class="built_in">len</span>(grid)-<span class="number">1</span>, <span class="built_in">len</span>(grid[<span class="number">0</span>])-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the cost associated with moving from a cell to an adjacent one</span></span><br><span class="line">cost = <span class="number">1</span> </span><br><span class="line"></span><br><span class="line">delta = [[-<span class="number">1</span>, <span class="number">0</span>], <span class="comment"># go up</span></span><br><span class="line">         [ <span class="number">0</span>, -<span class="number">1</span>], <span class="comment"># go left</span></span><br><span class="line">         [ <span class="number">1</span>, <span class="number">0</span>], <span class="comment"># go down</span></span><br><span class="line">         [ <span class="number">0</span>, <span class="number">1</span>]] <span class="comment"># go right</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_value</span>(<span class="params">grid,goal,cost</span>):</span></span><br><span class="line">  <span class="comment"># If a cell is a wall or it is impossible to reach the goal from a cell,assign that cell a value of 99.</span></span><br><span class="line">  value = [[<span class="number">99</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]</span><br><span class="line">    </span><br><span class="line">  change = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">  <span class="keyword">while</span> change:</span><br><span class="line">    change = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid)):</span><br><span class="line">      <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> goal[<span class="number">0</span>] == x <span class="keyword">and</span> goal[<span class="number">1</span>] == y:</span><br><span class="line">          <span class="keyword">if</span> value[x][y] &gt; <span class="number">0</span>:</span><br><span class="line">            value[x][y] = <span class="number">0</span></span><br><span class="line">            change = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> grid[x][y] == <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(delta)):</span><br><span class="line">            x2 = x + delta[a][<span class="number">0</span>]</span><br><span class="line">            y2 = y + delta[a][<span class="number">1</span>]</span><br><span class="line">            print((x2, y2))</span><br><span class="line">                        </span><br><span class="line">            <span class="keyword">if</span> x2 &gt;= <span class="number">0</span> <span class="keyword">and</span> x2 &lt; <span class="built_in">len</span>(grid) <span class="keyword">and</span> y2 &gt;= <span class="number">0</span> <span class="keyword">and</span> y2 &lt; <span class="built_in">len</span>(grid[<span class="number">0</span>]) <span class="keyword">and</span> grid[x2][y2] == <span class="number">0</span>:</span><br><span class="line">              v2 = value[x2][y2] + cost</span><br><span class="line">                </span><br><span class="line">              <span class="keyword">if</span> v2 &lt; value[x][y]:</span><br><span class="line">                change = <span class="literal">True</span></span><br><span class="line">                value[x][y] = v2</span><br><span class="line">                                </span><br><span class="line">  <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure>
<p>最后我们可以得到如下的Value值，通过它可以得到从任意位置到达目标位置的最短距离。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[11, 99, 7, 6, 5,  4],</span><br><span class="line"> [10, 99, 6, 5, 4,  3],</span><br><span class="line"> [9,  99, 5, 4, 3,  2],</span><br><span class="line"> [8,  99, 4, 3, 2,  1],</span><br><span class="line"> [7,  6,  5, 4, 99, 0]]</span><br></pre></td></tr></table></figure>

<p>将Value值映射为Policy，最终输出的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> [[&#39;v&#39;, &#39; &#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;v&#39;, &#39; &#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;v&#39;, &#39; &#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;v&#39;, &#39; &#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;&gt;&#39;, &#39;&gt;&#39;, &#39;^&#39;, &#39;^&#39;, &#39; &#39;, &#39;*&#39;]]</span><br></pre></td></tr></table></figure>

<h2 id="应用到车辆运动中"><a href="#应用到车辆运动中" class="headerlink" title="应用到车辆运动中"></a>应用到车辆运动中</h2><p>仍然以简化的方式来展示Dynamic Programming的应用。如下图所示，红色是车辆的当前位置，蓝色是车辆的目标姿态。假设车辆的运动角度$\theta$只有四个选择{Up, Down, Left, Right}， 车辆的运动只有三个选择: 左转、直行、右转。</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_sample_1.png"></p>
<p>我们首先构建地图信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0 = navigable space</span></span><br><span class="line"><span class="comment"># 1 = unnavigable space </span></span><br><span class="line">grid = [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>

<p>给定车辆的起始位置、结束位置和车辆的运动约束：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init = [<span class="number">4</span>, <span class="number">3</span>, <span class="number">0</span>] <span class="comment"># given in the form [row,col,direction]</span></span><br><span class="line"><span class="comment"># direction = 0: up</span></span><br><span class="line"><span class="comment">#             1: left</span></span><br><span class="line"><span class="comment">#             2: down</span></span><br><span class="line"><span class="comment">#             3: right</span></span><br><span class="line">                </span><br><span class="line">goal = [<span class="number">2</span>, <span class="number">0</span>] <span class="comment"># given in the form [row,col]</span></span><br><span class="line"></span><br><span class="line">forward = [[-<span class="number">1</span>,  <span class="number">0</span>], <span class="comment"># go up</span></span><br><span class="line">           [ <span class="number">0</span>, -<span class="number">1</span>], <span class="comment"># go left</span></span><br><span class="line">           [ <span class="number">1</span>,  <span class="number">0</span>], <span class="comment"># go down</span></span><br><span class="line">           [ <span class="number">0</span>,  <span class="number">1</span>]] <span class="comment"># go right</span></span><br><span class="line">forward_name = [<span class="string">&#x27;up&#x27;</span>, <span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;down&#x27;</span>, <span class="string">&#x27;right&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># action has 3 values: right turn, no turn, left turn</span></span><br><span class="line">action = [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">action_name = [<span class="string">&#x27;R&#x27;</span>, <span class="string">&#x27;#&#x27;</span>, <span class="string">&#x27;L&#x27;</span>]</span><br><span class="line"></span><br><span class="line">cost = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>] <span class="comment"># cost has 3 values, corresponding to making a right turn, no turn, and a left turn</span></span><br></pre></td></tr></table></figure>

<p>基于Dynamic Programming计算从起点到终点的车辆运动路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimum_policy2D</span>(<span class="params">grid,init,goal,cost</span>):</span></span><br><span class="line">    value = [[[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]]</span><br><span class="line">             </span><br><span class="line">    policy = [[[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]]</span><br><span class="line">             </span><br><span class="line">    policy2D = [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    change = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> change:</span><br><span class="line">        change = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid)):</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>])):</span><br><span class="line">                <span class="keyword">for</span> orientation <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    <span class="keyword">if</span> goal[<span class="number">0</span>] == x <span class="keyword">and</span> goal[<span class="number">1</span>] == y:</span><br><span class="line">                        <span class="keyword">if</span> value[orientation][x][y] &gt; <span class="number">0</span>:</span><br><span class="line">                            value[orientation][x][y] = <span class="number">0</span></span><br><span class="line">                            policy[orientation][x][y] = <span class="string">&#x27;*&#x27;</span></span><br><span class="line">                            change = <span class="literal">True</span></span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> grid[x][y] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">                            o2 = (orientation + action[i]) % <span class="number">4</span></span><br><span class="line">                            x2 = x + forward[o2][<span class="number">0</span>]</span><br><span class="line">                            y2 = y + forward[o2][<span class="number">1</span>]</span><br><span class="line">                            </span><br><span class="line">                            <span class="keyword">if</span> x2 &gt;= <span class="number">0</span> <span class="keyword">and</span> x2 &lt; <span class="built_in">len</span>(grid) <span class="keyword">and</span> y2 &gt;= <span class="number">0</span> <span class="keyword">and</span> y2 &lt; <span class="built_in">len</span>(grid[<span class="number">0</span>]) <span class="keyword">and</span> grid[x2][y2] == <span class="number">0</span>:</span><br><span class="line">                                v2 = value[o2][x2][y2] + cost[i]</span><br><span class="line">                                </span><br><span class="line">                                <span class="keyword">if</span> v2 &lt; value[orientation][x][y]:</span><br><span class="line">                                    value[orientation][x][y] = v2</span><br><span class="line">                                    policy[orientation][x][y] = action_name[i]</span><br><span class="line">                                    change = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    x = init[<span class="number">0</span>]</span><br><span class="line">    y = init[<span class="number">1</span>]</span><br><span class="line">    orientation = init[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    policy2D[x][y] = policy[orientation][x][y]</span><br><span class="line">    <span class="keyword">while</span> policy[orientation][x][y] != <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> policy[orientation][x][y] == <span class="string">&#x27;#&#x27;</span>:</span><br><span class="line">            o2 = orientation</span><br><span class="line">        <span class="keyword">elif</span> policy[orientation][x][y] == <span class="string">&#x27;R&#x27;</span>:</span><br><span class="line">            o2 = (orientation - <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">        <span class="keyword">elif</span> policy[orientation][x][y] == <span class="string">&#x27;L&#x27;</span>:</span><br><span class="line">            o2 = (orientation + <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">            </span><br><span class="line">        x = x + forward[o2][<span class="number">0</span>]</span><br><span class="line">        y = y + forward[o2][<span class="number">1</span>]</span><br><span class="line">        orientation = o2</span><br><span class="line">        </span><br><span class="line">        policy2D[x][y] = policy[orientation][x][y]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> policy2D</span><br></pre></td></tr></table></figure>

<p>最终输出的路径结果如下:</p>
<p>[[‘ ‘, ‘ ‘, ‘ ‘, ‘R’, ‘#’, ‘R’],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘#’],<br> [‘*’, ‘#’, ‘#’, ‘#’, ‘#’, ‘R’],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘]]</p>
<p>在上面的实现中，我们将左转的Cost设置为20，比较高的左转代价使得车辆更倾向于直行和右转，所以规划路径的效果如下：</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_sample_2.jpg"></p>
<p>我们将将左转的Cost降低到2，看看会发生什么效果？</p>
<p>[[‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘],<br> [‘*’, ‘#’, ‘#’, ‘L’, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘]]</p>
<p>可以看到，路径规划在路口位置选择了左转，规划效果如下：</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_sample_3.jpg"></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>本文内容源自Udacity的免费课程：Dynamic Programming - Artificial Intelligence for Robotics<br>Youtube链接: <a href="https://www.youtube.com/watch?v=r2bPY2s9wII&amp;t=12s">https://www.youtube.com/watch?v=r2bPY2s9wII&amp;t=12s</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
        <tag>动态规划</tag>
        <tag>机器人</tag>
        <tag>自动驾驶运动规划</tag>
        <tag>运动规划</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶Mapping-占位栅格图(Occupancy Grid Map)</title>
    <url>/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/</url>
    <content><![CDATA[<p>前面文章《自动驾驶运动规划(Motion Planning)》中提到可以使用占位图(Occupancy Grid Map)表示自动驾驶行驶区域的哪些区域被障碍物(如静止的车辆、路中间的石墩子、树木、路肩等)占用，Motion Planning模块会通过查询占位地图避开这些道路障碍物，避免与它们碰撞，从而达到安全驾驶的目的。</p>
<h1 id="占位栅格地图-Occupancy-Grid-Map"><a href="#占位栅格地图-Occupancy-Grid-Map" class="headerlink" title="占位栅格地图(Occupancy Grid Map)"></a>占位栅格地图(Occupancy Grid Map)</h1><p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-10-43-36-1024x604.png"></p>
<span id="more"></span>

<p>如上图所示，将车辆行驶道路环境用网格(Cell)切分，并且每个网格(Cell)用二值数值0和1填充，0表示该网格(Cell)被占用，1表示该网格(Cell)没有被占用。</p>
<p>$m^{i} \in \{0, 1\}$</p>
<p>由此，可以得到如下所示的一张栅格占位地图。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-10-54-24-1024x607.png"></p>
<p>要制作理想的占位栅格地图必须满足的以下几个假设条件：</p>
<p>1）占位栅格地图是对道路行驶区域中的静态环境(Static Environment)的描述。也就意味着，我们在制图前必须将地面、动态物体(车辆、行人等)从传感器数据中移除掉；</p>
<p>2）每个网格(Cell)与其它的所有网格的状态是相互独立的，即它的状态不受周围其它网格状态的影响；</p>
<p>3）在每个时刻，车辆的位置是精确的、已知的。</p>
<h1 id="概率占位栅格地图-Probabilistic-Occupancy-Grid-Map"><a href="#概率占位栅格地图-Probabilistic-Occupancy-Grid-Map" class="headerlink" title="概率占位栅格地图(Probabilistic Occupancy Grid Map)"></a>概率占位栅格地图(Probabilistic Occupancy Grid Map)</h1><p>在实际的应用中，车辆传感器的数据测量是存在误差的，车辆的定位结果也是存在误差的，动态障碍物的识别也是存在误差的，因此用概率表示一个网格(Cell)被占用的可能性是一个更加可行的方案。每个网格存储一个[0, 1]之间的概率值，这个值越大，表示网格被占用的可能性越大；这个值越小，表示网格被占用的可能性越小。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-11-32-38.png"></p>
<h1 id="概率占位栅格图-Probabilistic-Occupancy-Grid-Map-制图"><a href="#概率占位栅格图-Probabilistic-Occupancy-Grid-Map-制图" class="headerlink" title="概率占位栅格图(Probabilistic Occupancy Grid Map)制图"></a>概率占位栅格图(Probabilistic Occupancy Grid Map)制图</h1><p>栅格地图的每个Cell的概率值计算公式如下：</p>
<p>$bel_t(m^i) = p(m^i (y, x)_{1:t})$</p>
<p>其中$(y, x)_{1:t}$是1到t时刻的车辆位置和传感器测量结果，通过历史信息的累计，可以提升制作的地图的准确性。</p>
<p>如何将1到t时刻的所有传感器测量结果融合起来呢？贝叶斯理论(Bayes Theorem)是一个不错的选择。</p>
<p>$bel_t(m^i) = \eta p(y_t m^i) bel_{t-1}(m^i)$</p>
<p>其中$\eta$是归一化参数, $p(y_t m^i)$是传感器的测量模型。通过贝叶斯理论(Bayes Theorem)将多次传感器测量结果融合到同一个Cell中，从而获得高可信度的网格占用概率。</p>
<h2 id="贝叶斯理论-Bayes-Theorem-更新存在的问题"><a href="#贝叶斯理论-Bayes-Theorem-更新存在的问题" class="headerlink" title="贝叶斯理论(Bayes Theorem)更新存在的问题"></a>贝叶斯理论(Bayes Theorem)更新存在的问题</h2><p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-18-37-34.png"></p>
<p>重复的浮点数乘法运算导致计算结果的数值变得很小而难以精确表达和运算。Logit函数可以把自变量从(0,1)连续单调地映射到正负无穷。logit函数的定义如下：</p>
<p>$f(x) = log {\frac{x}{1 - x}}$</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/20180901104349204.png"></p>
<p>所以我们使用Logit函数替代标准的Bayes更新过程。</p>
<h2 id="贝叶斯更新过程的推导"><a href="#贝叶斯更新过程的推导" class="headerlink" title="贝叶斯更新过程的推导"></a>贝叶斯更新过程的推导</h2><p>贝叶斯理论(Bayes Theorem)更新网格(Cell)占用概率的公式如下：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(y_{t} y_{1: t-1}, m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(y_{t} y_{1: t-1}\right)} \tag{1}<br>$</p>
<p>根据一阶马尔科夫(Markov Assumption)假设，t时刻的状态只与t-1时刻的状态有关，因此公式(1)可写为如下形式：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(y_{t} m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(y_{t} y_{1: t-1}\right)} \tag{2}<br>$</p>
<p>对测量模型应用贝叶斯(Bayes Theorem)更新过程：</p>
<p>$<br>p\left(y_{t} m^{i}\right)=\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right)}{p\left(m^{i}\right)} \tag{3}<br>$</p>
<p>将公式3)代入公式2)，可得：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(m^{i}\right) p\left(y_{t} y_{1: t-1}\right)} \tag{4}<br>$</p>
<p>然后计算1-p的值：</p>
<p>$<br>p\left(\neg m^{i} y_{1: t}\right)=1-p\left(m^{i} y_{1: t}\right)=\frac{p\left(\neg m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i}\right) p\left(y_{t} y_{1: t-1}\right)} \tag{5}<br>$</p>
<p>将p和1-p代入logit函数：</p>
<p>$<br>\operatorname{logit}(p)=\log \left(\frac{p}{1-p}\right)<br>$</p>
<p>$<br>\begin{aligned}<br>\quad \frac{p\left(m^{i} y_{1: t}\right)}{p\left(\neg m^{i} y_{1: t}\right)} &amp; =\frac{\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(m^{i}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}}{\frac{p\left(\neg m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i}\right) p\left(y_{t} y_{1: t-1}\right)}} \\<br>&amp;=\frac{p\left(m^{i} y_{t}\right) p\left(\neg m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i} y_{t}\right) p\left(m^{i}\right) p\left(\neg m^{i} y_{1: t-1}\right)} \\<br>&amp;=\frac{p\left(m^{i} y_{t}\right)\left(1-p\left(m^{i}\right)\right) p\left(m^{i} y_{1: t-1}\right)}{\left(1-p\left(m^{i} y_{t}\right)\right) p\left(m^{i}\right)\left(1-p\left(m^{i} y_{1: t-1}\right)\right)}<br>\end{aligned} \tag{6}<br>$</p>
<p>对公式6）等号两侧取log，进行整理后，得到：</p>
<p>$<br>\operatorname{logit}\left(p\left(m^{i} y_{1: t}\right)\right)=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+\operatorname{logit}\left(p\left(m^{i} y_{1: t-1}\right)\right)-\operatorname{logit}\left(p\left(m^{i}\right)\right)<br>$</p>
<p>于是得到<strong>Bayes更新递推公式</strong>：</p>
<p>$<br>l_{t, i}=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+l_{t-1, i}-l_{0, i}<br>$</p>
<p>其中: $\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)$是Inverse Measurement Model，$l_{t-1, i}$是网格i在t-1时刻的置信度(belif)，$l_{0,i}$是Initial belief。</p>
<p>可以看到，该递推公式应用的关键是Inverse Measurement Model：$p\left(m^{i} y_{t}\right))$，如何计算该值呢？</p>
<h2 id="Inverse-Measurement-Model"><a href="#Inverse-Measurement-Model" class="headerlink" title="Inverse Measurement Model"></a>Inverse Measurement Model</h2><p>占位栅格地图的传感器测量模型为：$p(y_t m^{i})$，表示基于已有的地图Cell概率，叠加传感器测量结果，得到新的占位概率值。</p>
<p>而现在我们要求解的是：$p(m^{i} y_t)$，这也是为什么该公式被成为Inverse Measurement Model的原因。</p>
<p>下面来看看Inverse Measurement Model如何计算？下面以二维激光雷达扫描模型来说明(注意：实际应用的激光雷达是3D的，这里用2D Lidar是为了简化模型，所用理论可以很好推广到3D模型)。</p>
<p><strong>2D Lidar模型</strong></p>
<p>它在2D平面上进行扫描，包含两个参数：Scanner bearing和Scanner rangers。Scanner bearing均匀的分布在[$-{\phi_{max}}^s, {\phi_{max}}^s$]之间，一般的我们可以认为它们均匀分布在360度的各个方向上。Scanner rangers是从Lidar中心到障碍物的距离，Lidar发出激光、接收回波，从而计算出到周围障碍物的距离；为了简化期间，我们也假设Lidar发送激光后立即收到回波，不存在时间延迟。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-06-50-1024x362.png"></p>
<p><strong>Map坐标系&amp;Vehicle坐标系&amp;传感器坐标系</strong></p>
<p>数学模型构建过程中<strong>坐标系</strong>是不可或缺的。这里主要涉及到三个坐标系：Map坐标系、Vehicle坐标系以及传感器坐标系。2D Lidar的测量结果都是相对于自身传感器中心的，即以2D Lidar中心为坐标原点；所有的测量结果最终都要转换到Map坐标系，完成地图制作的计算。</p>
<p>假设2D Lidar在Map坐标系中的姿态为$(x_{1,t}, x_{2,t}, x_{3,t})$，其中$x_{1,t}$和$x_{2,t}$是x和y坐标，$x_{3,t}$是传感器朝向。通过该姿态，可以将2D Lidar测量结果转换到Map坐标系。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-21-57-1024x755.png"></p>
<p><strong>Lidar测量结果与Map Cell关联匹配</strong></p>
<p>如何将2D Lidar模型与Map Cell关联起来呢？如下图所示，第i个Map Cell用$(r^{i}, {\phi}^i)$表示。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-38-07-1024x461.png"></p>
<p>然后通过2D Lidar bearing与Map Cell相对于传感器的方位进行最小误差匹配，得到影响当前Map Cell的激光束。</p>
<p>$k = argmin({\phi}_i - {\phi}_i^s)$</p>
<p>匹配的过程如下：首先定义两个值$\alpha$和$\beta$，各个网格Cell的概率计算如下：</p>
<p>1）如果$r^i &gt; {r_{max}}^s$或者$\phi^i - \phi_k^s &gt; \beta /2$， 表示为探测区域，没有信息，这些区域的概率值一般为0.5，表示不确定是否被占用。</p>
<ol>
<li><p>如果$r_k^s &lt; r_{max}^s$并且$r^i - r_k^s &lt; \alpha / 2$，表示该区域大概率被占用，因此要赋予一个大于0.5的概率值。</p>
</li>
<li><p>如果$r^i - r_k^s &gt; \alpha / 2$，这些网格被占用的概率较低，因此要赋予一个小于0.5的概率值。</p>
</li>
</ol>
<p><img src="/creenshot-from-2020-02-01-12-50-41-1024x462.png"></p>
<p>如下图所示，红色区域为高概率被占用区域，灰色区域为未知区域，其余区域为低概率被占用区域。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-14-59-27-1024x472.png"></p>
<p>至此，有了Inverse Measurement Model，Bayes更新的过程可以正常进行了。</p>
<p><strong>更高效的Inverse Measurement Model计算方法</strong></p>
<p>采用光线跟踪(Ray Tracing)的Bresenham’s Line Algorithm可以大大减少复杂的浮点数计算，提升计算效率。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-20-01.png"></p>
<h1 id="移除Lidar中地面和动态物体"><a href="#移除Lidar中地面和动态物体" class="headerlink" title="移除Lidar中地面和动态物体"></a>移除Lidar中地面和动态物体</h1><p>实际应用中的激光雷达(Lidar)是3D的，会扫描到大量的地面点，这些地面点如果不被移除，按照计算匹配模型，会被当做障碍物处理。所以需要将地面点点云数据从激光雷达点云中移除掉。如何移除呢？一种可行的方法是，通过自动化识别算法从Lidar点云中将地面识别并剔除。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-43-18-1024x652.png"></p>
<p>地面识别的难度是比较高的，因为很多道路路面内外的界限在点云中是不明确的，自动化识别算法会误把道路边界外的区域识别为道路路面，从而导致错误的地图信息等。通过视觉分割算法辅助点云识别可以提升路面的识别率。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-47-38-1024x511.png"></p>
<p>动态物体(行人、车辆等)也需要从点云数据中移除，这依赖于基于点云和图像的感知技术。但同样也存在很多技术难题，比如如何提升识别的准确率，如何将静止的车辆识别出来等等。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-48-41.png"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>本文主要整理自Coursera自动驾驶课程：Motion Planning for Self-Driving Cars第二周课程的学习笔记。</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>概率占位栅格地图</tag>
        <tag>Mapping技术</tag>
        <tag>占位栅格地图</tag>
        <tag>自动驾驶Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶运动规划(Motion Planning)</title>
    <url>/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/</url>
    <content><![CDATA[<h1 id="什么是Motion-Planning"><a href="#什么是Motion-Planning" class="headerlink" title="什么是Motion Planning"></a>什么是Motion Planning</h1><p>Motion Planning是在遵循道路交通规则的前提下，将自动驾驶车辆从当前位置导航到目的地的一种方法。</p>
<p>在实际开放道理场景下，自动驾驶要处理的场景非常繁杂：空旷的道路场景、与行人、障碍物共用道理的场景、空旷的十字路口、繁忙的十字路口、违反交通规则的行人/车辆、正常行驶的车辆/行人等等。场景虽然复杂，但都可以拆解为一系列简单行为(behavior)的组合:</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-18-57-56-1024x468.png"></p>
<span id="more"></span>

<p>将这些简单的行为(behavior)组合起来，就可以完成复杂的驾驶行为。</p>
<h1 id="Motion-Planning的约束条件-constraints"><a href="#Motion-Planning的约束条件-constraints" class="headerlink" title="Motion Planning的约束条件(constraints)"></a>Motion Planning的约束条件(constraints)</h1><p>Motion Planning是一个复杂的问题，它的执行过程需要满足很多约束条件：</p>
<h2 id="车辆运动学约束"><a href="#车辆运动学约束" class="headerlink" title="车辆运动学约束"></a>车辆运动学约束</h2><p>车辆运动受到运动学约束，比如它不能实现瞬时侧向移动，前驱的车辆必须依赖前轮的转向才能实现变道、转向等操作，在弯道上不能速度过快等等。通常我们采用单车模型(Bicycle Model)对车辆运动进行建模。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-17-52.png"></p>
<h2 id="静态障碍物-Static-Obstacle-约束"><a href="#静态障碍物-Static-Obstacle-约束" class="headerlink" title="静态障碍物(Static Obstacle)约束"></a>静态障碍物(Static Obstacle)约束</h2><p>静态障碍物(Static Obstacle)是道路上静止的车辆、路面中间的石墩子等车辆不可行驶的区域。Motion Planning需要避开这些静态障碍物，避免与它们发生碰撞。解决碰撞的思路大概有两种：</p>
<p>1）将静态障碍物(Static Obstacle)在网格占位图中表示出来，然后检测规划路线是否与静态障碍物区域相交。</p>
<p>2）将车辆的轮廓扩大，比如扩展成一个圆形，然后检测障碍物是否与Circle发生碰撞。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-26-13.png"></p>
<h2 id="动态障碍物约束"><a href="#动态障碍物约束" class="headerlink" title="动态障碍物约束"></a>动态障碍物约束</h2><p>Motion Planning要实时处理行人、车辆等各种运动的障碍物，避免与障碍物发生碰撞事故。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-37-33-1024x584.png"></p>
<h2 id="道路交通规则约束"><a href="#道路交通规则约束" class="headerlink" title="道路交通规则约束"></a>道路交通规则约束</h2><p>车辆在道路上行驶必须要遵守车道线约束规则(比如左转专用道只能左转、实线不能变道、路口必须遵守红绿灯的指示)和各种标志标牌的指示。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-55-29-1024x281.png"></p>
<h1 id="Motion-Planning的优化目标"><a href="#Motion-Planning的优化目标" class="headerlink" title="Motion Planning的优化目标"></a>Motion Planning的优化目标</h1><p>了解Motion Planning的约束条件之后，需要构造目标优化函数，然后最小化目标函数，从而获得在当前环境下的最优运动轨迹。目标函数的种类有很多，下面枚举一些常用的目标函数。</p>
<p>1）关注路径长度(Path Length)，寻求到达目的地的最短路径。</p>
<p>$s_f = \int^{s_f}_{s_i}{\sqrt{1+ (\frac{dy}{dx})^2}dx}$</p>
<p>2）关注通行时间(Travel Time)，寻求到达目的地的最短时间。</p>
<p>$T_f = \int^{s_f}_{0} {\frac{1}{v(s)}ds}$</p>
<p>3）惩罚偏离参考轨迹和参考速度的行为。</p>
<p>$\int^{s_f}_{0} {x(s) - x_{ref}(s)ds}$</p>
<p>$\int^{s_f}_{0} {v(s) - v_{ref}(s)ds}$</p>
<p>4）考虑轨迹平滑性(Smoothness)</p>
<p>$\int^{s_f}_{0} {\dddot{x}(s)^2ds}$</p>
<p>5）考虑曲率约束(Curvature)</p>
<p>$\int^{s_f}_{0} {k(s)^2ds}$</p>
<p>通过组合设计自己的目标优化函数，从而获得较好的Planning效果。</p>
<h1 id="分级运动规划器-Hierarchical-Motion-Planning"><a href="#分级运动规划器-Hierarchical-Motion-Planning" class="headerlink" title="分级运动规划器(Hierarchical Motion Planning)"></a>分级运动规划器(Hierarchical Motion Planning)</h1><p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-37-52-1024x324.png"></p>
<p>Motion Planning是一个异常复杂的问题，所以通常我们把它切分为一系列的子问题(Sub Problem)。比如Mission Planner、Behavior Planner、Local Planner、Vehicle Control等。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-40-07.png"></p>
<h2 id="Mission-Planner"><a href="#Mission-Planner" class="headerlink" title="Mission Planner"></a>Mission Planner</h2><p>Mission Planner关注High-Level的地图级别的规划；通过Graph Based的图搜索算法实现自动驾驶路径的规划。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-51-30.png"></p>
<h2 id="Behavior-Planner"><a href="#Behavior-Planner" class="headerlink" title="Behavior Planner"></a>Behavior Planner</h2><p>Behavior Planner主要关注交通规则、其它道路交通参与者(自行车、行人、社会车辆)等等，决定在在当前场景下应该采取何种操作(如停车让行、加速通过、避让行人等等)。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-01-29.png"></p>
<p>Behavior Planner的实现方式比较常见的有几种：<strong>有限状态机(Finite State Machines)、规则匹配系统(Rule Based System)、强化学习系统(Reinforcement Learning)。</strong></p>
<p>有限状态机中的State是各个行为决策，根据对外界环境的感知和交通规则的约束在各个状态之间转换。比如在路口红绿灯的场景，当路口交通灯为红色不可通行时，车辆会首先切换到Decelerate to Stop状态，然后在路口停止线完全停下来，进入Stop状态，并持续在Stop状态等待，直至交通灯变为绿色允许车辆通行，车辆进入Track Speed状态，继续前行。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-15-18.png"></p>
<p>Rule-Based System是通过一系列的分级的规则匹配来决定下一步的决策行为。比如交通灯绿色-&gt;通行；交通灯红色-&gt;停车等待。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-25-58-1024x174.png"></p>
<p>基于强化学习的Behavior Planner系统如下：</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-31-32-1024x498.png"></p>
<h2 id="Local-Planner"><a href="#Local-Planner" class="headerlink" title="Local Planner"></a>Local Planner</h2><p>Local Planner关注如何生成舒适的、碰撞避免的行驶路径和舒适的运动速度，所以Local Planner又可以拆分为两个子问题：<strong>Path Planner和Velocity Profile Generation</strong>。Path Planner又分为Sampling-Based Planner、Variational Planner和Lattice Planner。</p>
<p>最经典的Sampling-Based Planner算法是Rapidly Exploring Random Tree，RRT算法。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-36-30.png"></p>
<p>Variational Planner根据Cost Function进行优化调整，从而避开障碍物，生成安全的轨迹。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-45-12-1024x464.png"></p>
<p>Lattice Planner将空间搜索限制在对车辆可行的Action Space。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-48-25.png"></p>
<p><strong>Velocity Profile Generation</strong>要考虑到限速、速度的平滑性等。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-52-05-1024x529.png"></p>
<p>Vehicle Control将Planner的规划结果转化为车辆的运动行为。</p>
<h1 id="待阅读材料"><a href="#待阅读材料" class="headerlink" title="待阅读材料"></a>待阅读材料</h1><ul>
<li><p>  P. Polack, F. Altche, B. Dandrea-Novel, and A. D. L. Fortelle, “<a href="https://ieeexplore.ieee.org/abstract/document/7995816">The kinematic bicycle model: A consistent model for planning feasible trajectories for autonomous vehicles</a>” 2017 IEEE Intelligent Vehicles Symposium (IV), 2017. Gives an overview of the kinematic bicycle model.</p>
</li>
<li><p>  S. Karaman and E. Frazzoli, “<a href="http://amav.gatech.edu/sites/default/files/papers/icra2013.Karaman.Frazzoli.submitted.pdf">Sampling-based optimal motion planning for non-holonomic dynamical systems</a>,” 2013 IEEE International Conference on Robotics and Automation, 2013. Introduces the RRT* algorithm as an example of sampling-based planning.</p>
</li>
<li><p>  N. Ratliff, M. Zucker, J. A. Bagnell, and S. Srinivasa, “<a href="https://kilthub.cmu.edu/articles/CHOMP_Gradient_Optimization_Techniques_for_Efficient_Motion_Planning/6552254/1">CHOMP: Gradient optimization techniques for efficient motion planning</a>,” 2009 IEEE International Conference on Robotics and Automation, 2009. Introduces the CHOMP algorithm as an example of applying calculus of variations to planning.</p>
</li>
<li><p>  M. Pivtoraiko, R. A. Knepper, and A. Kelly, “<a href="https://ri.cmu.edu/pub_files/2009/3/ross.pdf">Differentially constrained mobile robot motion planning in state lattices</a>,” Journal of Field Robotics, vol. 26, no. 3, pp. 308-333, 2009. Introduces the state lattice planning method.</p>
</li>
</ul>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1、Course自动驾驶课程： <a href="https://www.coursera.org/learn/motion-planning-self-driving-cars/home/welcome">Motion Planning for Self-Driving Cars</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶运动规划</tag>
        <tag>Hierarchical Motion Planning</tag>
        <tag>Motion Planner</tag>
        <tag>Motion Planning</tag>
        <tag>Motion Planning Objective Function</tag>
        <tag>分级运动规划器</tag>
        <tag>车辆动力学</tag>
        <tag>车辆运动学</tag>
        <tag>运动规划目标函数</tag>
        <tag>运动规划约束</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉-Camera标定</title>
    <url>/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/</url>
    <content><![CDATA[<h4 id="1、像素坐标系与图像坐标系之间的关系"><a href="#1、像素坐标系与图像坐标系之间的关系" class="headerlink" title="1、像素坐标系与图像坐标系之间的关系"></a>1、像素坐标系与图像坐标系之间的关系</h4><p>假设每一个像素在u轴和v轴方向上的物理尺寸为dx和dy</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad600029917f48fb5be.jpg" alt="计算机视觉-Camera标定(1)"></p>
<span id="more"></span>

<h4 id="2、图像坐标系到相机坐标系"><a href="#2、图像坐标系到相机坐标系" class="headerlink" title="2、图像坐标系到相机坐标系"></a>2、图像坐标系到相机坐标系</h4><p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad800028bebb494b0ce.jpg" alt="计算机视觉-Camera标定(1)"></p>
<h4 id="3、世界坐标系到相机坐标系"><a href="#3、世界坐标系到相机坐标系" class="headerlink" title="3、世界坐标系到相机坐标系"></a>3、世界坐标系到相机坐标系</h4><p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad70000db53d3553707.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad900013a3bb610ed89.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p>于是，从世界坐标系到像素坐标系的转换关系：</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad900013b9553cafc2f.jpg" alt="计算机视觉-Camera标定(1)"></p>
<h4 id="4、其他情况"><a href="#4、其他情况" class="headerlink" title="4、其他情况"></a>4、其他情况</h4><p>考虑像素坐标系坐标轴不垂直的情况(实际相机由于制造工艺上的问题，导致物理成像坐标轴不是绝对垂直)，如下图所示，假设O1在UV坐标系下的坐标为(u0, v0),像素的物理尺寸仍然为dx，dy，则有</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/472b0002dfc7ab409f46.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/47290002d8c8c581f478.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p>矩阵形式如下：</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/47290002d94419e50800.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p>世界坐标系与像素坐标系转换关系：</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad600029ca1c9969969.jpg" alt="计算机视觉-Camera标定(1)"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>computer vision</tag>
        <tag>坐标系变换</tag>
        <tag>相机标定</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶定位算法-直方图滤波(Histogram Filter)定位</title>
    <url>/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/</url>
    <content><![CDATA[<h1 id="直方图滤波-Histogram-Filter-的算法思想"><a href="#直方图滤波-Histogram-Filter-的算法思想" class="headerlink" title="直方图滤波(Histogram Filter)的算法思想"></a>直方图滤波(Histogram Filter)的算法思想</h1><p>直方图滤波的算法思想在于：它把整个状态空间dom(x(t))切分为互不相交的部分$b_1、b_2、…,b_{n-1}$，使得：</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/1524143870919032686cb02.jpeg" alt="无人驾驶高精度定位技术(1)-直方图滤波Histogram Filter"></p>
<p>然后定义一个新的状态空间$y_t \in \{0,…,n−1\}$，当且仅当$x(t)∈b_i$时，$y_t=i$,由于$y_t$是一个离散的状态空间，我们就可以采用离散贝叶斯算法计算$bel(y_t)$。$bel(y_t)$是对$bel(x_t)$的近似，它给出x(t)在每一个$b_i$的概率，$b_i$划分的越精细，计算的结果就越精确，当然精确的代价是计算成本的上升。</p>
<span id="more"></span>

<h1 id="1D直方图滤波在自动驾驶定位的应用"><a href="#1D直方图滤波在自动驾驶定位的应用" class="headerlink" title="1D直方图滤波在自动驾驶定位的应用"></a>1D直方图滤波在自动驾驶定位的应用</h1><p>如下图所示，无人驾驶汽车在一维的宽度为5m的世界重复循环，因为世界是循环的，所以如果无人驾驶汽车到了最右侧，再往前走一步，它就又回到了最左侧的位置。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/15242324358146dc9241c7f.png"></p>
<p>自动驾驶汽车上安装有Sensor可以检测车辆当前所在位置的颜色，但Sensor本身的存在一定检测错误率，即Sensor对颜色的检测不是100%准确的；</p>
<p>无人驾驶汽车以自认为1m/step的恒定速度向右运动，车辆运动本身也存在误差，即向车辆发出的控制命令是向右移动2m，而实际的车辆运动结果可能是待在原地不动，可能向右移动1m，也可能向右移动3m。</p>
<h2 id="数学模型"><a href="#数学模型" class="headerlink" title="数学模型"></a><strong>数学模型</strong></h2><p>从数学的语言来描述这个问题：机器人只有一个状态变量：位置，$pos(t) \in [0,5)$。应用直方图滤波(Histogram Filter)，对状态空间做如下分解:</p>
<p>$$<br>b_0 \in [0,1),<br>b_1 \in [1,2),<br>b_2 \in [2,3),<br>b_3 \in [3,4),<br>b_4 \in [4,5)<br>$$</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/15242324358146dc9241c7f.jpeg" alt="无人驾驶高精度定位技术(1)-直方图滤波Histogram Filter"></p>
<p>于是得到一个新的状态空间$y(t) \in {0,…,4}$，它是对连续状态空间的近似，在某一时刻车辆只能在这些离散状态中的一个。</p>
<p>虽然车辆自认为在向右运动，每一步运动1m，我们假设存在5%的概率，无人驾驶汽车仍待在原地没动；存在90%的概率车辆在向右移动1m；存在5%的概率无人驾驶汽车在向右运动2m。</p>
<p>$$<br>\begin{aligned}<br>\text{P}(y_t &amp;= (x+2) mod 5 y_{t-1} = x) = 0.05 \\<br>\text{P}(y_t &amp;= (x+1) mod 5 y_{t-1} = x = 0.9 \\<br>\text{P}(y_t &amp;= xy_{t-1} = x)=0.05 \\<br>\end{aligned}<br>$$</p>
<p>无人驾驶汽车的Sensor假设90%的概率感知结果是正确的，还有10%的情况下它感知的结果是错误的。</p>
<p>$$<br>\begin{aligned}<br>\text{P}(\text{MeasuredColor}_t = \text{Blue} y_t = 0,2,3) &amp;= 0.9 \\<br>\text{P}(\text{MeasuredColor}_t = \text{Orange} y_t = 0,2,3) &amp;= 0.1 \\<br>\text{P}(\text{MeasuredColor}_t = \text{Blue} y_t = 1,4) &amp;= 0.1 \\<br>\text{P}(\text{MeasuredColor}_t = \text{Orange} y_t = 1,4) &amp;= 0.9 \\<br>\end{aligned}<br>$$</p>
<h2 id="利用直方图滤波-Histogram-Filter-进行车辆定位的过程"><a href="#利用直方图滤波-Histogram-Filter-进行车辆定位的过程" class="headerlink" title="利用直方图滤波(Histogram Filter)进行车辆定位的过程"></a>利用直方图滤波(Histogram Filter)进行车辆定位的过程</h2><p>我们用一个5维的向量来描述t时刻，无人驾驶汽车位于第1个格子、第2个格子、第3个格子、第4个格子、第5个格子的概率。</p>
<p>$$\text{bel}(y_t) = (\text{bel}_{t,1}, \text{bel}_{t,2}, \text{bel}_{t,3}, \text{bel}_{t,4}, \text{bel}_{t,5})$$</p>
<p>无人驾驶汽车对自己所在位置一无所知，假设它连续三次【运动-向右走一步】-【观测】,三次观测结果分别是orange、blue、orange，我们一步步无人驾驶汽车是如何通过【运动】-【观测】过程逐步确认自己的位置的。</p>
<p><strong>t=0时刻</strong></p>
<p>没有任何先验知识，无人车不知道自己在哪里,所以在各个位置概率相等：</p>
<p>$$\text{bel}(y_0) = (0.2, 0.2, 0.2, 0.2, 0.2)$$</p>
<p><strong>t=1时刻</strong></p>
<p>首先向右走1m，用运动模型进行位置预测</p>
<p>$$<br>\begin{aligned}<br>\overline{\text{bel}}(y_1) &amp;= \sum_{y_0} \text{P}(y_1 y_0) P(y_0) \\<br>&amp;=(0.05,0.9,0.05,0,0) * 0.2+(0,0.05,0.9,0.05,0) * 0.2 \\<br>&amp;+(0,0.05,0.9,0.05) * 0.2+(0.05,0,0,0.05,0.9) * 0.2 \\<br>&amp;+(0.9,0.05,0,0,0.05) * 0.2 \\<br>&amp;=(0.2,0.2,0.2,0.2,0.2)<br>\end{aligned}<br>$$</p>
<p>可以看出无人车虽然向前运动一步，但它仍然对自己所在位置一无所知。这也和我们的认知相同，刚开始完全不知道自己在哪里，走了一步自然也完全不知道自己在哪里。</p>
<p>再用更新模型通过Sensor感知环境，更新当前位置的置信度。</p>
<p>$$<br>\begin{aligned}<br>\text{bel}(y_1) &amp;= \eta \text{P}(\text {MeasuredColor}_1 = \text{orange} {y_1}) \overline {\text {bel}} (y_1) \\<br>&amp; = \eta (0.1,0.9,0.1,0.1,0.9) {*} (0.2,0.2,0.2,0.2,0.2) \\<br>&amp; = \eta (0.02,0.18,0.02,0.02,0.18) \\<br>&amp; = (0.04762,0.42857,0.04762,0.04761,0.42857)<br>\end{aligned}<br>$$</p>
<p>orange的颜色感知信息使得传感器认为自己很可能位于第二个和第五个方格中。</p>
<p><strong>t=2时刻</strong>  </p>
<p>运动模型-向右走1m</p>
<p>$$<br>\begin{aligned}<br>\overline{\text{bel}}(y_2) &amp;= \sum_{y_1} \text{P}(y_2 y_1) \text{P}(y_1) \\<br>&amp;= (0.39048,0.08571,0.39048,0.06667,0.06667) \\<br>\end{aligned}<br>$$</p>
<p>更新模型-sensor环境感知</p>
<p>$$<br>\begin{aligned}<br>\text{bel}(y_2) &amp;= \eta \text{P}(\text{MeasuredColor}_2 = \text{blue} y_2) \overline{\text{bel}}(y_2) \\<br>&amp;=\eta (0.1,0.9,0.1,0.1,0.9) {*} (0.39048,0.08571,0.39048,0.06667,0.06667) \\<br>&amp;=(0.45165,0.01102,0.45165,0.07711,0.00857) \\<br>\end{aligned}<br>$$</p>
<p><strong>t=3时刻</strong></p>
<p>运动模型-向右走1m</p>
<p>$$<br>\begin{aligned}<br>\overline{\text{bel}}(y_3) &amp;= \sum_{y_2} \text{P}(y_3 y_2) \text{P}(y_2) \\<br>&amp;= =(0.03415,0.40747,0.05508,0.41089,0.09241) \\<br>\end{aligned}<br>$$</p>
<p>感知更新模型-sensor环境感知</p>
<p>$$<br>\begin{aligned}<br>\text{bel}(y_3) &amp;= \eta \text{P}(\text{MeasuredColor}_3 = \text{orange} y_3) \overline{\text{bel}}(y_3) \\<br>&amp;= \eta(0.1,0.9,0.1,0.1,0.9){*}(0.03415,0.40747,0.05508,0.41089,0.09241) \\<br>&amp;= (0.00683,0.73358,0.01102,0.08219,0.16637) \\<br>\end{aligned}<br>$$</p>
<p>可以看到经过三次的运动和观测后，无人驾驶汽车已经有73%的概率确认自己位于第二个网格中，事实再经过三次的运动观测，无人驾驶汽车可以有94%的概率确认自己的位置。</p>
<h1 id="2D直方图滤波在自动驾驶定位中的应用-一"><a href="#2D直方图滤波在自动驾驶定位中的应用-一" class="headerlink" title="2D直方图滤波在自动驾驶定位中的应用(一)"></a>2D直方图滤波在自动驾驶定位中的应用(一)</h1><p>1D的直方图滤波可以很好的帮助我们理解直方图滤波的原理以及在如何应用在自动驾驶的定位过程中。但是1D的直方图滤波在实际应用中几乎是不存在的，所以我们从更偏向应用的角度，看看2D直方图滤波在自动驾驶定位中是如何工作的。</p>
<h2 id="定义二维地图"><a href="#定义二维地图" class="headerlink" title="定义二维地图"></a>定义二维地图</h2><p>首先定义一张二维地图，R和G代表地图块的颜色：R为红色，G为绿色。每个地图块的大小根据实际应用而定，比如0.0125m*0.125m、0.025m*0.025m等。地图块越小，定位精度越高，但是地图数据量和计算量也就越大；反之，地图块越大，定位精度越低，但数据量和计算量也相应较低。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grid &#x3D; [</span><br><span class="line">    [R,G,G,G,R,R,R],</span><br><span class="line">    [G,G,R,G,R,G,R],</span><br><span class="line">    [G,R,G,G,G,G,R],</span><br><span class="line">    [R,R,G,R,G,G,G],</span><br><span class="line">    [R,G,R,G,R,R,R],</span><br><span class="line">    [G,R,R,R,G,R,G],</span><br><span class="line">    [R,R,R,G,R,G,G],</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>t=0时刻，车辆不知道自己处于地图中的具体位置，转化为数学表述，就是车辆在各个地图块的置信度相同，代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_beliefs</span>(<span class="params">grid</span>):</span></span><br><span class="line">    height = <span class="built_in">len</span>(grid)</span><br><span class="line">    width = <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">    area = height * width</span><br><span class="line">    belief_per_cell = <span class="number">1.0</span> / area</span><br><span class="line">    beliefs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        row = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">            row.append(belief_per_cell)</span><br><span class="line">        beliefs.append(row)</span><br><span class="line">    <span class="keyword">return</span> beliefs</span><br></pre></td></tr></table></figure>

<p>初始置信度如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020</span><br></pre></td></tr></table></figure>

<p>置信度的可视化如下，红色星星位置为车辆的真实初始实际位置，蓝色圈大小代表置信度的高低，蓝色圈越大，置信度越高，蓝色圈越小，置信度越低。t=0时刻，车辆不确定自己的位置，所以各个位置的置信度相等。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-06-22-50-31.png"></p>
<h2 id="运动更新"><a href="#运动更新" class="headerlink" title="运动更新"></a>运动更新</h2><p>车辆运动模型简化为x、y两个方向的运动，同时由于运动的不确定性，需要对运动后的位置增加概率性信息。</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move</span>(<span class="params">dy, dx, beliefs, blurring</span>):</span></span><br><span class="line">    height = <span class="built_in">len</span>(beliefs)</span><br><span class="line">    width = <span class="built_in">len</span>(beliefs[<span class="number">0</span>])</span><br><span class="line">    new_G = [[<span class="number">0.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(width)] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(height)]</span><br><span class="line">    <span class="keyword">for</span> i, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(beliefs):</span><br><span class="line">        <span class="keyword">for</span> j, cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(row):</span><br><span class="line">            new_i = (i + dy ) % height</span><br><span class="line">            new_j = (j + dx ) % width</span><br><span class="line">            new_G[<span class="built_in">int</span>(new_i)][<span class="built_in">int</span>(new_j)] = cell</span><br><span class="line">    <span class="keyword">return</span> blur(new_G, blurring)</span><br></pre></td></tr></table></figure>

<h2 id="观测更新"><a href="#观测更新" class="headerlink" title="观测更新"></a>观测更新</h2><p>观测更新的过程中，当观测的Color等于地图块的Color时，hit=1， bel=beliefs[i][j] * p_hit；当观测到的Color不等于地图块的Color时，hit=0， bel=beliefs[i][j] * p_miss。</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sense</span>(<span class="params">color, grid, beliefs, p_hit, p_miss</span>):</span></span><br><span class="line">    new_beliefs = []</span><br><span class="line"> </span><br><span class="line">    height = <span class="built_in">len</span>(grid)</span><br><span class="line">    width = <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop through all grid cells</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        row = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">            hit = (color == grid[i][j])</span><br><span class="line">            row.append(beliefs[i][j] * (hit * p_hit + (<span class="number">1</span>-hit) * p_miss))</span><br><span class="line">        new_beliefs.append(row)</span><br><span class="line">        </span><br><span class="line">    s = <span class="built_in">sum</span>(<span class="built_in">map</span>(<span class="built_in">sum</span>, new_beliefs))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">            new_beliefs[i][j] = new_beliefs[i][j] / s</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> new_beliefs</span><br></pre></td></tr></table></figure>

<h2 id="运行定位流程"><a href="#运行定位流程" class="headerlink" title="运行定位流程"></a>运行定位流程</h2><p>单次直方图滤波定位过程中，先进行观测更新，再进行运动更新。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, num_steps=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">self.sense()</span><br><span class="line">dy, dx = self.random_move()</span><br><span class="line">self.move(dy,dx)</span><br></pre></td></tr></table></figure>

<p>设置运动更新的不确定度为0.1，观测更新的错误率：每隔100次观测出现一次观测错误，车辆的真实初始位置为(3,3),注意，这个真实位置车辆自己并不知道，我们只是为了仿真而设置的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blur = <span class="number">0.1</span></span><br><span class="line">p_hit = <span class="number">100.0</span></span><br><span class="line">init_pos = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">simulation = sim.Simulation(grid, blur, p_hit, init_pos)</span><br><span class="line"></span><br><span class="line">simulation.run(<span class="number">1</span>)</span><br><span class="line">simulation.show_beliefs()</span><br><span class="line">show_rounded_beliefs(simulation.beliefs)</span><br></pre></td></tr></table></figure>

<p>经过一次直方图滤波定位之后，各个位置的置信度已经发生了变化。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.003  0.002  0.036  0.002  0.037  0.003  0.038  </span><br><span class="line">0.003  0.037  0.002  0.002  0.001  0.002  0.037  </span><br><span class="line">0.038  0.038  0.003  0.036  0.002  0.002  0.003  </span><br><span class="line">0.038  0.004  0.038  0.003  0.037  0.038  0.038  </span><br><span class="line">0.003  0.038  0.039  0.038  0.003  0.037  0.003  </span><br><span class="line">0.038  0.038  0.038  0.003  0.037  0.003  0.003  </span><br><span class="line">0.038  0.003  0.002  0.002  0.038  0.038  0.038</span><br></pre></td></tr></table></figure>

<p>置信度的可视化效果如下。可以看到，车辆已经对自己的置信度有了一定的认知，但是还是有大量的可能位置需要进一步确认。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-06-23-25-12.png"></p>
<p>连续执行直方图滤波100次，各个位置置信度的数值如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.008  0.000  0.000  0.000  0.000  0.016  0.016  </span><br><span class="line">0.032  0.001  0.000  0.000  0.001  0.032  0.833  </span><br><span class="line">0.016  0.000  0.000  0.000  0.000  0.025  0.017  </span><br><span class="line">0.001  0.000  0.000  0.000  0.000  0.000  0.000  </span><br><span class="line">0.000  0.000  0.000  0.000  0.000  0.000  0.000  </span><br><span class="line">0.000  0.000  0.000  0.000  0.000  0.000  0.000  </span><br><span class="line">0.000  0.000  0.000  0.000  0.000  0.000  0.000</span><br></pre></td></tr></table></figure>

<p>置信度的可视化效果如下,可以看到，车辆已经83.3%的概率可以确定自己所处的位置了。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-06-23-32-51.png"></p>
<h1 id="2D直方图滤波在自动驾驶定位中的应用-二"><a href="#2D直方图滤波在自动驾驶定位中的应用-二" class="headerlink" title="2D直方图滤波在自动驾驶定位中的应用(二)"></a>2D直方图滤波在自动驾驶定位中的应用(二)</h1><p>车辆状态的定义如下：</p>
<p>$$<br>\begin{aligned}<br>{x}_{t} &amp;=<br>\begin{bmatrix}<br>\text{x} \\<br>\text{y} \\<br>\theta \\<br>\text{v} \\<br>\end{bmatrix}<br>\end{aligned}<br>$$</p>
<p>其中，(x,y)是车辆的位置，$\theta$是车辆的朝向，v是车辆的运动速度，我们假设车辆是匀速运动的，$\omega$是车辆运动的角速度。</p>
<p>车辆运动方程如下，其实就是:</p>
<p>$x_{t+1} = x_t + v * cos(\theta) * \Delta t$<br>$y_{t+1} = y_t + v * sin(\theta) * \Delta t$<br>$\theta_{t+1} = \theta_t + \omega * \Delta t$</p>
<p>写成矩阵形式:</p>
<p>$$<br>x_{t+1} =<br>\begin{bmatrix}<br>1.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\<br>0.0 &amp; 1.0 &amp; 0.0 &amp; 0.0 \\<br>0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\<br>0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\<br>\end{bmatrix} *<br>\begin{bmatrix}<br>\text{x} \\<br>\text{y} \\<br>\theta \\<br>\text{v}<br>\end{bmatrix}  +<br>\begin{bmatrix}<br>\Delta t cos(\theta) &amp; 0.0 \\<br>\Delta t sin(\theta) &amp; 0.0 \\<br>0.0 &amp; \Delta t \\<br>1.0 &amp; 0.0 \\<br>\end{bmatrix} *<br>\begin{bmatrix}<br>v \\<br>\omega \\<br>\end{bmatrix}<br>$$</p>
<p>车辆的运动模型代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">motion_model</span>(<span class="params">x, u</span>):</span></span><br><span class="line">    F = np.array([[<span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    B = np.array([[DT * math.cos(x[<span class="number">2</span>, <span class="number">0</span>]), <span class="number">0</span>],</span><br><span class="line">                  [DT * math.sin(x[<span class="number">2</span>, <span class="number">0</span>]), <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0.0</span>, DT],</span><br><span class="line">                  [<span class="number">1.0</span>, <span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line">    x = F @ x + B @ u</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="运动更新-1"><a href="#运动更新-1" class="headerlink" title="运动更新"></a>运动更新</h2><p>运动更新的过程与前面谈到的车辆运动模型一致，车辆运动有不确定性，所以增加了Gaussian Filter用来处理不确定性。还有一个细节，就是车辆运动距离和直方图滤波的分块地图之间的转换关系：</p>
<p>x_shift = $\Delta$ x / map_x_resolution</p>
<p>y_shift = $\Delta$ y / map_y_resolution</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># grid_map是网格地图，u=(v,w)是车辆运动的控制参数，yaw是车辆朝向</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">motion_update</span>(<span class="params">grid_map, u, yaw</span>):</span></span><br><span class="line">    <span class="comment"># DT是时间间隔</span></span><br><span class="line">    grid_map.dx += DT * math.cos(yaw) * u[<span class="number">0</span>]</span><br><span class="line">    grid_map.dy += DT * math.sin(yaw) * u[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># grid_map.xy_reso是地图分辨率</span></span><br><span class="line">    x_shift = grid_map.dx // grid_map.xy_reso</span><br><span class="line">    y_shift = grid_map.dy // grid_map.xy_reso</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">abs</span>(x_shift) &gt;= <span class="number">1.0</span> <span class="keyword">or</span> <span class="built_in">abs</span>(y_shift) &gt;= <span class="number">1.0</span>:  <span class="comment"># map should be shifted</span></span><br><span class="line">        grid_map = map_shift(grid_map, <span class="built_in">int</span>(x_shift), <span class="built_in">int</span>(y_shift))</span><br><span class="line">        grid_map.dx -= x_shift * grid_map.xy_reso</span><br><span class="line">        grid_map.dy -= y_shift * grid_map.xy_reso</span><br><span class="line">    <span class="comment"># MOTION_STD是车辆运动不确定性的标准差</span></span><br><span class="line">    grid_map.data = gaussian_filter(grid_map.data, sigma=MOTION_STD)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grid_map</span><br></pre></td></tr></table></figure>

<h2 id="观测更新-1"><a href="#观测更新-1" class="headerlink" title="观测更新"></a>观测更新</h2><p>这个例子中通过测量车辆到LandMark的距离来确定自身的位置，LandMark的位置都是已知的。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-07-11-55-06.png"></p>
<p>2D 直方图滤波定位算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_gaussian_observation_pdf</span>(<span class="params">gmap, z, iz, ix, iy, std</span>):</span></span><br><span class="line">    <span class="comment"># predicted range</span></span><br><span class="line">    x = ix * gmap.xy_reso + gmap.minx</span><br><span class="line">    y = iy * gmap.xy_reso + gmap.miny</span><br><span class="line">    d = math.sqrt((x - z[iz, <span class="number">1</span>]) ** <span class="number">2</span> + (y - z[iz, <span class="number">2</span>]) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># likelihood</span></span><br><span class="line">    pdf = (<span class="number">1.0</span> - norm.cdf(<span class="built_in">abs</span>(d - z[iz, <span class="number">0</span>]), <span class="number">0.0</span>, std))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pdf</span><br><span class="line"></span><br><span class="line">    <span class="comment">#z=[(车辆到Landmark的测量距离，Landmark的x坐标，Landmark的y坐标),...]，z是所有Landmark测量距离和位置的集合，std是测量误差的标准差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">observation_update</span>(<span class="params">gmap, z, std</span>):</span></span><br><span class="line">    <span class="keyword">for</span> iz <span class="keyword">in</span> <span class="built_in">range</span>(z.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> ix <span class="keyword">in</span> <span class="built_in">range</span>(gmap.xw):</span><br><span class="line">            <span class="keyword">for</span> iy <span class="keyword">in</span> <span class="built_in">range</span>(gmap.yw):</span><br><span class="line">                gmap.data[ix][iy] *= calc_gaussian_observation_pdf(</span><br><span class="line">                    gmap, z, iz, ix, iy, std)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 概率归一化</span></span><br><span class="line">    gmap = normalize_probability(gmap)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gmap</span><br></pre></td></tr></table></figure>

<h2 id="运行定位流程-1"><a href="#运行定位流程-1" class="headerlink" title="运行定位流程"></a>运行定位流程</h2><p>设置地图和测量相关参数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DT = <span class="number">0.1</span>  <span class="comment"># time tick [s]</span></span><br><span class="line">MAX_RANGE = <span class="number">10.0</span>  <span class="comment"># maximum observation range</span></span><br><span class="line">MOTION_STD = <span class="number">1.0</span>  <span class="comment"># standard deviation for motion gaussian distribution</span></span><br><span class="line">RANGE_STD = <span class="number">3.0</span>  <span class="comment"># standard deviation for observation gaussian distribution</span></span><br></pre></td></tr></table></figure>
<p>grid map param</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">XY_RESO = <span class="number">0.5</span>  <span class="comment"># xy grid resolution</span></span><br><span class="line">MINX = -<span class="number">15.0</span></span><br><span class="line">MINY = -<span class="number">5.0</span></span><br><span class="line">MAXX = <span class="number">15.0</span></span><br><span class="line">MAXY = <span class="number">25.0</span></span><br></pre></td></tr></table></figure>
<p>Landmark Position</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RF_ID = np.array([[<span class="number">10.0</span>, <span class="number">0.0</span>],</span><br><span class="line">                 [<span class="number">10.0</span>, <span class="number">10.0</span>],</span><br><span class="line">                 [<span class="number">0.0</span>, <span class="number">15.0</span>],</span><br><span class="line">                 [-<span class="number">5.0</span>, <span class="number">20.0</span>]])</span><br></pre></td></tr></table></figure>
<p>车辆的初始位置(for simulation)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xTrue = np.zeros((<span class="number">4</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>通过Observation模拟自动驾驶车辆对各个LandMark的观测结果和车辆速度的误差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">observation</span>(<span class="params">xTrue, u, RFID</span>):</span></span><br><span class="line">    xTrue = motion_model(xTrue, u)</span><br><span class="line"></span><br><span class="line">    z = np.zeros((<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(RFID[:, <span class="number">0</span>])):</span><br><span class="line"></span><br><span class="line">        dx = xTrue[<span class="number">0</span>, <span class="number">0</span>] - RFID[i, <span class="number">0</span>]</span><br><span class="line">        dy = xTrue[<span class="number">1</span>, <span class="number">0</span>] - RFID[i, <span class="number">1</span>]</span><br><span class="line">        d = math.sqrt(dx ** <span class="number">2</span> + dy ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> d &lt;= MAX_RANGE:</span><br><span class="line">            <span class="comment"># add noise to range observation</span></span><br><span class="line">            dn = d + np.random.randn() * NOISE_RANGE</span><br><span class="line">            zi = np.array([dn, RFID[i, <span class="number">0</span>], RFID[i, <span class="number">1</span>]])</span><br><span class="line">            z = np.vstack((z, zi))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add noise to speed</span></span><br><span class="line">    ud = u[:, :]</span><br><span class="line">    ud[<span class="number">0</span>] += np.random.randn() * NOISE_SPEED</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xTrue, z, ud</span><br></pre></td></tr></table></figure>

<p>执行车辆定位流程:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> SIM_TIME &gt;= time:</span><br><span class="line">    time += DT</span><br><span class="line">    print(<span class="string">&quot;Time:&quot;</span>, time)</span><br><span class="line"></span><br><span class="line">    u = calc_input()</span><br><span class="line"></span><br><span class="line">    yaw = xTrue[<span class="number">2</span>, <span class="number">0</span>]  <span class="comment"># Orientation is known</span></span><br><span class="line">    xTrue, z, ud = observation(xTrue, u, RF_ID)</span><br><span class="line"></span><br><span class="line">    grid_map = histogram_filter_localization(grid_map, u, z, yaw)</span><br></pre></td></tr></table></figure>

<p>定位效果如下:</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/animation.gif" alt="3"></p>
<p>图片来源:<a href="https://github.com/AtsushiSakai/PythonRobotics">https://github.com/AtsushiSakai/PythonRobotics</a></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1.<a href="http://www.deepideas.net/robot-localization-histogram-filter/">http://www.deepideas.net/robot-localization-histogram-filter/</a><br>2.<a href="https://github.com/AtsushiSakai/PythonRobotics">https://github.com/AtsushiSakai/PythonRobotics</a></p>
<p>文中完整代码路径:</p>
<p>第3节中的代码github路径:<a href="https://github.com/iamshnoo/localization">https://github.com/iamshnoo/localization</a>，如果无法访问，可以直接通过以下链接下载。</p>
<p><a href="2D_Histogram_Filter.zip">2D_Histogram_Filter</a><a href="2D_Histogram_Filter.zip">下载</a></p>
<p>第4节中的代码的github路径:<a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/histogram_filter/histogram_filter.py">https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/histogram_filter/histogram_filter.py</a> 如果github无法访问，可以通过以下链接下载:</p>
<p><a href="histogram_filter.zip">histogram_filter</a><a href="histogram_filter.zip">下载</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>直方图滤波</tag>
        <tag>自动驾驶定位</tag>
      </tags>
  </entry>
</search>
