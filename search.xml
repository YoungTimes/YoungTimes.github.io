<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度神经网络识别交通标牌</title>
    <url>/2021/03/15/cnn-sign-detection/</url>
    <content><![CDATA[<p>这里我们实现一个入门级的CNN交通标牌分类网络。</p>
<p><img src="/2021/03/15/cnn-sign-detection/dataset_input_output.png"></p>
<span id="more"></span>

<p>首先导入基础依赖库。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>

<h1 id="数据集-Dataset"><a href="#数据集-Dataset" class="headerlink" title="数据集(Dataset)"></a>数据集(Dataset)</h1><p>数据集(Dataset)中包含43个不同分类的、大小为32x32的RGB图像，分类如下:</p>
<ul>
<li>0 = Speed limit (20km/h)</li>
<li>1 = Speed limit (30km/h)</li>
<li>2 = Speed limit (50km/h)</li>
<li>3 = Speed limit (60km/h)</li>
<li>4 = Speed limit (70km/h)</li>
<li>5 = Speed limit (80km/h)</li>
<li>6 = End of speed limit (80km/h)</li>
<li>7 = Speed limit (100km/h)</li>
<li>8 = Speed limit (120km/h)</li>
<li>9 = No passing</li>
<li>10 = No passing for vehicles over 3.5 metric tons</li>
<li>11 = Right-of-way at the next intersection</li>
<li>12 = Priority road</li>
<li>13 = Yield</li>
<li>14 = Stop</li>
<li>15 = No vehicles</li>
<li>16 = Vehicles over 3.5 metric tons prohibited</li>
<li>17 = No entry</li>
<li>18 = General caution</li>
<li>19 = Dangerous curve to the left</li>
<li>20 = Dangerous curve to the right</li>
<li>21 = Double curve</li>
<li>22 = Bumpy road</li>
<li>23 = Slippery road</li>
<li>24 = Road narrows on the right</li>
<li>25 = Road work</li>
<li>26 = Traffic signals</li>
<li>27 = Pedestrians</li>
<li>28 = Children crossing</li>
<li>29 = Bicycles crossing</li>
<li>30 = Beware of ice/snow</li>
<li>31 = Wild animals crossing</li>
<li>32 = End of all speed and passing limits</li>
<li>33 = Turn right ahead</li>
<li>34 = Turn left ahead</li>
<li>35 = Ahead only</li>
<li>36 = Go straight or right</li>
<li>37 = Go straight or left</li>
<li>38 = Keep right</li>
<li>39 = Keep left</li>
<li>40 = Roundabout mandatory</li>
<li>41 = End of no passing</li>
<li>42 = End of no passing by vehicles over 3.5 metric tons</li>
</ul>
<h2 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h2><p>读取训练集、验证集和测试集:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/train.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> training_data:</span><br><span class="line">    train = pickle.load(training_data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/valid.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> validation_data:</span><br><span class="line">    valid = pickle.load(validation_data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./traffic-signs-data/test.p&quot;</span>, mode=<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> testing_data:</span><br><span class="line">    test = pickle.load(testing_data)</span><br></pre></td></tr></table></figure>

<p>查看训练集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train, y_train = train[<span class="string">&quot;features&quot;</span>], train[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>

<p>(34799, 32, 32, 3)</p>
<p>查看验证集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_validation, y_validation = valid[<span class="string">&quot;features&quot;</span>], valid[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_validation.shape)</span><br></pre></td></tr></table></figure>

<p>(4410, 32, 32, 3)</p>
<p>查看测试集大小：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_test, y_test = test[<span class="string">&quot;features&quot;</span>], test[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">print(x_test.shape)</span><br></pre></td></tr></table></figure>

<p>(12630, 32, 32, 3)</p>
<h2 id="数据集可视化"><a href="#数据集可视化" class="headerlink" title="数据集可视化"></a>数据集可视化</h2><p>随机选取一张图片：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = np.random.randint(<span class="number">1</span>, <span class="built_in">len</span>(X_train))</span><br><span class="line"></span><br><span class="line">plt.imshow(X_train[i])</span><br><span class="line"></span><br><span class="line">y_train[i]</span><br></pre></td></tr></table></figure>

<p>图片展示效果如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/traffic_sign.png"></p>
<p>多看一些数据集的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_grid = <span class="number">5</span></span><br><span class="line">L_grid = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(L_grid, W_grid, figsize = (<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">axes = axes.ravel() <span class="comment"># flaten the 5 x 5 matrix into 25 array</span></span><br><span class="line"></span><br><span class="line">n_training = <span class="built_in">len</span>(X_train) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>, W_grid * L_grid):</span><br><span class="line">    <span class="comment"># Select a random number</span></span><br><span class="line">    index = np.random.randint(<span class="number">0</span>, n_training)</span><br><span class="line">    <span class="comment"># read and display an image with the selected index    </span></span><br><span class="line">    axes[i].imshow(X_train[index])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>图片展示效果如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/traffic_signs.png"></p>
<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>使用之前，先对数据进行一些预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line">X_train, y_train = shuffle(X_train, y_train)</span><br></pre></td></tr></table></figure>

<h2 id="灰度化"><a href="#灰度化" class="headerlink" title="灰度化"></a>灰度化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_gray = np.<span class="built_in">sum</span>(X_train / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_validation_gray = np.<span class="built_in">sum</span>(X_validation / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_test_gray = np.<span class="built_in">sum</span>(X_test / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(X_train_gray.shape)</span><br></pre></td></tr></table></figure>

<p>(34799, 32, 32, 1)</p>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>将所有图像数据归一化到[-1, 1]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_gray_norm = (X_train_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X_validation_gray_norm = (X_validation_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">X_test_gray_norm = (X_test_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">print(X_train_gray_norm)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[[[-0.52083333]</span><br><span class="line">   [-0.52604167]</span><br><span class="line">   [-0.51822917]</span><br><span class="line">   ...</span><br><span class="line">   [-0.48958333]</span><br><span class="line">   [-0.47916667]</span><br><span class="line">   [-0.46614583]]</span><br><span class="line"></span><br><span class="line">  [[-0.52083333]</span><br><span class="line">   [-0.52083333]</span><br><span class="line">   [-0.52864583]</span><br><span class="line">   ...</span><br><span class="line">   [-0.5       ]</span><br><span class="line">   [-0.48958333]</span><br><span class="line">   [-0.4765625 ]]</span><br><span class="line"></span><br><span class="line">  [[-0.54427083]</span><br><span class="line">   [-0.53385417]</span><br><span class="line">   [-0.53385417]</span><br><span class="line">   ...</span><br><span class="line">   [-0.50520833]</span><br><span class="line">   [-0.47916667]</span><br><span class="line">   [-0.47395833]]</span><br><span class="line"></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = random.randint(<span class="number">1</span>, <span class="built_in">len</span>(X_train_gray))</span><br><span class="line">plt.imshow(X_train_gray[i].squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(X_train[i])</span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(X_train_gray_norm[i].squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>  灰度化和Normalization的效果如下，从上到下依次为：灰度图像，原图像、归一化的图像。</p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_origin.png" alt="灰度图"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_gray.png" alt="原图"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/train_normal.png" alt="标准化"></p>
<h1 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h1><p>  <img src="/2021/03/15/cnn-sign-detection/cnn.png"></p>
<p>  <img src="/2021/03/15/cnn-sign-detection/dropout.png"></p>
<h2 id="构建深度神经网络"><a href="#构建深度神经网络" class="headerlink" title="构建深度神经网络"></a>构建深度神经网络</h2><p>使用Keras构建CNN网络模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"></span><br><span class="line">CNN = models.Sequential()</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Conv2D(<span class="number">6</span>, (<span class="number">5</span>, <span class="number">5</span>), activation = <span class="string">&#x27;relu&#x27;</span>, input_shape = (<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)))</span><br><span class="line">CNN.add(layers.AveragePooling2D())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Conv2D(<span class="number">16</span>, (<span class="number">5</span>, <span class="number">5</span>), activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.AveragePooling2D())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Dropout(<span class="number">0.2</span>))</span><br><span class="line">CNN.add(layers.Flatten())</span><br><span class="line"></span><br><span class="line">CNN.add(layers.Dense(<span class="number">120</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.Dense(<span class="number">84</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">CNN.add(layers.Dense(<span class="number">43</span>, activation = <span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">CNN.summary()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Model: &quot;sequential_1&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">average_pooling2d_1 (Average (None, 14, 14, 6)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)            (None, 10, 10, 16)        2416      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">average_pooling2d_2 (Average (None, 5, 5, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout (Dropout)            (None, 5, 5, 16)          0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 400)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense)              (None, 120)               48120     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_4 (Dense)              (None, 84)                10164     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_5 (Dense)              (None, 43)                3655      </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 64,511</span><br><span class="line">Trainable params: 64,511</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>

<h2 id="编译和训练"><a href="#编译和训练" class="headerlink" title="编译和训练"></a>编译和训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CNN.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;Adam&#x27;</span>, loss = <span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">history = CNN.fit(X_train_gray_norm,</span><br><span class="line">                  y_train,</span><br><span class="line">                  batch_size = <span class="number">500</span>,</span><br><span class="line">                  epochs = <span class="number">50</span>,</span><br><span class="line">                  verbose = <span class="number">1</span>,</span><br><span class="line">                  validation_data = (X_validation_gray_norm, y_validation))</span><br></pre></td></tr></table></figure>

<p>进行50个Epoch的训练，训练集Accuracy达到98.64%，验证集的Accuracy达到91.61%。</p>
<p>训练过程如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 136ms&#x2F;step - loss: 3.1861 - accuracy: 0.1649 - val_loss: 2.5817 - val_accuracy: 0.3082</span><br><span class="line">Epoch 2&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 135ms&#x2F;step - loss: 1.6409 - accuracy: 0.5335 - val_loss: 1.2052 - val_accuracy: 0.6458</span><br><span class="line">Epoch 3&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 145ms&#x2F;step - loss: 0.9414 - accuracy: 0.7220 - val_loss: 0.8685 - val_accuracy: 0.7417</span><br><span class="line">Epoch 4&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 147ms&#x2F;step - loss: 0.7074 - accuracy: 0.7915 - val_loss: 0.7434 - val_accuracy: 0.7834</span><br><span class="line">Epoch 5&#x2F;5</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 10s 145ms&#x2F;step - loss: 0.5825 - accuracy: 0.8317 - val_loss: 0.6605 - val_accuracy: 0.7955</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Epoch 45&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 115ms&#x2F;step - loss: 0.0509 - accuracy: 0.9844 - val_loss: 0.3022 - val_accuracy: 0.9220</span><br><span class="line">Epoch 46&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 114ms&#x2F;step - loss: 0.0480 - accuracy: 0.9861 - val_loss: 0.2822 - val_accuracy: 0.9254</span><br><span class="line">Epoch 47&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 110ms&#x2F;step - loss: 0.0462 - accuracy: 0.9864 - val_loss: 0.2971 - val_accuracy: 0.9259</span><br><span class="line">Epoch 48&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 111ms&#x2F;step - loss: 0.0460 - accuracy: 0.9860 - val_loss: 0.2665 - val_accuracy: 0.9268</span><br><span class="line">Epoch 49&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 118ms&#x2F;step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.3237 - val_accuracy: 0.9218</span><br><span class="line">Epoch 50&#x2F;50</span><br><span class="line">70&#x2F;70 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 112ms&#x2F;step - loss: 0.0452 - accuracy: 0.9864 - val_loss: 0.3150 - val_accuracy: 0.9161</span><br></pre></td></tr></table></figure>

<h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CNN.save(<span class="string">&#x27;traffic_sign_weights.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<h2 id="模型效果评估"><a href="#模型效果评估" class="headerlink" title="模型效果评估"></a>模型效果评估</h2><p><img src="/2021/03/15/cnn-sign-detection/confusion_mask.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">score = CNN.evaluate(X_test_gray_norm, y_test)</span><br><span class="line">print(<span class="string">&#x27;Test Accuracy: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(score[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<p>395/395 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.9123<br>Test Accuracy: 0.9122723937034607</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history.history.keys()</span><br></pre></td></tr></table></figure>

<p>dict_keys([‘loss’, ‘accuracy’, ‘val_loss’, ‘val_accuracy’])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accuracy = history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">val_accuracy = history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>对训练过程中的Loss进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(accuracy))</span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;ro&#x27;</span>, label = <span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;r&#x27;</span>, label = <span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training, And Validation Loss&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/loss.png"></p>
<p>对训练过程中的Accuracy进行可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(accuracy))</span><br><span class="line">plt.plot(epochs, accuracy, <span class="string">&#x27;ro&#x27;</span>, label = <span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_accuracy, <span class="string">&#x27;r&#x27;</span>, label = <span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Training, And Validation Accuracy&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/accuracy.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predicted_classes = CNN.predict_classes(X_test_gray_norm)</span><br><span class="line">y_true = y_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(y_true, predicted_classes)</span><br><span class="line">plt.figure(figsize = (<span class="number">25</span>, <span class="number">25</span>))</span><br><span class="line">sns.heatmap(cm, annot = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/matrix.png"></p>
<p>在测试集上验证网络效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L = <span class="number">5</span></span><br><span class="line">W = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(L, W, figsize = (<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">axes = axes.ravel()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>, L*W):</span><br><span class="line">    axes[i].imshow(X_test[i])</span><br><span class="line">    axes[i].set_title(<span class="string">&#x27;Prediction = &#123;&#125;\n True = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(predicted_classes[i], y_true[i]))</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplots_adjust(wspace = <span class="number">1</span>) </span><br></pre></td></tr></table></figure>

<p><img src="/2021/03/15/cnn-sign-detection/test_result.png"></p>
<h1 id="实际检测效果验证"><a href="#实际检测效果验证" class="headerlink" title="实际检测效果验证"></a>实际检测效果验证</h1><p>在网上找了两张图片(限速标牌和Stop标牌)试验， 图像中冗余内容越多，检测效果越差。当标牌充满图像时，检测效果还是不错的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> img_to_array</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_to_array</span>(<span class="params">path</span>):</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(path)</span><br><span class="line">    image = image.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    image = img_to_array(image)</span><br><span class="line"></span><br><span class="line">    image = image.reshape([<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prediction</span>(<span class="params">path</span>):</span></span><br><span class="line"></span><br><span class="line">    img = image_to_array(path)</span><br><span class="line"></span><br><span class="line">    plt.imshow(img.squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    img_gray = np.<span class="built_in">sum</span>(img / <span class="number">3</span>, axis = <span class="number">3</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    img_norm = (img_gray - <span class="number">128</span>) / <span class="number">128</span></span><br><span class="line"></span><br><span class="line">    print(img_norm.shape)</span><br><span class="line"></span><br><span class="line">    plt.imshow(img_norm.squeeze(), cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    detection_model = load_model(<span class="string">&#x27;traffic_sign_weights.h5&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    predicted_classes = detection_model.predict_classes(img_norm)</span><br><span class="line"></span><br><span class="line">    print(predicted_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">prediction(<span class="string">&quot;./stop.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">prediction(<span class="string">&quot;./speed_limit_60.jpg&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输入图像如下：</p>
<p><img src="/2021/03/15/cnn-sign-detection/stop.jpg"></p>
<p>检测输出内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">14</span>]</span><br></pre></td></tr></table></figure>
<p>14对应Stop Sign的类型。</p>
<p><img src="/2021/03/15/cnn-sign-detection/speed_limit_60.jpg"></p>
<p>检测输出内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<p>3对应60KM/h的限速。</p>
<h1 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h1><p>Coursera - Traffic Sign Classification Using Deep Learning in Python/Keras</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>环境感知</tag>
        <tag>自动驾驶</tag>
      </tags>
  </entry>
  <entry>
    <title>Lanelets: 高效的自动驾驶地图表达方式</title>
    <url>/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<p>LaneLets是自动驾驶领域高精度地图的一种高效表达方式，它以彼此相互连接的LaneLets来描述自动驾驶可行驶区域，不仅可以表达车道几何，也可以完整表述车道拓扑，同时可以集成交通规则和人的驾驶习惯。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-19-52-41.png" alt="LaneLets Map"></p>
<p>如上图所示，每个Lanelet由left bound和right bound组成，left/right bound有一系列点序列组成，因此可以以任意精度逼近任意车道形状。</p>
<span id="more"></span>

<h1 id="用于Routing的Lanelets-Graph"><a href="#用于Routing的Lanelets-Graph" class="headerlink" title="用于Routing的Lanelets Graph"></a>用于Routing的Lanelets Graph</h1><p>为了能够基于Lanelets进行路径规划，我们可以构建Lanelets邻接图结构。当Lanelets A的左右边界的终点与Lanelets B的左右边界的起点相同时，我们就称Lanelets A和Lanelets B是相邻接的。</p>
<p>如下图所示，图(右)是对图(左)构建的Graph，同时将每个Lanelets的长度作为Graph Edge的权重。基于该Graph，我们就可以采用Dijkstra算法，实现从任意起点到终点的路径规划。当然读者也可以给Graph Edge赋予道路边界类型、权重因子等属性，从而实现其它类型的Routing规划算法。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-20-05-59.png"></p>
<h1 id="Lanelets中的交通规则"><a href="#Lanelets中的交通规则" class="headerlink" title="Lanelets中的交通规则"></a>Lanelets中的交通规则</h1><p>开放的公共道路上存在各种各样的交通控制要素，比如红绿灯、交通标牌等。我们将这些交通规则按照一定的方式组织起来，并关联到对应的Lanelets上。行驶在Lanelets上的车辆必须遵守该Lanelets关联的交通规则。</p>
<p>交通规则通常由两部分内容组成：1、规则的名称和内容；2、遵守这一规则的静态信息或者参数。举个路口红绿灯的例子，它的规则为车辆必须在交通灯为红色的时候，必须停止在路口停止线前等待；它的参数为停止线和关联交通灯的位置。</p>
<p>这里要特别提到是没有红绿灯的十字路口，它的通行规则必须以尽可能少的阻碍其它拥有通行权的交通参与者为准则。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-20-36-38.png"></p>
<h2 id="交通规则表达"><a href="#交通规则表达" class="headerlink" title="交通规则表达"></a>交通规则表达</h2><p>在实际数据中，交通规则通过”<strong>type=regulatory element</strong>“标识，再通过名称为maneuver的tag区分不同的交通规则。</p>
<p><strong>merge and cross</strong></p>
<p>当maneuver=merge时，该规则的参数是：merge发生的第一个Lanelets。该规则期望车辆在进入merge的第一个Lanelets时，尽可能的与同向车道的车辆的运动速度趋同，并且保持安全距离。</p>
<p>当maneuver=cross时，该规则的参数是：与当前Lanelets发生cross的Lanelets，以及为了避免碰撞发生主车的停止位置。</p>
<p><strong>traffic light</strong></p>
<p>当maneuver=traffic light，该规则的参数是：路口的停止线和关联红绿灯的位置。该规则期望当红绿灯为红色时，车辆停止在停止线之前。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-22-48-26.png"></p>
<h1 id="Lanelets中高效的距离计算和测量"><a href="#Lanelets中高效的距离计算和测量" class="headerlink" title="Lanelets中高效的距离计算和测量"></a>Lanelets中高效的距离计算和测量</h1><p>在使用Lanelets的过程中，计算车辆Pose到Lanelets边界的距离非常重要。由于LaneLets的左右边界是由折线组成的，因此我们可以先看看单个Segment的距离如何计算。</p>
<p>假设单个Segment的定义如下:</p>
<p>$G = (p_b, p_t, t_b, t_t)$</p>
<p>其中，$p_b$是Segment的起点，$p_t$是Segment的终点，$t_b$是起点的切向量，$t_t$是终点的切向量。我们可以通过$lambda$对Segment进行插值。</p>
<p>$<br>\begin{aligned}<br>&amp;\mathbf{t}{\lambda}=\lambda \mathbf{t}{t}+(1-\lambda) \mathbf{t}{b}\\ &amp; \mathbf{p}{\lambda}=\lambda \mathbf{p}{t}+(1-\lambda) \mathbf{p}{b}<br>\end{aligned}<br>$</p>
<p>点X = $(x, y)^T$到Segment G的距离定义如下：</p>
<p>$d = n_{\lambda} = x - p_{\lambda}$</p>
<p>并且$n_{\lambda}$满足如下约束：</p>
<p>$n_{\lambda}^T t_{\lambda} = 0 \tag{1}$</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-22-06-18.png"></p>
<p>不失一般性，我们假设$p_b = (0, 0)^T, p_t = (l, 0)^T, t_b = (1, m_b)^T, t_t = (1, m_t)^T$，将这些信息代入公式(1)，可以得到:</p>
<p>$<br>\lambda=\frac{x+y m_{b}}{l-y\left(m_{t}-m_{b}\right)}<br>$</p>
<p>也就是说，只要知道了车辆的Pose(位置和朝向)，我们就能迅速计算出车辆到边界的距离。</p>
<p>除此之外，通过计算$n_{\lambda}/n_{\lambda}$就可以得到任意一点的梯度信息。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-22-24-11.png"></p>
<h1 id="开源的Lanelets地图加载库-libLanelet"><a href="#开源的Lanelets地图加载库-libLanelet" class="headerlink" title="开源的Lanelets地图加载库-libLanelet"></a>开源的Lanelets地图加载库-libLanelet</h1><p>libLanelet使用Boost C++代码库实现，它提供了读取、加载和查询XML文件的功能；使用RTree检索查询空间要素的功能；使用诸如Dijkstra进行Routing路线规划的功能；</p>
<p>随着地图范围的不断扩张，Lanelets的数量会快速膨胀，为了提升数据检索的速度，libLanelets使用RTree对Lanelets进行检索，可以做到在O(logn)时间内实现对任意Object的查询。</p>
<p>Github代码路径：<a href="https://github.com/phbender/liblanelet">https://github.com/phbender/liblanelet</a></p>
<h1 id="Lanelets高精度地图生成"><a href="#Lanelets高精度地图生成" class="headerlink" title="Lanelets高精度地图生成"></a>Lanelets高精度地图生成</h1><h2 id="Top-View-Map生成"><a href="#Top-View-Map生成" class="headerlink" title="Top-View Map生成"></a>Top-View Map生成</h2><p>我们利用立体图像技术，将车辆周围的3D点投影到地面上，再利用轨迹将这些3D投影点聚合起来，形成全局的鸟瞰图。然后利用这些鸟瞰图，使用OSM Editor人工标注高精度地图。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-23-25-52-1024x147.png"></p>
<p>Virtual topview: Single image, reconstructed top view, superposed top view with annotated lanelets</p>
<h2 id="Lanelets存储格式及编辑工具"><a href="#Lanelets存储格式及编辑工具" class="headerlink" title="Lanelets存储格式及编辑工具"></a>Lanelets存储格式及编辑工具</h2><p>Lanelets Map采用OSM格式存储：即文件格式采用XML格式；包含三个基础结构，nodes、ways和relations；坐标系采用WGS-84坐标系。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-06-22-49-26-954x1024.png" alt="Java OSM editor"></p>
<h1 id="Lanelets在实际驾驶中的应用"><a href="#Lanelets在实际驾驶中的应用" class="headerlink" title="Lanelets在实际驾驶中的应用"></a>Lanelets在实际驾驶中的应用</h1><p>自动车辆利用Lanelets地图实现状态转换状态机(State Machine)不同状态之间的转换。如下图所示，当自动驾驶车辆行驶到绑定了交通规则的Lanelets时，并且距离交通规则参数的距离小于一定阈值时，事件S被触发，提醒车辆在停止线前停下来。当自动驾驶车辆距离交通规则参数的距离大于一定距离时，事件A被触发，车辆进入路口驾驶模式。当离开路口时，事件F被触发，整个系统被重置到空闲状态。</p>
<p><img src="/2020/02/07/lanelets-%E9%AB%98%E6%95%88%E7%9A%84%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%9C%B0%E5%9B%BE%E8%A1%A8%E8%BE%BE%E6%96%B9%E5%BC%8F/Screenshot-from-2020-02-07-15-07-48.png"></p>
<p>为了完成周围其它车辆的驾驶行为预测，我们首先获取社会车辆周围一定范围内的所有Lanelets，然后通过社会车辆的Pose与Lanelets中心线的距离和角度阈值过滤掉主车无法到达的Lanelets，最后在自动驾驶车辆的预测范围(比如可以预测社会车辆10s的运动范围)内，以社会车辆的运动朝向和运动速度计算所有的可能运动路径，从而调整自动驾驶汽车的运动规划，以避免可能存在的碰撞行为。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>Lanelets: Efficient Map Representation for Autonomous Driving</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
      </tags>
  </entry>
  <entry>
    <title>低速自动驾驶车辆的定位与建图</title>
    <url>/2021/03/25/localization-mapping-zhixingzhe/</url>
    <content><![CDATA[<p><img src="/2021/03/25/localization-mapping-zhixingzhe/1.png"></p>
<p>本文是高翔博士关于低速自动驾驶定位建图的相关介绍。</p>
<span id="more"></span>

<p><img src="/2021/03/25/localization-mapping-zhixingzhe/2.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/3.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/4.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/5.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/6.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/7.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/8.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/9.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/10.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/11.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/12.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/13.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/14.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/15.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/16.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/17.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/18.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/19.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/20.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/21.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/22.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/23.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/24.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/25.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/26.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/27.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/28.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/29.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/30.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/31.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/32.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/33.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/34.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/35.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/36.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/37.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/38.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/39.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/40.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/41.png"><br><img src="/2021/03/25/localization-mapping-zhixingzhe/42.png"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>高精地图</tag>
        <tag>Mobileye</tag>
      </tags>
  </entry>
  <entry>
    <title>Mobileye REM地图</title>
    <url>/2021/03/14/mobileye-rem-map-md/</url>
    <content><![CDATA[<h1 id="为什么需要高精地图"><a href="#为什么需要高精地图" class="headerlink" title="为什么需要高精地图"></a>为什么需要高精地图</h1><p><img src="/2021/03/14/mobileye-rem-map-md/rem_map.png" alt="Mobileye Rem Map"></p>
<p>理论上来讲，可以在车载系统检测和获取所有道路信息(可行驶路径、车道优先级、红绿灯与车道的关联关系、车道与人行横道与红绿灯的关系等)，但是目前的AI能力无法保证实现很高的MTBF(Mean Time Between Failures, 平均无故障时间)，所以需要提前把这些信息都准备好。</p>
<span id="more"></span>

<p><img src="/2021/03/14/mobileye-rem-map-md/hdmap_motivation.png" alt="Motivation Behind HDMap"></p>
<h1 id="高精地图的挑战"><a href="#高精地图的挑战" class="headerlink" title="高精地图的挑战"></a>高精地图的挑战</h1><h2 id="规模化-Scale"><a href="#规模化-Scale" class="headerlink" title="规模化-Scale"></a>规模化-Scale</h2><p>如果自动驾驶车辆只在一个区域、一个城市、或者几个城市运营，那就不存在规模化的问题。但是2025年之后，自动驾驶会在消费者层面全面落地，用户需要驾车到任意想去的地方，在这种场景下，Scale是一个无法规避的问题。</p>
<h2 id="鲜度-Fresh"><a href="#鲜度-Fresh" class="headerlink" title="鲜度-Fresh"></a>鲜度-Fresh</h2><p>理想情况下，地图是在实时更新的。当物理环境发生变化时，需要实时反映到地图上。月级更新、甚至天级更新都是不够的，我们需要做到分钟级，甚至更短。</p>
<h2 id="精度-Accuracy"><a href="#精度-Accuracy" class="headerlink" title="精度-Accuracy"></a>精度-Accuracy</h2><p>车载系统(OnBoard System)检测的车辆和行人需要与高精地图(High Definiation Map)实现厘米级精度的匹配，因此地图的精度至关重要。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_challange.png" alt="高精地图挑战"></p>
<h1 id="通用高精地图制作方法的缺陷"><a href="#通用高精地图制作方法的缺陷" class="headerlink" title="通用高精地图制作方法的缺陷"></a>通用高精地图制作方法的缺陷</h1><p><img src="/2021/03/14/mobileye-rem-map-md/common_approach_map.png" alt="高精地图通用制作方法"></p>
<h2 id="全局坐标系下厘米级精度不是必需的"><a href="#全局坐标系下厘米级精度不是必需的" class="headerlink" title="全局坐标系下厘米级精度不是必需的"></a>全局坐标系下厘米级精度不是必需的</h2><p>AV车辆行驶过程中只关注周围几百米范围即可，所以只要这个范围内的足够准确即可。至于几公里之外的全局精度，Who Care…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_geometric.png"></p>
<h2 id="语义层数据生产难以自动化"><a href="#语义层数据生产难以自动化" class="headerlink" title="语义层数据生产难以自动化"></a>语义层数据生产难以自动化</h2><p><img src="/2021/03/14/mobileye-rem-map-md/semantic_map.png"></p>
<p>如下图所示，没有车道线的双向车道，单从图像观察，难以识别它的Drive Path。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/drivable_path.png"></p>
<p>如下图所示，转向规则千奇百怪：禁止红灯右转，完全停车后允许红灯右转，绿灯禁止左转，绿灯Yield后允许左转…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/priority_map.png"></p>
<p>如下图所示，红绿灯异常复杂，识别车道、人行横道与红绿灯的关联关系难度很大…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_association.png"></p>
<p>如下图所示，除非地图可以表达所有的3D要素，否则很难自动化的计算出车道的最优Stop/Yield Point。但是表达所有的3D信息对于地图来说又是不现实的…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/stop_point.png"></p>
<p>影响车辆行驶速度的因素有很多，道路几何、限速、文化等，难以量化，但它对Smooth Driving体验至关重要…</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/smooth_driving.png"></p>
<h1 id="Mobileye如何解决这些问题"><a href="#Mobileye如何解决这些问题" class="headerlink" title="Mobileye如何解决这些问题"></a>Mobileye如何解决这些问题</h1><p>scalability依赖众包数据生成Millions Map Agents；Accuracy不是全局的Accuracy，而是局部的Accuracy，相对于道路上的静态元素位置。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/av_map.png"></p>
<p>REM的处理流程如下，首先从成百上千辆车获取检测信息(没有使用差分GPS，而是使用了普通的GPS)，这些数据传送到云端；每辆车Detection的角度不同，由于遮挡等原因，每辆车检测的landmark也有差异，将这些数据进行Alignment处理，生成高精度的地图数据；最后，Modeling And Semantics负责生成地图的语义数据。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/rem_process.png"></p>
<h2 id="Harvesting"><a href="#Harvesting" class="headerlink" title="Harvesting"></a>Harvesting</h2><p>下图中黄色的框是车辆检测的landmarks和lane marks，同时车辆会尝试检测driving path等语义信息，一辆车可能检测不准确，但是成百上千的过路车辆会让检测结果越来越好。</p>
<p>Mobileye Harvesting的数据量为10K/公里，这些检测的数据会被发送到云端。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/harvesting.png"></p>
<h2 id="Aligning-Drives"><a href="#Aligning-Drives" class="headerlink" title="Aligning Drives"></a>Aligning Drives</h2><p>检测每个RSD中每个元素的6D Pose，然后对齐相同位置的元素，得到厘米度精度的driving path等信息。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/map_align_drive.png"></p>
<p>由于GPS存在误差，每个车辆检测的道路元素位置都存在噪声，所以只依靠简单的位置求均值是不可行的。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/align_noise.png"></p>
<p>Align之后可以明显的看到两条Driving Path(蓝色)和两侧的道路边界(红色)。对齐的过程是靠几何运算进行。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/path_align.png"></p>
<p>仅仅靠聚类(Clustering)和Spline Fiting得到下图右上角的结果，这个结果不是特别理想。后来通过神经网络生成高精度地图，效果好了很多。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/model_process.png"></p>
<h1 id="为什么语义理解离不开众包"><a href="#为什么语义理解离不开众包" class="headerlink" title="为什么语义理解离不开众包"></a>为什么语义理解离不开众包</h1><p>如下左图所示，通过众包数据可以在没有Lane Marking的道路上获取Driving Path。</p>
<p>如下右图所示，众包数据提供了复杂场景下的所有可通行路径。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_driving_path.png"></p>
<p>如下图所示，通过众包数据可以获得红绿灯与车道的关联关系、Yield Sign的Stop Point、Crosswalk与红绿灯的关联关系等。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_assocation.png"></p>
<p>如下左图所示，通过检测哪个Drive Path的Stop Point比较多，我们可以从众包数据中获取到没有Traffic Sign情况下各个道路的路权优先级。</p>
<p>如下中图所示，我们可以从众包数据学习到在路口其它司机的停车位置。</p>
<p>如下右图所示，从众包数据可以学习到，在无保护左转的场景下车辆的Stop Point。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_more_info.png"></p>
<p>众包数据是获得各个道路Common Speed的唯一高效的方法，Common Speed提供了当道路没有车辆时候AV车的目标行驶速度。采用这种方法可以使得无论在哪个国家、地区，或者不同的道路类型，AV车都可以自然的融入车流。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/crowd_speed.png"></p>
<h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>到目前为止，Mobileye与超过6家汽车制造厂商合作，每天可以覆盖800万公里的路网更新。预计到2024年，每天覆盖的路网会达到10亿公里。</p>
<p><img src="/2021/03/14/mobileye-rem-map-md/mobileye_situation.png"></p>
<p><strong>说明</strong>： 本文所有内容都来源于Mobileye CEO Amnon Shashua教授在2021 CES的分享。</p>
<p>YouTube链接：<br><a href="https://www.youtube.com/watch?v=B7YNj66GxRA&amp;t=301s">https://www.youtube.com/watch?v=B7YNj66GxRA&amp;t=301s</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>高精地图</tag>
        <tag>Mobileye</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN预测行人运动轨迹</title>
    <url>/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/</url>
    <content><![CDATA[<blockquote>
<p>最近在研究论文-Social LSTM: Human Trajectory Prediction in Crowded Spaces, 先从最基本的RNN模型入手看看效果。</p>
<p>本文代码已经上传到Github:<br><a href="https://github.com/YoungTimes/GNN/blob/master/Social-LSTM/train.py">https://github.com/YoungTimes/GNN/blob/master/Social-LSTM/train.py</a></p>
</blockquote>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/Screenshot_from_2021-03-28_22-40-11.png"></p>
<span id="more"></span>

<h1 id="行人轨迹数据集"><a href="#行人轨迹数据集" class="headerlink" title="行人轨迹数据集"></a>行人轨迹数据集</h1><p>数据集来源自[1]，每个数据目录包含一个pixel_pos.csv文件，它的文件格式如下:</p>
<p>pixel_pose.csv包含4行，它的列数是所有行人轨迹点的数量。</p>
<p>第一行是所有的Frame Number；</p>
<p>第二行是所有行人的ID；</p>
<p>第三行是所有的y坐标；</p>
<p>第四行是所有的x坐标。</p>
<p>先看下数据集的内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">directory = <span class="string">&quot;./data/eth/univ&quot;</span></span><br><span class="line"></span><br><span class="line">file_path = os.path.join(directory, <span class="string">&#x27;pixel_pos.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data = np.genfromtxt(file_path, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the number of pedestrians in the current dataset</span></span><br><span class="line">pedIDs = np.unique(data[<span class="number">1</span>, :])</span><br><span class="line">numPeds = np.size(pedIDs)</span><br><span class="line"></span><br><span class="line">pedIndexLookup = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ped_index, ped_id <span class="keyword">in</span> <span class="built_in">enumerate</span>(pedIDs):</span><br><span class="line">    pedIndexLookup[ped_id] = ped_index</span><br><span class="line"></span><br><span class="line">frameIDs = np.unique(data[<span class="number">0</span>, :])</span><br><span class="line">numFrames = np.size(frameIDs)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;number of pedestrians is: &#123;&#125;, number of frames is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(numPeds, numFrames))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将data/eth/univ数据场景的数据可视化如下图所示，每个不同颜色的圆点都是一个运动的行人。</p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/315ft-jpfdw.gif"></p>
<h1 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h1><p>RNN模型参考了[1][2][3]，大概分为三层: 输入层(Embedding)、RNN层(LSTM/GRU)、输出层。</p>
<p>Embedding层将坐标(x,y)嵌入到64维的向量空间；输出层输出每个预测点的二维高斯分布参数(包含5个参数:mux, muy, sx, sy, corr), 时刻t的预测坐标点最后通过$({x}^{t}, {y}^{t}) \sim \mathcal{N}(\mu_i^t, \sigma_i^t, \rho_i^t)$获取。</p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-8f7cdefd76e7c3a3bc9260eee986f828_720w.jpg" alt="TensorFlow Tutorial-RNN文本生成"></p>
<p>模型代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">args</span>):</span></span><br><span class="line">    output_size = <span class="number">5</span></span><br><span class="line">    model = tf.keras.Sequential([</span><br><span class="line">        tf.keras.layers.Dense(args.embedding_size, activation = tf.keras.activations.relu,</span><br><span class="line">            batch_input_shape = [args.batch_size, <span class="literal">None</span>, <span class="number">2</span>]),</span><br><span class="line">        tf.keras.layers.GRU(args.rnn_size,</span><br><span class="line">                            return_sequences=<span class="literal">True</span>,</span><br><span class="line">                            stateful=<span class="literal">True</span>,</span><br><span class="line">                            recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>),</span><br><span class="line">        tf.keras.layers.Dense(output_size)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><h2 id="Loss函数"><a href="#Loss函数" class="headerlink" title="Loss函数"></a>Loss函数</h2><p>模型的Loss函数为所有待预测轨迹点的负对数似然估计之和，模型训练的过程就是最小化所有待预测轨迹的Loss的过程。</p>
<p>$$<br>L^{i}=-\sum_{t=T_{obs}+1}^{T_{pred}} \log \left(\mathbb{P}\left(x_{t}^{i}, y_{t}^{i} \mid \sigma_{t}^{i}, \mu_{t}^{i}, \rho_{t}^{i}\right)\right)<br>$$</p>
<p>loss代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lossfunc</span>(<span class="params">z_mux, z_muy, z_sx, z_sy, z_corr, x_data, y_data</span>):</span></span><br><span class="line"></span><br><span class="line">    result0 = tf_2d_normal(x_data, y_data, z_mux, z_muy, z_sx, z_sy, z_corr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For numerical stability purposes</span></span><br><span class="line">    epsilon = <span class="number">1e-20</span></span><br><span class="line"></span><br><span class="line">    result1 = -tf.math.log(tf.math.maximum(result0, epsilon))  <span class="comment"># Numerical stability</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sum up all log probabilities for each data point</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum(result1)</span><br></pre></td></tr></table></figure>
<h2 id="轨迹预测效果的Metric"><a href="#轨迹预测效果的Metric" class="headerlink" title="轨迹预测效果的Metric"></a>轨迹预测效果的Metric</h2><p>轨迹预测效果的衡量指标为:<strong>Average Displacement Error</strong>和<strong>Final Displacement Error</strong>。</p>
<p>Average Displacement Error = 所有预测轨迹点与GroundTruth对应轨迹点的空间距离之和/预测轨迹点个数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mean_error</span>(<span class="params">pred_traj, true_traj, observed_length</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The data structure to store all errors</span></span><br><span class="line">    error = np.zeros(<span class="built_in">len</span>(true_traj) - observed_length)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(observed_length, <span class="built_in">len</span>(true_traj)):</span><br><span class="line">        <span class="comment"># The predicted position</span></span><br><span class="line">        pred_pos = pred_traj[i, :]</span><br><span class="line">        <span class="comment"># The true position</span></span><br><span class="line">        true_pos = true_traj[i, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The euclidean distance is the error</span></span><br><span class="line">        error[i-observed_length] = np.linalg.norm(true_pos - pred_pos)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.mean(error)</span><br></pre></td></tr></table></figure>

<p>Final Displacement Error = 最后一个预测轨迹点与GroundTruth对应轨迹点的空间距离；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_final_error</span>(<span class="params">pred_traj, true_traj</span>):</span></span><br><span class="line">    error = np.linalg.norm(pred_traj[-<span class="number">1</span>, :] - true_traj[-<span class="number">1</span>, :])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> error</span><br></pre></td></tr></table></figure>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">args</span>):</span></span><br><span class="line">    datasets = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    data_loader = DataLoader(args.batch_size, args.seq_length, datasets, forcePreProcess=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    model = build_model(args)</span><br><span class="line"></span><br><span class="line">    optimizer = tf.keras.optimizers.RMSprop(args.learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(args.num_epochs):</span><br><span class="line">        data_loader.reset_batch_pointer()</span><br><span class="line">        model.reset_states()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(data_loader.num_batches):</span><br><span class="line">            start = time.time()</span><br><span class="line"></span><br><span class="line">             x, y = data_loader.next_batch()</span><br><span class="line"></span><br><span class="line">            base_pos = np.array([[e_x[<span class="number">0</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(e_x))] <span class="keyword">for</span> e_x <span class="keyword">in</span> x])</span><br><span class="line"></span><br><span class="line">            x_offset = x - base_pos</span><br><span class="line">            y_offset = y - base_pos</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                tensor_x = tf.convert_to_tensor(x_offset, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">                logits = model(tensor_x)</span><br><span class="line"></span><br><span class="line">                [o_mux, o_muy, o_sx, o_sy, o_corr] = get_coef(logits)</span><br><span class="line"></span><br><span class="line">                tensor_y = tf.convert_to_tensor(y_offset, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">                [x_data, y_data] = tf.split(tensor_y, <span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Compute the loss function</span></span><br><span class="line">                loss = get_lossfunc(o_mux, o_muy, o_sx, o_sy, o_corr, x_data, y_data)</span><br><span class="line"></span><br><span class="line">                mean_error, final_error = calc_prediction_error(o_mux, o_muy, o_sx, o_sy, o_corr, tensor_y, args)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Compute the cost</span></span><br><span class="line">                loss = tf.math.divide(loss, (args.batch_size * args.seq_length))</span><br><span class="line"></span><br><span class="line">                grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">                optimizer.lr.assign(args.learning_rate * (args.decay_rate ** e))</span><br><span class="line">                optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.trainable_variables))</span><br></pre></td></tr></table></figure>

<p>training过程:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">5195&#x2F;5200 (epoch 199), train\_loss &#x3D; -5.898, time&#x2F;batch &#x3D; 0.583, mean error &#x3D; 0.019682016324174278, final\_error &#x3D; 0.019766580550931393</span><br><span class="line">5196&#x2F;5200 (epoch 199), train\_loss &#x3D; -5.707, time&#x2F;batch &#x3D; 0.583, mean error &#x3D; 0.01821616107675557, final\_error &#x3D; 0.018569258209317922</span><br><span class="line">5197&#x2F;5200 (epoch 199), train\_loss &#x3D; -5.726, time&#x2F;batch &#x3D; 0.581, mean error &#x3D; 0.021631291888964673, final\_error &#x3D; 0.024468516283668577</span><br><span class="line">5198&#x2F;5200 (epoch 199), train\_loss &#x3D; -6.308, time&#x2F;batch &#x3D; 0.595, mean error &#x3D; 0.02178817719841997, final\_error &#x3D; 0.024148114868439735</span><br><span class="line">5199&#x2F;5200 (epoch 199), train\_loss &#x3D; -2.924, time&#x2F;batch &#x3D; 0.603, mean error &#x3D; 0.035233428867844245, final\_error &#x3D; 0.036289180340245364</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-79fae3377250006a5521fb82983b7e62_720w.jpg"></p>
<p>最后放一些测试效果的图吧，绿色是Ground Truth Trajectory，红色是Prediction Trajectory。</p>
<p>最终效果中，预测Trajectory与Ground Truth Trajectory的绝对偏差并不大，因为行人的运动速度通常不会太快。但最终的预测趋势与真实的运动意图个人感觉还比较大，不确定是模型的问题，还是行人运动预测难度比较大，单凭LSTM很难搞定。后面再尝试下Social LSTM，看看效果。</p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-2c6eca62a722134c41825e200e8bfe51_720w.jpg"></p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-7464b730afec00a99d8d4cc9738819a1_720w.jpg"></p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-a0b714e1f20cc24717b90d795fc605ea_720w.jpg"></p>
<p><img src="/2020/07/11/rnn%E9%A2%84%E6%B5%8B%E8%A1%8C%E4%BA%BA%E8%BF%90%E5%8A%A8%E8%BD%A8%E8%BF%B9/v2-fd8e403ac5d3c4a411d8e2d2f497368e_720w.jpg"></p>
<h1 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h1><p>1.<a href="https://github.com/xuerenlv/social-lstm-tf">https://github.com/xuerenlv/social-lstm-tf</a><br>2.<a href="https://github.com/quancore/social-lstm">https://github.com/quancore/social-lstm</a><br>3.<a href="https://www.tensorflow.org/tutorials/text/text/_generation?hl=zh-cn">https://www.tensorflow.org/tutorials/text/text\_generation?hl=zh-cn</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>RNN</tag>
        <tag>行人轨迹预测</tag>
        <tag>轨迹预测</tag>
      </tags>
  </entry>
  <entry>
    <title>Waymo-自动驾驶长尾问题挑战</title>
    <url>/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/</url>
    <content><![CDATA[<p>尽管Waymo已经在开放道路上积累超过10 Million Miles，Waymo的工程师们仍然发现有层出不穷的新自动驾驶场景待解决。</p>
<h1 id="自动驾驶长尾场景举例"><a href="#自动驾驶长尾场景举例" class="headerlink" title="自动驾驶长尾场景举例"></a>自动驾驶长尾场景举例</h1><p><strong>场景一</strong>：一个骑自行车的人手中拿着一个Stop Sign标识牌。我们不知道它何时会举起标识牌。无人车必须理解这种场景，即使他举起了Stop Sign标识牌，自动驾驶汽车也不应该停下来。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-07-22-03-14-1024x574.png"></p>
<span id="more"></span>

<p><strong>场景二:</strong> 迎面而来的车辆上装载的塑料管子撒了一地，自动驾驶汽车必须学会应对这种突发情况，并且避开它们对无人车行驶的影响。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-07-22-08-44-1024x370.png"></p>
<p><strong>场景三：</strong>由于道路施工等因素，路面布满锥桶。无人车必须正确识别这些场景，在布满路面锥桶的场景下实现合理驾驶。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_10s.gif"></p>
<p><strong>场景四：</strong>路口绿灯，无人车拥有路权，虽然我们的无人车先到达路口，但必须为稍后到达的特种车辆让行。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_8s.gif"></p>
<p><strong>场景五：</strong> 路口绿灯，无人车准备左转，遇到闯红灯高速通过的社会车辆，无人车需要识别这种场景，并及时停车避让违规车辆。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_5s-1.gif"></p>
<h1 id="自动驾驶核心模块-Perception-Prediction和Planning"><a href="#自动驾驶核心模块-Perception-Prediction和Planning" class="headerlink" title="自动驾驶核心模块-Perception, Prediction和Planning"></a>自动驾驶核心模块-Perception, Prediction和Planning</h1><p>Perception、Prediction和Planning模块是自动驾驶的核心模块，每个模块都存在巨大的挑战。</p>
<h2 id="Perception"><a href="#Perception" class="headerlink" title="Perception"></a>Perception</h2><p>Perception输入：传感器(激光雷达)输入信息以及场景的先验信息。</p>
<p>Perception输出：道路交通对象(行人、车辆等)，对道路场景的语义分割和理解。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-08-56-17.png"></p>
<p>Perception本身是一个非常复杂、高难度的问题，它必须能够识别各种形态各异、不同种类的对象。比如下左一图，一群穿着恐龙服的行人，感知必须能够正确识别它们。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-03-50-1024x338.png"></p>
<p>相同的物体在不同的时间、不同的季节它们的外观表现也会有很大的差异，这会对Perception带来巨大挑战。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-12-28-1024x311.png"></p>
<p>各种复杂场景的分割理解难度极高。如下图左一：一个搬着箱子的人；下图左三：骑马的人。Perception必须能够正确的分割识别这些场景，而不会因为遮挡导致出现识别的错误。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-16-52-1024x300.png"></p>
<h2 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h2><p>Perception对检测到的物体进行下一步行为的预测，以辅助自动驾驶车辆进行合理的行为决策。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-29-49-1024x259.png"></p>
<p>Perception要考虑物体的历史行为，比如车辆不会在短时间内实现90度的转弯，因此我们可以假设车辆在短时间内仍然按照当前的朝向和速度前进；要对场景有更高语义层面的理解；要能够关注到不同对象的属性差异和视觉线索，比如车辆大概率是会在车道上行驶上，行人会走斑马线，车辆的朝向能够大概率反应它的意图，如果行人做出停车的手势，大概率是要过马路；要能够解决待预测物体与其它物体的行为交互。</p>
<p>如下图所示，路边有一辆静止的车辆，骑自行车的人在靠近静止车辆时，会侵入无人车车道。Perception模块需要正确理解这些场景，并生成合理的预测曲线。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_5s.gif"></p>
<p>如何能够准确的预测社会车辆的行为仍然是一个存在巨大挑战的开放性问题。</p>
<h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>Planning是Decision Making Machine，它基于Perception和Prediction的输出，规划车辆的行为，并输出Control模块，控制车辆的加减速、刹车等行为。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-09-51-50.png"></p>
<p>Planning首要考虑的是安全(safe)，其次要考虑驾乘的舒适性(comfortable)，再次要能够与其它交通参与者正确交互，最后要保证乘客送达目的地。如何能够满足这些条件实现良好的Planning效果仍然是一个开放性的问题。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x432_10s-1-2.gif"></p>
<h1 id="大规模机器学习技术-Machine-Learning-At-Scale"><a href="#大规模机器学习技术-Machine-Learning-At-Scale" class="headerlink" title="大规模机器学习技术(Machine Learning At Scale)"></a>大规模机器学习技术(Machine Learning At Scale)</h1><p>Machine Learning是解决自动驾驶长尾问题的一种有效工具。利用Machine Learning技术可以实现从数据采集、标注、训练、车端部署的闭环循环流程，从而实现Case的不断积累，模型的不断完善。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-22-39-1024x426.png"></p>
<h2 id="Automated-Machine-Learning技术"><a href="#Automated-Machine-Learning技术" class="headerlink" title="Automated Machine Learning技术"></a>Automated Machine Learning技术</h2><p>Waymo使用了Automated Machine Learning技术生成和优化针对无人车的数据模型，极大提升了模型训练的效率。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-37-13-1024x530.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-39-29-1024x521.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-40-27-1024x425.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-43-04-1024x523.png"></p>
<h2 id="机器学习技术的局限-Limits-Of-Machine-Learning"><a href="#机器学习技术的局限-Limits-Of-Machine-Learning" class="headerlink" title="机器学习技术的局限(Limits Of Machine Learning)"></a>机器学习技术的局限(Limits Of Machine Learning)</h2><p>机器学习模型不能解决所有的问题，但我们需要的是一个安全的自动驾驶系统，所以必须有其它措施来补充ML的不足。</p>
<p>首先可以借助于冗余互补的传感器辅助解决这个问题。车辆同时配备了视觉、Lidar、Radar系统，各个系统彼此独立，相互补充，以最大限度保证无人车不会缺失任何信息。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-48-43-1024x599.png"></p>
<p>其次，我们可以采用ML和Non-ML混合系统，利用专家系统来弥补ML的不足。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-10-56-06.png"></p>
<h1 id="大规模的测试技术-Large-Scale-Testing"><a href="#大规模的测试技术-Large-Scale-Testing" class="headerlink" title="大规模的测试技术(Large Scale Testing)"></a>大规模的测试技术(Large Scale Testing)</h1><p>首先Waymo有庞大的自动驾驶车队，可以支撑大规模的测试。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-16-08-1024x494.png"></p>
<p>有些场景在实际道路上出现的概率很低，为了测试验证这些低频问题，需要自己构建场景，进行结构化测试。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-19-08-1024x471.png"></p>
<p>仿真是一种重要的验证测试手段，可以轻量级安全的构造各种各样的测试场景。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-22-53.png"></p>
<p>自动驾驶仿真必须能够真实模拟车辆和行人的行为。这仅仅依靠简单的规则模型是不够的，我们需要更加复杂的模型，Waymo使用一种Mid-2-Mid的Drive Agent机器学习模型，它接收定位、感知等信息，输出更加拟人化的运动规划。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-40-00-1024x269.png"></p>
<p>Waymo提出的ChauffeurNet将Map、交通规则、道路环境等信息转化为图像信息，从而可以最大限度的利用比较成熟的机器学习模型，最终输出Agent的Trajectory。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-11-46-38-1024x543.png"></p>
<p>ChauffeurNet可以解决大部分简单场景下的Prediction和Planning问题。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/unnamed.gif"></p>
<p>场景中红色的拖尾是Agent的历史轨迹，绿色是未来2s的预测轨迹。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/unnamed-1.gif"></p>
<p>主车成功的通过路边静止车辆的场景</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/unnamed-2.gif"></p>
<p>主车遇到缓慢前行的车辆后减速</p>
<p>当然ChauffeurNet也有其局限性，比如以下复杂场景目前还不能很好的处理。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_406x306_10s.gif"></p>
<p>主车由于视距遮挡，直接冲出了路口</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_406x306_8s.gif"></p>
<p>车辆没有成功完成掉头操作</p>
<h1 id="机器学习难以覆盖的长尾问题挑战"><a href="#机器学习难以覆盖的长尾问题挑战" class="headerlink" title="机器学习难以覆盖的长尾问题挑战"></a>机器学习难以覆盖的长尾问题挑战</h1><p>对自动驾驶测试来讲，最大的挑战在于很难收集到所有Corner Case。如下图所示，是人类驾驶行为分布，要经过非常长时间的积累才能得到一些Corner的驾驶行为Case。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-15-46-53.png"></p>
<p>在自动驾驶网络的神经网络模型中，可能有上千万的参数，如果Corner Case的样本数量太少，就难以保证网络模型能够学会这些Corner场景。</p>
<p>在神经网络模型覆盖长尾Case前，如何来解决长尾Case呢？专家系统是一个选择。专家系统融入专业的知识，通过小批量的样本就可以获得效果比较好的参数。</p>
<p>比如我们计划得到实现一个轨迹优化机器学习模型，在基于运动控制理论和一系列的约束设计好专家模型之后，通过采集历史车辆轨迹，我们就可以调整参数最小化Cost的方法，使得专家系统的轨迹输出尽可能的逼近人类驾驶轨迹。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-15-53-40-1024x576.png"></p>
<p>轨迹优化专家系统的另一种模型是Inverse Reinforcement Learning技术，通过历史驾驶轨迹训练模型参数，使得它的输出尽可能的逼近预期效果。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-02-30.png"></p>
<p>如下图所示，红色的主车，蓝色的是社会车辆。左图的社会车辆更加保守，右侧的社会车辆更加激进。用保守的轨迹训练出的模型表现就趋于保守，用激进的轨迹训练出的模型表现就趋于激进。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/gifhome_774x222_17s.gif"></p>
<h1 id="Smart-Agent对于自动驾驶规模化不可或缺"><a href="#Smart-Agent对于自动驾驶规模化不可或缺" class="headerlink" title="Smart Agent对于自动驾驶规模化不可或缺"></a>Smart Agent对于自动驾驶规模化不可或缺</h1><p>不管是专家系统，还是神经网络，它们都在努力模拟人的驾驶行为，使Agent变得聪明起来，聪明的Agent可以辅助自动驾驶技术快速规模化。</p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-19-28-1024x319.png"></p>
<p><img src="/2020/02/08/waymo-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%95%BF%E5%B0%BE%E9%97%AE%E9%A2%98%E6%8C%91%E6%88%98/Screenshot-from-2020-02-08-16-21-58-1024x433.png"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>waymo</tag>
        <tag>自动驾驶长尾问题</tag>
      </tags>
  </entry>
  <entry>
    <title>未知环境下的Lidar概率占位栅格图(Occupancy Grid Map) Python代码实现</title>
    <url>/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>前面文章&lt;&lt;自动驾驶Mapping-占位栅格图(Occupancy Grid Map)&gt;&gt;中介绍了概率占位栅格地图(Probabilistic Occupancy Grid)的原理，并推导了如何利用贝叶斯理论(Bayes Theorem)更新生成概率占位栅格地图。下面看看如何用Python代码实现未知环境中的运动车辆上安装的激光雷达(lidar)生成概率占位栅格图。</p>
<h4 id="１、构建环境地图和车辆运动模型"><a href="#１、构建环境地图和车辆运动模型" class="headerlink" title="１、构建环境地图和车辆运动模型"></a>１、构建环境地图和车辆运动模型</h4><p>在生成栅格地图之前，首先需要构造一个用于车辆运动的环境地图(这个地图是用于仿真的真值，对于车辆来说是未知的环境)。我们用０和１值来构造Ｍ*N的环境地图，０表示可行驶区域，１表示占用区域。</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">M = <span class="number">50</span></span><br><span class="line">N = <span class="number">60</span></span><br><span class="line">true_map = np.zeros((M, N))</span><br><span class="line">true_map[<span class="number">0</span>:<span class="number">10</span>, <span class="number">0</span>:<span class="number">10</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">30</span>:<span class="number">35</span>, <span class="number">40</span>:<span class="number">45</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">3</span>:<span class="number">6</span>,<span class="number">40</span>:<span class="number">60</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">20</span>:<span class="number">30</span>,<span class="number">25</span>:<span class="number">29</span>] = <span class="number">1</span></span><br><span class="line">true_map[<span class="number">40</span>:<span class="number">50</span>,<span class="number">5</span>:<span class="number">25</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>然后构建车辆的运动模型。这里实现了一个简单的运动模型：车辆遇到障碍物或者到达地图边界之前，沿一个方向一直行驶；遇到障碍物或者到达地图边界之后，调整方向继续行驶。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initializing the robot&#x27;s location.</span></span><br><span class="line">x_0 = [<span class="number">30</span>, <span class="number">30</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># The sequence of robot motions.</span></span><br><span class="line">u = np.array([[<span class="number">3</span>, <span class="number">0</span>, -<span class="number">3</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, -<span class="number">3</span>]])</span><br><span class="line">u_i = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the vector of states for our simulation.</span></span><br><span class="line">x = np.zeros((<span class="number">3</span>, <span class="built_in">len</span>(time_steps)))</span><br><span class="line">x[:, <span class="number">0</span>] = x_0</span><br><span class="line"> </span><br><span class="line"><span class="keyword">while</span>(Some Conditon...) :</span><br><span class="line">　　<span class="comment"># Perform robot motion.</span></span><br><span class="line">    move = np.add(x[<span class="number">0</span>:<span class="number">2</span>, t-<span class="number">1</span>], u[:, u_i]) </span><br><span class="line">    <span class="comment"># If we hit the map boundaries, or a collision would occur, remain still.</span></span><br><span class="line">    <span class="keyword">if</span> (move[<span class="number">0</span>] &gt;= M - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &gt;= N - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">0</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> true_map[<span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">0</span>])), <span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">1</span>]))] == <span class="number">1</span>:</span><br><span class="line">        x[:, t] = x[:, t-<span class="number">1</span>]</span><br><span class="line">        u_i = (u_i + <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x[<span class="number">0</span>:<span class="number">2</span>, t] = move</span><br></pre></td></tr></table></figure>

<p>车辆的运动效果如下所示：</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/i2bis-0v5o3.gif"></p>
<p>最后要构建激光雷达(Lidar)的旋转模型。这里假设在车辆运动过程中，激光雷达(lidar)以0.3/Step的速度持续旋转，对周围的环境进行扫描。</p>
<p>$$<br>x[2, t] = (x[2, t-1] + w[t]) % (2 * math.pi)<br>$$</p>
<h4 id="２、生成激光雷达-Lidar-测量数据"><a href="#２、生成激光雷达-Lidar-测量数据" class="headerlink" title="２、生成激光雷达(Lidar)测量数据"></a>２、生成激光雷达(Lidar)测量数据</h4><p>有了地图和车辆运动模型，我们看看如何生成运动车辆上的激光雷达(lidar)扫描数据。</p>
<p>首先，我们需要搞清楚激光雷达的外参和内参，并以此推导出激光雷达(lidar)在Map坐标系下的姿态(x, y, $\theta$)和激光雷达(lidar)的激光束的水平和垂直角度分布(激光束的水平和垂直角度分布跟激光雷达自身的硬件属性相关，一般可以从Lidar产品说明书中获取)。</p>
<p>其次，我们需要知道激光雷达(Lidar)的最大扫描范围，超出该范围的区域不能被当前位置的Lidar扫描到，因而是定义为未知区域。最大扫描范围其实也是跟激光雷达自身属性相关的参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parameters for the sensor model.</span></span><br><span class="line">meas_phi = np.arange(-<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.05</span>)</span><br><span class="line">rmax = <span class="number">30</span> <span class="comment"># Max beam range.</span></span><br><span class="line">alpha = <span class="number">1</span> <span class="comment"># Width of an obstacle (distance about measurement to fill in).</span></span><br><span class="line">beta = <span class="number">0.05</span> <span class="comment"># Angular width of a beam.</span></span><br></pre></td></tr></table></figure>

<p>基于已知环境地图、车辆位置、Lidar激光束分布和Lidar最大扫描范围获取Lidar扫描数据的详细的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ranges</span>(<span class="params">true_map, X, meas_phi, rmax</span>):</span></span><br><span class="line">    (M, N) = np.shape(true_map)</span><br><span class="line">    x = X[<span class="number">0</span>]</span><br><span class="line">    y = X[<span class="number">1</span>]</span><br><span class="line">    theta = X[<span class="number">2</span>]</span><br><span class="line">    meas_r = rmax * np.ones(meas_phi.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Iterate for each measurement bearing.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(meas_phi)):</span><br><span class="line">        <span class="comment"># Iterate over each unit step up to and including rmax.</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, rmax+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Determine the coordinates of the cell.</span></span><br><span class="line">            xi = <span class="built_in">int</span>(<span class="built_in">round</span>(x + r * math.cos(theta + meas_phi[i])))</span><br><span class="line">            yi = <span class="built_in">int</span>(<span class="built_in">round</span>(y + r * math.sin(theta + meas_phi[i])))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If not in the map, set measurement there and stop going further.</span></span><br><span class="line">            <span class="keyword">if</span> (xi &lt;= <span class="number">0</span> <span class="keyword">or</span> xi &gt;= M-<span class="number">1</span> <span class="keyword">or</span> yi &lt;= <span class="number">0</span> <span class="keyword">or</span> yi &gt;= N-<span class="number">1</span>):</span><br><span class="line">                meas_r[i] = r</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># If in the map, but hitting an obstacle, set the measurement range</span></span><br><span class="line">            <span class="comment"># and stop ray tracing.</span></span><br><span class="line">            <span class="keyword">elif</span> true_map[<span class="built_in">int</span>(<span class="built_in">round</span>(xi)), <span class="built_in">int</span>(<span class="built_in">round</span>(yi))] == <span class="number">1</span>:</span><br><span class="line">                meas_r[i] = r</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> meas_r</span><br></pre></td></tr></table></figure>

<h4 id="３、计算Inverse-Scanner-Model"><a href="#３、计算Inverse-Scanner-Model" class="headerlink" title="３、计算Inverse Scanner Model"></a>３、计算Inverse Scanner Model</h4><p>获取激光雷达(Lidar)的测量数据之后，下一步就是将其关联匹配到地图的Map Cell上。主要流程是：</p>
<p>1）将 Lidar bearing与Map Cell相对于传感器的方位进行最小误差匹配，得到影响当前Map Cell的激光束；</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/Screenshot-from-2020-02-01-12-38-07-1-1024x461.png"></p>
<p>匹配的代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = math.sqrt((i - x)**<span class="number">2</span> + (j - y)**<span class="number">2</span>)</span><br><span class="line">phi = (math.atan2(j - y, i - x) - theta + math.pi) % (<span class="number">2</span> * math.pi) - math.pi</span><br><span class="line">            </span><br><span class="line"><span class="comment"># Find the range measurement associated with the relative bearing.</span></span><br><span class="line">k = np.argmin(np.<span class="built_in">abs</span>(np.subtract(phi, meas_phi)))</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>计算每个Cell被占用的概率。计算完成之后，得到三种不同类型的区域：未探测区域、障碍物区域和非障碍物区域，并赋给它们不同的占用概率。这里将未探测区域的占用概率设为0.5，表示不确定是否占用；障碍物区域占用概率等于0.7，表示大概率被占用；可行驶区域占用概率0.3，表示小概率被占用。</li>
</ol>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/Screenshot-from-2020-02-01-14-59-27-1-1024x472.png"></p>
<p>完整的Inverse Scanner Model的实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inverse_scanner</span>(<span class="params">num_rows, num_cols, x, y, theta, meas_phi, meas_r, rmax, alpha, beta</span>):</span></span><br><span class="line">    m = np.zeros((M, N))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_rows):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_cols):</span><br><span class="line">            <span class="comment"># Find range and bearing relative to the input state (x, y, theta).</span></span><br><span class="line">            r = math.sqrt((i - x)**<span class="number">2</span> + (j - y)**<span class="number">2</span>)</span><br><span class="line">            phi = (math.atan2(j - y, i - x) - theta + math.pi) % (<span class="number">2</span> * math.pi) - math.pi</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Find the range measurement associated with the relative bearing.</span></span><br><span class="line">            k = np.argmin(np.<span class="built_in">abs</span>(np.subtract(phi, meas_phi)))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the range is greater than the maximum sensor range, or behind our range</span></span><br><span class="line">            <span class="comment"># measurement, or is outside of the field of view of the sensor, then no</span></span><br><span class="line">            <span class="comment"># new information is available.</span></span><br><span class="line">            <span class="keyword">if</span> (r &gt; <span class="built_in">min</span>(rmax, meas_r[k] + alpha / <span class="number">2.0</span>)) <span class="keyword">or</span> (<span class="built_in">abs</span>(phi - meas_phi[k]) &gt; beta / <span class="number">2.0</span>):</span><br><span class="line">                m[i, j] = <span class="number">0.5</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the range measurement lied within this cell, it is likely to be an object.</span></span><br><span class="line">            <span class="keyword">elif</span> (meas_r[k] &lt; rmax) <span class="keyword">and</span> (<span class="built_in">abs</span>(r - meas_r[k]) &lt; alpha / <span class="number">2.0</span>):</span><br><span class="line">                m[i, j] = <span class="number">0.7</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the cell is in front of the range measurement, it is likely to be empty.</span></span><br><span class="line">            <span class="keyword">elif</span> r &lt; meas_r[k]:</span><br><span class="line">                m[i, j] = <span class="number">0.3</span></span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> m</span><br></pre></td></tr></table></figure>

<p>Inverse Scanner Model的测量结果如下图所示：</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/cv1uj-nhp9b.gif"></p>
<h4 id="４、生成概率占位栅格地图-Probabilistic-Occupancy-Grid"><a href="#４、生成概率占位栅格地图-Probabilistic-Occupancy-Grid" class="headerlink" title="４、生成概率占位栅格地图(Probabilistic Occupancy Grid)"></a>４、生成概率占位栅格地图(Probabilistic Occupancy Grid)</h4><p>生成概率占位地图的过程就是循环对激光雷达(lidar)的测量结果应用Inverse Scanner Model，然后更新各个Map Cell的Log Odds的过程详细推导过程参见：&lt;&lt;自动驾驶Mapping-占位栅格图(Occupancy Grid Map)&gt;&gt;:</p>
<p>$l_{t, i}=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+l_{t-1, i}-l_{0, i}$</p>
<p>其中: $\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)$是Inverse Measurement Model，$l_{t-1, i}$是网格i在t-1时刻的置信度(belif)，$l_{0,i}$是Initial belief。</p>
<p>最后，将log odds还原为真实概率，得到每个网格的占位概率值。</p>
<p>$p = e^{l_{t}} / (1 + e^{1_{t}})$</p>
<p>生成概率占位地图的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">meas_rs = []</span><br><span class="line">meas_r = get_ranges(true_map, x[:, <span class="number">0</span>], meas_phi, rmax)</span><br><span class="line">meas_rs.append(meas_r)</span><br><span class="line">invmods = []</span><br><span class="line">invmod = inverse_scanner(M, N, x[<span class="number">0</span>, <span class="number">0</span>], x[<span class="number">1</span>, <span class="number">0</span>], x[<span class="number">2</span>, <span class="number">0</span>], meas_phi, meas_r, rmax, alpha, beta)</span><br><span class="line">invmods.append(invmod)</span><br><span class="line">ms = []</span><br><span class="line">ms.append(m)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Main simulation loop.</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(time_steps)):</span><br><span class="line">    <span class="comment"># Perform robot motion.</span></span><br><span class="line">    move = np.add(x[<span class="number">0</span>:<span class="number">2</span>, t-<span class="number">1</span>], u[:, u_i]) </span><br><span class="line">    <span class="comment"># If we hit the map boundaries, or a collision would occur, remain still.</span></span><br><span class="line">    <span class="keyword">if</span> (move[<span class="number">0</span>] &gt;= M - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &gt;= N - <span class="number">1</span>) <span class="keyword">or</span> (move[<span class="number">0</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> (move[<span class="number">1</span>] &lt;= <span class="number">0</span>) <span class="keyword">or</span> true_map[<span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">0</span>])), <span class="built_in">int</span>(<span class="built_in">round</span>(move[<span class="number">1</span>]))] == <span class="number">1</span>:</span><br><span class="line">        x[:, t] = x[:, t-<span class="number">1</span>]</span><br><span class="line">        u_i = (u_i + <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x[<span class="number">0</span>:<span class="number">2</span>, t] = move</span><br><span class="line">    x[<span class="number">2</span>, t] = (x[<span class="number">2</span>, t-<span class="number">1</span>] + w[t]) % (<span class="number">2</span> * math.pi)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Gather the measurement range data, which we will convert to occupancy probabilities</span></span><br><span class="line">    meas_r = get_ranges(true_map, x[:, t], meas_phi, rmax)</span><br><span class="line">    meas_rs.append(meas_r)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Given our range measurements and our robot location, apply inverse scanner model</span></span><br><span class="line">    invmod = inverse_scanner(M, N, x[<span class="number">0</span>, t], x[<span class="number">1</span>, t], x[<span class="number">2</span>, t], meas_phi, meas_r, rmax, alpha, beta)</span><br><span class="line">    invmods.append(invmod)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate and update the log odds of our occupancy grid, given our measured occupancy probabilities from the inverse model.</span></span><br><span class="line">    L = np.log(np.divide(invmod, np.subtract(<span class="number">1</span>, invmod))) + L - L0</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate a grid of probabilities from the log odds.</span></span><br><span class="line">    m = np.divide(np.exp(L), np.add(<span class="number">1</span>, np.exp(L)))</span><br><span class="line">    ms.append(m)</span><br></pre></td></tr></table></figure>

<p>生成概率占用地图的过程如下：</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/ut3rg-81b5y.gif"></p>
<p>最终生成的概率占用栅格地图如下图所示。可以看看它基本反应了真实的实际车辆运行环境。</p>
<p><img src="/2020/02/15/%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84lidar%E6%A6%82%E7%8E%87%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/Screenshot-from-2020-02-13-07-53-05.png"></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>Coursera自动驾驶课程：Motion Planning for Self-Driving Cars的Weekly Assignment: Occupancy Grid Generation</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>Occupancy Grid Map</tag>
        <tag>概率占位栅格地图</tag>
        <tag>自动驾驶Planning</tag>
        <tag>自动驾驶路径规划</tag>
      </tags>
  </entry>
  <entry>
    <title>机器人动态规划(Dynamic Programming)入门</title>
    <url>/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="什么是动态规划"><a href="#什么是动态规划" class="headerlink" title="什么是动态规划"></a>什么是动态规划</h1><p>CS专业出身的人大抵没有人不知道动态规划(Dynamic Programming)的，该算法的本质就是把复杂的大问题分解成相互重叠的简单子问题，将子问题的最优解层层组合起来，就得到了复杂大问题的最优解。</p>
<p>能用动态规划解决的问题必须满足两个条件：一是最优子结构。即问题的最优解所包含的子问题的解也是最优的；二是子问题相互重叠。即是当使用递归进行自顶向下的求解时,每次产生的子问题不总是新的问题,而是已经被重复计算过的问题。<br>最典型的经常被拿来讲解Dynamic Programming的例子就是斐波那契数列(Fibonacci sequence)，它的数学定义如下:</p>
<span id="more"></span>

<p>$$<br>\begin{aligned} <br>F(0) &amp; = 0,\\<br>F(1) &amp; = 1,\\<br>F(n) &amp; = F(n-1) + F(n-2),\\<br>\end{aligned}<br>$$</p>
<p>斐波那契数列(Fibonacci sequence)计算的Python实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span>(<span class="params">n</span>):</span></span><br><span class="line">    f = [<span class="number">0</span>] * (n + <span class="number">1</span>)</span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    f[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">        f[i] = f[i - <span class="number">1</span>] + f[i - <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> f[n]</span><br></pre></td></tr></table></figure>

<h1 id="动态规划算法在自动驾驶中的应用"><a href="#动态规划算法在自动驾驶中的应用" class="headerlink" title="动态规划算法在自动驾驶中的应用"></a>动态规划算法在自动驾驶中的应用</h1><p>在如下的自动驾驶场景中，无人车在位置A处进行右转，目标是达到位置G处。理想的驾驶路径是: </p>
<p>(位置A处右转)-&gt;(进入车道C)-&gt;(变道进入车道B)-&gt;(左转到达目标位置G)</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_map_scence-1024x841.png"></p>
<p>自动驾驶场景。</p>
<p>但是由于环境是随机的，我们的无人车在实际上路时，可能遇到各种情况。比如当我们计划从车道C变道进入车道B时，发现左侧被一辆大卡车挡住了；如果停下来等大卡车驶过之后再变道，会被车道C上无人车后方的司机拼命用大喇叭催你，无奈之下，我们只好放弃左转，继续直行，再寻求其它路径达到目的地。</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_stuck_scence-1024x863.png"></p>
<p>目标车道被阻塞的场景</p>
<p>所以最后的行驶路径可能就变成了:</p>
<p>(位置A处右转)-&gt;(进入车道C，直行)-&gt;(通过路口，进入车道D)-&gt;(连续右转，达到位置E)-&gt;(直行到达目标位置G)</p>
<p>这里可以看到，我们需要一种方法，使得在无人车放弃车道C到车道B的变道时继续前进时，能够快速找到下一条可通行路径。动态规划(Dynamic Programming)可以用来解决这类问题，它可以给出从任意一个位置出发到达目的地的最优路径。</p>
<h2 id="简化的问题"><a href="#简化的问题" class="headerlink" title="简化的问题"></a>简化的问题</h2><p>为了应用动态规划(Dynamic Programming)算法，我们首先看下简化版的问题。如下图所示，我们将道路区域按照空间进行网格划分，带阴影线的网格表示不可通行区域，G表示目标位置。</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_grid.png"></p>
<p>图中的蓝色箭头表示车辆在该位置的规划策略，也是我们要求解的目标，可以认为我们的目标就是通过Policy函数，将位置(x，y)映射到车辆的运动Action上。简化起见，我们假设车辆只有四个运动Action：向上、向下、向左、向右。</p>
<p>$$Policy(x, y) = Action\{left, right, up, down\}$$</p>
<p>如何将(x,y)转化为具体的Action呢？ 为了计算Action，我们首先需要为每个网格计算Value值。Value的大小与该网格距离目标位置的最短距离成正比。有了Value值之后，Action的方向就是从Value值大的网格指向Value值小的网格。</p>
<h2 id="网格Value的计算"><a href="#网格Value的计算" class="headerlink" title="网格Value的计算"></a>网格Value的计算</h2><p>每个Cell的Value的Value Function定义如下：</p>
<p>$f(x, y) = min_{(x^{\prime}, y^{\prime})} f(x^{\prime}, y^{\prime})$ + Cost</p>
<p>$(x^{\prime}, y^{\prime})$的取值为: $(x-1, y)$,$(x, y-1)$, $(x+1, y)$, $(x, y+1)$，即它的左、上、右、下四个方向的Cell； cost为Cell之间的Cost，这里取Cost为步长1。</p>
<p>可以看到，这是一个典型的动态规划的问题。我们看下如何使用动态规划(Dynamic Programming)算法求解每个Cell的Value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Map Represention</span></span><br><span class="line">grid = [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">goal = [<span class="built_in">len</span>(grid)-<span class="number">1</span>, <span class="built_in">len</span>(grid[<span class="number">0</span>])-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the cost associated with moving from a cell to an adjacent one</span></span><br><span class="line">cost = <span class="number">1</span> </span><br><span class="line"></span><br><span class="line">delta = [[-<span class="number">1</span>, <span class="number">0</span>], <span class="comment"># go up</span></span><br><span class="line">         [ <span class="number">0</span>, -<span class="number">1</span>], <span class="comment"># go left</span></span><br><span class="line">         [ <span class="number">1</span>, <span class="number">0</span>], <span class="comment"># go down</span></span><br><span class="line">         [ <span class="number">0</span>, <span class="number">1</span>]] <span class="comment"># go right</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_value</span>(<span class="params">grid,goal,cost</span>):</span></span><br><span class="line">  <span class="comment"># If a cell is a wall or it is impossible to reach the goal from a cell,assign that cell a value of 99.</span></span><br><span class="line">  value = [[<span class="number">99</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]</span><br><span class="line">    </span><br><span class="line">  change = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">  <span class="keyword">while</span> change:</span><br><span class="line">    change = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid)):</span><br><span class="line">      <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> goal[<span class="number">0</span>] == x <span class="keyword">and</span> goal[<span class="number">1</span>] == y:</span><br><span class="line">          <span class="keyword">if</span> value[x][y] &gt; <span class="number">0</span>:</span><br><span class="line">            value[x][y] = <span class="number">0</span></span><br><span class="line">            change = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> grid[x][y] == <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(delta)):</span><br><span class="line">            x2 = x + delta[a][<span class="number">0</span>]</span><br><span class="line">            y2 = y + delta[a][<span class="number">1</span>]</span><br><span class="line">            print((x2, y2))</span><br><span class="line">                        </span><br><span class="line">            <span class="keyword">if</span> x2 &gt;= <span class="number">0</span> <span class="keyword">and</span> x2 &lt; <span class="built_in">len</span>(grid) <span class="keyword">and</span> y2 &gt;= <span class="number">0</span> <span class="keyword">and</span> y2 &lt; <span class="built_in">len</span>(grid[<span class="number">0</span>]) <span class="keyword">and</span> grid[x2][y2] == <span class="number">0</span>:</span><br><span class="line">              v2 = value[x2][y2] + cost</span><br><span class="line">                </span><br><span class="line">              <span class="keyword">if</span> v2 &lt; value[x][y]:</span><br><span class="line">                change = <span class="literal">True</span></span><br><span class="line">                value[x][y] = v2</span><br><span class="line">                                </span><br><span class="line">  <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure>
<p>最后我们可以得到如下的Value值，通过它可以得到从任意位置到达目标位置的最短距离。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[11, 99, 7, 6, 5,  4],</span><br><span class="line"> [10, 99, 6, 5, 4,  3],</span><br><span class="line"> [9,  99, 5, 4, 3,  2],</span><br><span class="line"> [8,  99, 4, 3, 2,  1],</span><br><span class="line"> [7,  6,  5, 4, 99, 0]]</span><br></pre></td></tr></table></figure>

<p>将Value值映射为Policy，最终输出的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> [[&#39;v&#39;, &#39; &#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;v&#39;, &#39; &#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;v&#39;, &#39; &#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;v&#39;, &#39; &#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;v&#39;],</span><br><span class="line">  [&#39;&gt;&#39;, &#39;&gt;&#39;, &#39;^&#39;, &#39;^&#39;, &#39; &#39;, &#39;*&#39;]]</span><br></pre></td></tr></table></figure>

<h2 id="应用到车辆运动中"><a href="#应用到车辆运动中" class="headerlink" title="应用到车辆运动中"></a>应用到车辆运动中</h2><p>仍然以简化的方式来展示Dynamic Programming的应用。如下图所示，红色是车辆的当前位置，蓝色是车辆的目标姿态。假设车辆的运动角度$\theta$只有四个选择{Up, Down, Left, Right}， 车辆的运动只有三个选择: 左转、直行、右转。</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_sample_1.png"></p>
<p>我们首先构建地图信息：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0 = navigable space</span></span><br><span class="line"><span class="comment"># 1 = unnavigable space </span></span><br><span class="line">grid = [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>

<p>给定车辆的起始位置、结束位置和车辆的运动约束：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init = [<span class="number">4</span>, <span class="number">3</span>, <span class="number">0</span>] <span class="comment"># given in the form [row,col,direction]</span></span><br><span class="line"><span class="comment"># direction = 0: up</span></span><br><span class="line"><span class="comment">#             1: left</span></span><br><span class="line"><span class="comment">#             2: down</span></span><br><span class="line"><span class="comment">#             3: right</span></span><br><span class="line">                </span><br><span class="line">goal = [<span class="number">2</span>, <span class="number">0</span>] <span class="comment"># given in the form [row,col]</span></span><br><span class="line"></span><br><span class="line">forward = [[-<span class="number">1</span>,  <span class="number">0</span>], <span class="comment"># go up</span></span><br><span class="line">           [ <span class="number">0</span>, -<span class="number">1</span>], <span class="comment"># go left</span></span><br><span class="line">           [ <span class="number">1</span>,  <span class="number">0</span>], <span class="comment"># go down</span></span><br><span class="line">           [ <span class="number">0</span>,  <span class="number">1</span>]] <span class="comment"># go right</span></span><br><span class="line">forward_name = [<span class="string">&#x27;up&#x27;</span>, <span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;down&#x27;</span>, <span class="string">&#x27;right&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># action has 3 values: right turn, no turn, left turn</span></span><br><span class="line">action = [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">action_name = [<span class="string">&#x27;R&#x27;</span>, <span class="string">&#x27;#&#x27;</span>, <span class="string">&#x27;L&#x27;</span>]</span><br><span class="line"></span><br><span class="line">cost = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>] <span class="comment"># cost has 3 values, corresponding to making a right turn, no turn, and a left turn</span></span><br></pre></td></tr></table></figure>

<p>基于Dynamic Programming计算从起点到终点的车辆运动路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimum_policy2D</span>(<span class="params">grid,init,goal,cost</span>):</span></span><br><span class="line">    value = [[[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="number">999</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]]</span><br><span class="line">             </span><br><span class="line">    policy = [[[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))],</span><br><span class="line">             [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]]</span><br><span class="line">             </span><br><span class="line">    policy2D = [[<span class="string">&#x27; &#x27;</span> <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>]))] <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid))]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    change = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> change:</span><br><span class="line">        change = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid)):</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>])):</span><br><span class="line">                <span class="keyword">for</span> orientation <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                    <span class="keyword">if</span> goal[<span class="number">0</span>] == x <span class="keyword">and</span> goal[<span class="number">1</span>] == y:</span><br><span class="line">                        <span class="keyword">if</span> value[orientation][x][y] &gt; <span class="number">0</span>:</span><br><span class="line">                            value[orientation][x][y] = <span class="number">0</span></span><br><span class="line">                            policy[orientation][x][y] = <span class="string">&#x27;*&#x27;</span></span><br><span class="line">                            change = <span class="literal">True</span></span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> grid[x][y] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">                            o2 = (orientation + action[i]) % <span class="number">4</span></span><br><span class="line">                            x2 = x + forward[o2][<span class="number">0</span>]</span><br><span class="line">                            y2 = y + forward[o2][<span class="number">1</span>]</span><br><span class="line">                            </span><br><span class="line">                            <span class="keyword">if</span> x2 &gt;= <span class="number">0</span> <span class="keyword">and</span> x2 &lt; <span class="built_in">len</span>(grid) <span class="keyword">and</span> y2 &gt;= <span class="number">0</span> <span class="keyword">and</span> y2 &lt; <span class="built_in">len</span>(grid[<span class="number">0</span>]) <span class="keyword">and</span> grid[x2][y2] == <span class="number">0</span>:</span><br><span class="line">                                v2 = value[o2][x2][y2] + cost[i]</span><br><span class="line">                                </span><br><span class="line">                                <span class="keyword">if</span> v2 &lt; value[orientation][x][y]:</span><br><span class="line">                                    value[orientation][x][y] = v2</span><br><span class="line">                                    policy[orientation][x][y] = action_name[i]</span><br><span class="line">                                    change = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    x = init[<span class="number">0</span>]</span><br><span class="line">    y = init[<span class="number">1</span>]</span><br><span class="line">    orientation = init[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    policy2D[x][y] = policy[orientation][x][y]</span><br><span class="line">    <span class="keyword">while</span> policy[orientation][x][y] != <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> policy[orientation][x][y] == <span class="string">&#x27;#&#x27;</span>:</span><br><span class="line">            o2 = orientation</span><br><span class="line">        <span class="keyword">elif</span> policy[orientation][x][y] == <span class="string">&#x27;R&#x27;</span>:</span><br><span class="line">            o2 = (orientation - <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">        <span class="keyword">elif</span> policy[orientation][x][y] == <span class="string">&#x27;L&#x27;</span>:</span><br><span class="line">            o2 = (orientation + <span class="number">1</span>) % <span class="number">4</span></span><br><span class="line">            </span><br><span class="line">        x = x + forward[o2][<span class="number">0</span>]</span><br><span class="line">        y = y + forward[o2][<span class="number">1</span>]</span><br><span class="line">        orientation = o2</span><br><span class="line">        </span><br><span class="line">        policy2D[x][y] = policy[orientation][x][y]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> policy2D</span><br></pre></td></tr></table></figure>

<p>最终输出的路径结果如下:</p>
<p>[[‘ ‘, ‘ ‘, ‘ ‘, ‘R’, ‘#’, ‘R’],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘#’],<br> [‘*’, ‘#’, ‘#’, ‘#’, ‘#’, ‘R’],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘]]</p>
<p>在上面的实现中，我们将左转的Cost设置为20，比较高的左转代价使得车辆更倾向于直行和右转，所以规划路径的效果如下：</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_sample_2.jpg"></p>
<p>我们将将左转的Cost降低到2，看看会发生什么效果？</p>
<p>[[‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘, ‘ ‘],<br> [‘*’, ‘#’, ‘#’, ‘L’, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘],<br> [‘ ‘, ‘ ‘, ‘ ‘, ‘#’, ‘ ‘, ‘ ‘]]</p>
<p>可以看到，路径规划在路口位置选择了左转，规划效果如下：</p>
<p><img src="/2020/10/30/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92dynamic-programming%E5%85%A5%E9%97%A8/dp_sample_3.jpg"></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>本文内容源自Udacity的免费课程：Dynamic Programming - Artificial Intelligence for Robotics<br>Youtube链接: <a href="https://www.youtube.com/watch?v=r2bPY2s9wII&amp;t=12s">https://www.youtube.com/watch?v=r2bPY2s9wII&amp;t=12s</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
        <tag>动态规划</tag>
        <tag>机器人</tag>
        <tag>自动驾驶运动规划</tag>
        <tag>运动规划</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶Mapping-占位栅格图(Occupancy Grid Map)</title>
    <url>/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/</url>
    <content><![CDATA[<p>前面文章《自动驾驶运动规划(Motion Planning)》中提到可以使用占位图(Occupancy Grid Map)表示自动驾驶行驶区域的哪些区域被障碍物(如静止的车辆、路中间的石墩子、树木、路肩等)占用，Motion Planning模块会通过查询占位地图避开这些道路障碍物，避免与它们碰撞，从而达到安全驾驶的目的。</p>
<h1 id="占位栅格地图-Occupancy-Grid-Map"><a href="#占位栅格地图-Occupancy-Grid-Map" class="headerlink" title="占位栅格地图(Occupancy Grid Map)"></a>占位栅格地图(Occupancy Grid Map)</h1><p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-10-43-36-1024x604.png"></p>
<span id="more"></span>

<p>如上图所示，将车辆行驶道路环境用网格(Cell)切分，并且每个网格(Cell)用二值数值0和1填充，0表示该网格(Cell)被占用，1表示该网格(Cell)没有被占用。</p>
<p>$m^{i} \in \{0, 1\}$</p>
<p>由此，可以得到如下所示的一张栅格占位地图。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-10-54-24-1024x607.png"></p>
<p>要制作理想的占位栅格地图必须满足的以下几个假设条件：</p>
<p>1）占位栅格地图是对道路行驶区域中的静态环境(Static Environment)的描述。也就意味着，我们在制图前必须将地面、动态物体(车辆、行人等)从传感器数据中移除掉；</p>
<p>2）每个网格(Cell)与其它的所有网格的状态是相互独立的，即它的状态不受周围其它网格状态的影响；</p>
<p>3）在每个时刻，车辆的位置是精确的、已知的。</p>
<h1 id="概率占位栅格地图-Probabilistic-Occupancy-Grid-Map"><a href="#概率占位栅格地图-Probabilistic-Occupancy-Grid-Map" class="headerlink" title="概率占位栅格地图(Probabilistic Occupancy Grid Map)"></a>概率占位栅格地图(Probabilistic Occupancy Grid Map)</h1><p>在实际的应用中，车辆传感器的数据测量是存在误差的，车辆的定位结果也是存在误差的，动态障碍物的识别也是存在误差的，因此用概率表示一个网格(Cell)被占用的可能性是一个更加可行的方案。每个网格存储一个[0, 1]之间的概率值，这个值越大，表示网格被占用的可能性越大；这个值越小，表示网格被占用的可能性越小。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-11-32-38.png"></p>
<h1 id="概率占位栅格图-Probabilistic-Occupancy-Grid-Map-制图"><a href="#概率占位栅格图-Probabilistic-Occupancy-Grid-Map-制图" class="headerlink" title="概率占位栅格图(Probabilistic Occupancy Grid Map)制图"></a>概率占位栅格图(Probabilistic Occupancy Grid Map)制图</h1><p>栅格地图的每个Cell的概率值计算公式如下：</p>
<p>$bel_t(m^i) = p(m^i (y, x)_{1:t})$</p>
<p>其中$(y, x)_{1:t}$是1到t时刻的车辆位置和传感器测量结果，通过历史信息的累计，可以提升制作的地图的准确性。</p>
<p>如何将1到t时刻的所有传感器测量结果融合起来呢？贝叶斯理论(Bayes Theorem)是一个不错的选择。</p>
<p>$bel_t(m^i) = \eta p(y_t m^i) bel_{t-1}(m^i)$</p>
<p>其中$\eta$是归一化参数, $p(y_t m^i)$是传感器的测量模型。通过贝叶斯理论(Bayes Theorem)将多次传感器测量结果融合到同一个Cell中，从而获得高可信度的网格占用概率。</p>
<h2 id="贝叶斯理论-Bayes-Theorem-更新存在的问题"><a href="#贝叶斯理论-Bayes-Theorem-更新存在的问题" class="headerlink" title="贝叶斯理论(Bayes Theorem)更新存在的问题"></a>贝叶斯理论(Bayes Theorem)更新存在的问题</h2><p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-01-31-18-37-34.png"></p>
<p>重复的浮点数乘法运算导致计算结果的数值变得很小而难以精确表达和运算。Logit函数可以把自变量从(0,1)连续单调地映射到正负无穷。logit函数的定义如下：</p>
<p>$f(x) = log {\frac{x}{1 - x}}$</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/20180901104349204.png"></p>
<p>所以我们使用Logit函数替代标准的Bayes更新过程。</p>
<h2 id="贝叶斯更新过程的推导"><a href="#贝叶斯更新过程的推导" class="headerlink" title="贝叶斯更新过程的推导"></a>贝叶斯更新过程的推导</h2><p>贝叶斯理论(Bayes Theorem)更新网格(Cell)占用概率的公式如下：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(y_{t} y_{1: t-1}, m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(y_{t} y_{1: t-1}\right)} \tag{1}<br>$</p>
<p>根据一阶马尔科夫(Markov Assumption)假设，t时刻的状态只与t-1时刻的状态有关，因此公式(1)可写为如下形式：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(y_{t} m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(y_{t} y_{1: t-1}\right)} \tag{2}<br>$</p>
<p>对测量模型应用贝叶斯(Bayes Theorem)更新过程：</p>
<p>$<br>p\left(y_{t} m^{i}\right)=\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right)}{p\left(m^{i}\right)} \tag{3}<br>$</p>
<p>将公式3)代入公式2)，可得：</p>
<p>$<br>p\left(m^{i} y_{1: t}\right)=\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(m^{i}\right) p\left(y_{t} y_{1: t-1}\right)} \tag{4}<br>$</p>
<p>然后计算1-p的值：</p>
<p>$<br>p\left(\neg m^{i} y_{1: t}\right)=1-p\left(m^{i} y_{1: t}\right)=\frac{p\left(\neg m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i}\right) p\left(y_{t} y_{1: t-1}\right)} \tag{5}<br>$</p>
<p>将p和1-p代入logit函数：</p>
<p>$<br>\operatorname{logit}(p)=\log \left(\frac{p}{1-p}\right)<br>$</p>
<p>$<br>\begin{aligned}<br>\quad \frac{p\left(m^{i} y_{1: t}\right)}{p\left(\neg m^{i} y_{1: t}\right)} &amp; =\frac{\frac{p\left(m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(m^{i}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}}{\frac{p\left(\neg m^{i} y_{t}\right) p\left(y_{t}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i}\right) p\left(y_{t} y_{1: t-1}\right)}} \\<br>&amp;=\frac{p\left(m^{i} y_{t}\right) p\left(\neg m^{i}\right) p\left(m^{i} y_{1: t-1}\right)}{p\left(\neg m^{i} y_{t}\right) p\left(m^{i}\right) p\left(\neg m^{i} y_{1: t-1}\right)} \\<br>&amp;=\frac{p\left(m^{i} y_{t}\right)\left(1-p\left(m^{i}\right)\right) p\left(m^{i} y_{1: t-1}\right)}{\left(1-p\left(m^{i} y_{t}\right)\right) p\left(m^{i}\right)\left(1-p\left(m^{i} y_{1: t-1}\right)\right)}<br>\end{aligned} \tag{6}<br>$</p>
<p>对公式6）等号两侧取log，进行整理后，得到：</p>
<p>$<br>\operatorname{logit}\left(p\left(m^{i} y_{1: t}\right)\right)=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+\operatorname{logit}\left(p\left(m^{i} y_{1: t-1}\right)\right)-\operatorname{logit}\left(p\left(m^{i}\right)\right)<br>$</p>
<p>于是得到<strong>Bayes更新递推公式</strong>：</p>
<p>$<br>l_{t, i}=\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)+l_{t-1, i}-l_{0, i}<br>$</p>
<p>其中: $\operatorname{logit}\left(p\left(m^{i} y_{t}\right)\right)$是Inverse Measurement Model，$l_{t-1, i}$是网格i在t-1时刻的置信度(belif)，$l_{0,i}$是Initial belief。</p>
<p>可以看到，该递推公式应用的关键是Inverse Measurement Model：$p\left(m^{i} y_{t}\right))$，如何计算该值呢？</p>
<h2 id="Inverse-Measurement-Model"><a href="#Inverse-Measurement-Model" class="headerlink" title="Inverse Measurement Model"></a>Inverse Measurement Model</h2><p>占位栅格地图的传感器测量模型为：$p(y_t m^{i})$，表示基于已有的地图Cell概率，叠加传感器测量结果，得到新的占位概率值。</p>
<p>而现在我们要求解的是：$p(m^{i} y_t)$，这也是为什么该公式被成为Inverse Measurement Model的原因。</p>
<p>下面来看看Inverse Measurement Model如何计算？下面以二维激光雷达扫描模型来说明(注意：实际应用的激光雷达是3D的，这里用2D Lidar是为了简化模型，所用理论可以很好推广到3D模型)。</p>
<p><strong>2D Lidar模型</strong></p>
<p>它在2D平面上进行扫描，包含两个参数：Scanner bearing和Scanner rangers。Scanner bearing均匀的分布在[$-{\phi_{max}}^s, {\phi_{max}}^s$]之间，一般的我们可以认为它们均匀分布在360度的各个方向上。Scanner rangers是从Lidar中心到障碍物的距离，Lidar发出激光、接收回波，从而计算出到周围障碍物的距离；为了简化期间，我们也假设Lidar发送激光后立即收到回波，不存在时间延迟。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-06-50-1024x362.png"></p>
<p><strong>Map坐标系&amp;Vehicle坐标系&amp;传感器坐标系</strong></p>
<p>数学模型构建过程中<strong>坐标系</strong>是不可或缺的。这里主要涉及到三个坐标系：Map坐标系、Vehicle坐标系以及传感器坐标系。2D Lidar的测量结果都是相对于自身传感器中心的，即以2D Lidar中心为坐标原点；所有的测量结果最终都要转换到Map坐标系，完成地图制作的计算。</p>
<p>假设2D Lidar在Map坐标系中的姿态为$(x_{1,t}, x_{2,t}, x_{3,t})$，其中$x_{1,t}$和$x_{2,t}$是x和y坐标，$x_{3,t}$是传感器朝向。通过该姿态，可以将2D Lidar测量结果转换到Map坐标系。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-21-57-1024x755.png"></p>
<p><strong>Lidar测量结果与Map Cell关联匹配</strong></p>
<p>如何将2D Lidar模型与Map Cell关联起来呢？如下图所示，第i个Map Cell用$(r^{i}, {\phi}^i)$表示。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-12-38-07-1024x461.png"></p>
<p>然后通过2D Lidar bearing与Map Cell相对于传感器的方位进行最小误差匹配，得到影响当前Map Cell的激光束。</p>
<p>$k = argmin({\phi}_i - {\phi}_i^s)$</p>
<p>匹配的过程如下：首先定义两个值$\alpha$和$\beta$，各个网格Cell的概率计算如下：</p>
<p>1）如果$r^i &gt; {r_{max}}^s$或者$\phi^i - \phi_k^s &gt; \beta /2$， 表示为探测区域，没有信息，这些区域的概率值一般为0.5，表示不确定是否被占用。</p>
<ol>
<li><p>如果$r_k^s &lt; r_{max}^s$并且$r^i - r_k^s &lt; \alpha / 2$，表示该区域大概率被占用，因此要赋予一个大于0.5的概率值。</p>
</li>
<li><p>如果$r^i - r_k^s &gt; \alpha / 2$，这些网格被占用的概率较低，因此要赋予一个小于0.5的概率值。</p>
</li>
</ol>
<p><img src="/creenshot-from-2020-02-01-12-50-41-1024x462.png"></p>
<p>如下图所示，红色区域为高概率被占用区域，灰色区域为未知区域，其余区域为低概率被占用区域。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-14-59-27-1024x472.png"></p>
<p>至此，有了Inverse Measurement Model，Bayes更新的过程可以正常进行了。</p>
<p><strong>更高效的Inverse Measurement Model计算方法</strong></p>
<p>采用光线跟踪(Ray Tracing)的Bresenham’s Line Algorithm可以大大减少复杂的浮点数计算，提升计算效率。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-20-01.png"></p>
<h1 id="移除Lidar中地面和动态物体"><a href="#移除Lidar中地面和动态物体" class="headerlink" title="移除Lidar中地面和动态物体"></a>移除Lidar中地面和动态物体</h1><p>实际应用中的激光雷达(Lidar)是3D的，会扫描到大量的地面点，这些地面点如果不被移除，按照计算匹配模型，会被当做障碍物处理。所以需要将地面点点云数据从激光雷达点云中移除掉。如何移除呢？一种可行的方法是，通过自动化识别算法从Lidar点云中将地面识别并剔除。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-43-18-1024x652.png"></p>
<p>地面识别的难度是比较高的，因为很多道路路面内外的界限在点云中是不明确的，自动化识别算法会误把道路边界外的区域识别为道路路面，从而导致错误的地图信息等。通过视觉分割算法辅助点云识别可以提升路面的识别率。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-47-38-1024x511.png"></p>
<p>动态物体(行人、车辆等)也需要从点云数据中移除，这依赖于基于点云和图像的感知技术。但同样也存在很多技术难题，比如如何提升识别的准确率，如何将静止的车辆识别出来等等。</p>
<p><img src="/2020/02/01/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6mapping-%E5%8D%A0%E4%BD%8D%E6%A0%85%E6%A0%BC%E5%9B%BEoccupancy-grid-map/Screenshot-from-2020-02-01-15-48-41.png"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>本文主要整理自Coursera自动驾驶课程：Motion Planning for Self-Driving Cars第二周课程的学习笔记。</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>概率占位栅格地图</tag>
        <tag>Mapping技术</tag>
        <tag>占位栅格地图</tag>
        <tag>自动驾驶Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶定位算法-基于多传感器融合的状态估计(muti-Sensors Fusion)</title>
    <url>/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/</url>
    <content><![CDATA[<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-12-29-38-1-1024x470.png" alt="Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars -&gt; Multisensor Fusion for State Estimation"></p>
<h1 id="传感器-Sensor-选取"><a href="#传感器-Sensor-选取" class="headerlink" title="传感器(Sensor)选取"></a>传感器(Sensor)选取</h1><p>自动驾驶系统中用于状态估计(State Estimation)的常用传感器包括GPS/GNSS、IMU、激光雷达(Lidar)。</p>
<span id="more"></span>

<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-12-38-59-1024x226.png"></p>
<p>状态估计(State Estimation)选用传感器需要考虑哪些因素：</p>
<p>1）误差不相关性。也就是说，用于Sensor Fusion的传感器其中单个传感器(Sensor Measurement)测量失败，不会导致其它传感器(Sensor)由于相同的原因而同时失败。</p>
<p>2）传感器的相互补充性。 比如IMU可以填充GPS两次定位间隔期间的定位输出，用于平滑GPS/GNSS的定位结果；GPS为IMU提供初值，消除IMU单独使用出现的偏移(Drift)的问题；Lidar可以弥补定位精度的问题，而GNSS可以为Lidar定位地图匹配提供地图范围数据。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-13-18-59-1024x482.png" alt="松耦合的系统"></p>
<h1 id="传感器的标定-Sensor-Calibration"><a href="#传感器的标定-Sensor-Calibration" class="headerlink" title="传感器的标定(Sensor Calibration)"></a>传感器的标定(Sensor Calibration)</h1><p>如果想要各个传感器能够相互协同，无间配合，传感器的标定是必不可少的。传感器的标定通常分为三种: 内参标定(Intrinsic Calibration)、外参标定(Extrinsic Calibration)和时间校准(Temporal Calibration)。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-17-45-45-1024x285.png"></p>
<h2 id="内参标定-Intrinsic-Calibration"><a href="#内参标定-Intrinsic-Calibration" class="headerlink" title="内参标定(Intrinsic Calibration)"></a>内参标定(Intrinsic Calibration)</h2><p>传感器或者车辆的内参在传感器制造的时候就已经固定下来，传感器模型中的固定参数都是内参，都需要通过Intrinsic Calibration事先确定。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-17-57-49-1024x492.png"></p>
<p>比如估计车辆运动距离的轮速计模型$v=r \omega$中，r就一个内参。另外激光雷达(Lidar)中扫描线的角度，在激光雷达计算模型中需要事先知道这个参数，以实现激光雷达扫描线(Scan Line)的拼接。</p>
<p>如何获取传感器的内参呢？实践中有几种方法:</p>
<p>1）从传感器制造商的使用说明书中获取。这种方法往往只能获取大概的参数，每个设备的内参都是不同，所以并不能获取比较精确的参数。</p>
<p>2）手工测量内参。比如车轮的半径，可以通过手工测量的方法获取。但是类似于激光雷达的内参无法通过手工测量获取。</p>
<p>3）Estimate as part of State。这种方式不仅可以获取精确的传感器内参，而且可以解决内参随时间变化的情况。比如汽车的轮胎半径漏气导致半径变小等。</p>
<h2 id="外参标定-Extrinsic-Calibration"><a href="#外参标定-Extrinsic-Calibration" class="headerlink" title="外参标定(Extrinsic Calibration)"></a>外参标定(Extrinsic Calibration)</h2><p>传感器的外参主要表达各个传感器之间的位置相对姿态，它是把各个传感器的数据坐标统一起来的必不可少的参数。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-11-28-13.png"></p>
<p>如何获取传感器的外参呢？实践中有几种方法:</p>
<p>1、CAD图纸。如果你能获取传感器安装的CAD图纸，那你就可以获得比较准确的传感器外参。</p>
<p>2、手动测量。当然手动测量的难度也非常高，因为传感器的中心往往在传感器内部，难以精确测量。</p>
<p>3、Estimate as part of State。这也是一个研究的方向。可以比较好的应对外参标定问题，但难度也比较高。</p>
<h2 id="时间校准-Temporal-Calibration"><a href="#时间校准-Temporal-Calibration" class="headerlink" title="时间校准(Temporal Calibration)"></a>时间校准(Temporal Calibration)</h2><p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-19-03-16-1024x380.png"></p>
<p>时间校准对于各个传感器的数据融合至关重要。比如IMU的输出频率是200HZ，Lidar的输出频率是20HZ，只有按照最相近的时间进行对齐，才能将IMU和Lidar数据准确融合起来。</p>
<p>在实际应用中，各个传感器的相对时间误差是未知的，这些误差可能是由于各个传感器的预处理耗时不同导致的，也可能是由于各个传感器的计时器精度不同造成的。</p>
<p>如何校准传感器的时间呢？实践中有几种方法:</p>
<p>1）假设这些传感器的时间相对误差为0。当然忽略这些误差，会导致最终的融合结果比预期要差。</p>
<p>2）硬件同步。在硬件设计上保证各个传感器的时间戳对齐。</p>
<h1 id="EKF-多传感器融合-Multi-Sensors-Fusion"><a href="#EKF-多传感器融合-Multi-Sensors-Fusion" class="headerlink" title="EKF-多传感器融合(Multi-Sensors Fusion)"></a>EKF-多传感器融合(Multi-Sensors Fusion)</h1><p>自动驾驶对车辆的状态(Vehicle State)的描述一般包括：位置(Position，一般为三维空间坐标x、y、z)、速度($v_x, v_y, v_z$)、朝向(四元数，x，y，z，w)，它是一个10维向量。</p>
<p>$$<br>x_k =<br>\begin{bmatrix}<br>p_k \\<br>v_k \\<br>q_k \\<br>\end{bmatrix}<br>\in R^{10}<br>$$</p>
<p>自动驾驶汽车一般包含多个Camera、3D 激光雷达(Lidar)、惯性测量单元(IMU)、多个Radar、GPS/GNSS Reciver、轮速计(Wheel Odmetry)，这些传感器在运行过程中时刻都在以不同的频率发送不同类型的数据，多传感器融合模块需要将这些信息融合起来，不断更新自动驾驶车辆的状态(Vehicle State)。多传感器融合进行状态估计(State Estimation)的流程如下：</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/1_7r_YClMa9KdIYmCWKJTEgA.png"></p>
<p>车辆运动模型(Motion Model Input)如下，它的信息一般来自于IMU，包含x、y、z三个方向上的加速度和角速度，是一个6维向量。</p>
<p>$$<br>u_k =<br>\begin{bmatrix}<br>f_k \\<br>\omega_k \\<br>\end{bmatrix}<br>\in R^6<br>$$</p>
<p>车辆运动模型的计算过程如下:</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-16-30-17-1024x467.png"></p>
<p>为了应用EKF，我们定义Error State如下，其中$\phi_k$是3x1的矩阵。</p>
<p>$$<br>\delta x_k =<br>\begin{bmatrix}<br>\delta p_k \\<br>\delta v_k \\<br>\delta \phi_k \\<br>\end{bmatrix}<br>\in R^6<br>$$</p>
<p>EKF的Motion Model如下:</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-10-46-1024x473.png"></p>
<p>EKF中的GNSS测量模型:</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-16-04-1024x519.png"></p>
<p>EKF中的Lidar测量模型:</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-16-20-1024x555.png"></p>
<p>这里假设激光雷达(Lidar)的测量结果和GNSS的测量结果都在同一个坐标系下(注意，实际情况下，需要经过坐标变换才能达到这种效果)</p>
<h2 id="EKF的IMU-GNSS-Lidar多传感器融合流程如下"><a href="#EKF的IMU-GNSS-Lidar多传感器融合流程如下" class="headerlink" title="EKF的IMU+GNSS+Lidar多传感器融合流程如下:"></a>EKF的IMU+GNSS+Lidar多传感器融合流程如下:</h2><p>1）Update State With IMU Inputs</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-22-00-1024x591.png" alt="Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars -&gt;  Sensor Calibration - A Necessary Evil"></p>
<p>2、Propagate Uncertainty</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-30-02-1024x394.png" alt="Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars -&gt;  Sensor Calibration - A Necessary Evil"></p>
<p>3、当有GNSS或者LIDAR测量结果到达时，进入步骤4），否则进入步骤1）。</p>
<p>4、计算GNSS/Lidar的卡尔曼增益(Kalman Gain)。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-33-50-1024x417.png" alt="Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars -&gt;  Sensor Calibration - A Necessary Evil"></p>
<p>4、计算Error State。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-37-59-1024x362.png" alt="Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars -&gt;  Sensor Calibration - A Necessary Evil"></p>
<p>5、Correct Predicted State。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-41-36-1024x459.png" alt="Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars -&gt;  Sensor Calibration - A Necessary Evil"></p>
<p>6、Compute Corrected Covariance。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-23-44-46-1024x454.png" alt="Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars -&gt;  Sensor Calibration - A Necessary Evil"></p>
<h1 id="状态估计-State-Estimation-的精度需求-Accuracy-Requirements"><a href="#状态估计-State-Estimation-的精度需求-Accuracy-Requirements" class="headerlink" title="状态估计(State Estimation)的精度需求(Accuracy Requirements)"></a>状态估计(State Estimation)的精度需求(Accuracy Requirements)</h1><p>不同的应用场景对State Estimation的精度的要求不同，比如高速场景下的Lane Keeping一般要求亚米级级精度。如下图所示的场景，车辆宽度为1.8m，机动车道宽度为3m，所以车辆两侧有约60cm的冗余空间，在这种场景下，如果要实现Lane Keeping的功能，只要状态估计的精度小于60cm就可以满足实际应用的需求。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-28-12-00-56.png"></p>
<p>但在拥挤的城市道路交通场景下，对State Estimation的精度要求是越高越好，状态估计的精度越高，自动驾驶就越安全。</p>
<h1 id="状态估计-State-Estimation-的更新频率要求"><a href="#状态估计-State-Estimation-的更新频率要求" class="headerlink" title="状态估计(State Estimation)的更新频率要求"></a>状态估计(State Estimation)的更新频率要求</h1><p>以人类驾驶汽车为例，一个人开车过程中闭着眼睛，但为了保证行车安全，她每间隔1s睁开一次眼睛，以确定自己所在的位置。在空旷的道路场景下，1HZ的位置确认频率就可以保证，但是在繁忙的交通的道路上，1s确认一次位置的做法就非常不靠谱了。</p>
<p>但是，越高的定位频率带来的越高的计算资源消耗，而车载计算资源是有限的，并且还是感知、控制、决策、路径规划等所有功能共享的，所以在更新频率和计算资源之间需要有一个trade-off。</p>
<p>根据经验，15HZ-30HZ的状态更新的频率就能够满足自动驾驶的应用需求，当然在计算资源允许的情况下，状态更新(State Estimation)频率越高越好。</p>
<h1 id="Sensor-Failures"><a href="#Sensor-Failures" class="headerlink" title="Sensor Failures"></a>Sensor Failures</h1><p>自动驾驶使用的传感器系统可能由于外部环境因素而失效，比如恶劣天气状况、硬件故障、系统连接线松了等等；也可能由于传感器自身的短板导致，比如GNSS在隧道场景下无法定位、在城市环境下定位的误差达到数十米，IMU容易收到温度变换的影响等。</p>
<p>即使在没有传感器异常的情况下，我们依然能够从多传感器的使用中收益。如下图所示，各个传感器的功能相互补充，构建安全的自动驾驶系统。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-29-09-21-22-1024x427.png"></p>
<p>各个传感器各有所长，比如短距测量传感器可以在停车场景下，检测附近的障碍物，避免发生碰撞；中距测量传感器在车道保持场景下，检测周围的行人、机动/非机动车辆；长距测量传感器帮助我们检测和预测远距离障碍物的运动等等。在实际应用要充分考虑到这些传感器的长处和短板，并增加一定的冗余系统，保证在部分系统无法工作的情况下，仍然可以保证车辆的正常运行。</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2019-12-29-09-42-39-1024x600.png"></p>
<h1 id="多传感器融合的代码实战"><a href="#多传感器融合的代码实战" class="headerlink" title="多传感器融合的代码实战"></a>多传感器融合的代码实战</h1><p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2020-01-05-13-42-59-1024x403.png"></p>
<p>Couresas上的Multi-Sensors Fusion Project效果如下:</p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2020-01-05-13-11-47-1024x489.png" alt="Vehicle Trajectory-Ground Truth and Estimated Trajectory"></p>
<p><img src="/2019/12/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%BA%94%E5%9F%BA%E4%BA%8E%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%9A%84%E7%8A%B6%E6%80%81%E4%BC%B0/Screenshot-from-2020-01-05-13-09-04-1024x644.png" alt="Vehicle Trajectory-Estimation Error and Uncertainty Bounds"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接:"></a>参考链接:</h1><p><a href="https://medium.com/@wilburdes/sensor-fusion-algorithms-for-autonomous-driving-part-1-the-kalman-filter-and-extended-kalman-a4eab8a833dd">https://medium.com/@wilburdes/sensor-fusion-algorithms-for-autonomous-driving-part-1-the-kalman-filter-and-extended-kalman-a4eab8a833dd</a>  </p>
<p>Coursera Lecture -&gt; State Estimation and Localization for Self-Driving Cars</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>Muti Sensors Fusion</tag>
        <tag>State Estimation</tag>
        <tag>多传感器融合</tag>
        <tag>状态估计</tag>
        <tag>自动驾驶定位</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶运动规划-Hybird A*算法</title>
    <url>/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>下面的视频展示了DARPA Urban Challenge(DARPA 2007)中Stanford Racing Team的无人车Junior使用的运动规划(Motion Planning)算法Hybird A*在增量构建的迷宫场景、阻断的道路场景和停车场狭窄停车位场景的实际表现。</p>
<p>在迷宫场景中，可以看到随着车辆的运动，周围在不断的做增量构建,这也就意味着，迷宫中的障碍物是通过车端的传感器实时感知结果得到的。车辆只能看到它周围的环境，随着车辆的持续运动，周围的环境被增量式的构建出来。车辆根据增量构建的场景，实时的调整自身的运动规划策略。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-4b43504df6921dce7e5ff362a1a53c23_b.gif" alt="Hybird A\*算法在迷宫场景的规划效果。图片来源：参考材料2"></p>
<p>视频中黄色的小短线是Hybird A*搜索树，可以看到该算法在不同位置、不同转向角度的情况下都可以实时的为车辆规划出可行的运动路径。</p>
<p>在道路阻断导致车辆无法继续前行的场景下，Hybird A*算法可以规划出掉头曲线，从而避开阻塞的道路，从其它道路继续前进。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-4dfb7dacc8bc2d7da86aa9d5764bf5a1_b.jpg" alt="Hybird A\*算法在道路阻断场景的规划效果。图片来源：参考材料2"></p>
<p>最后是一个在停车场进入狭窄停车位的场景，可以看到Hybird A*算法可以规划出复杂的运动路线，使得车辆先前进，再后退，再一次性的进入到狭窄的空车位中。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-242fb603c6b7478b26476e1d72df7351_b.jpg" alt="Hybird A\*算法在狭窄停车位场景的规划效果。图片来源：参考材料2"></p>
<p>既然是A*算法，Hybird A*算法具有A*算法的基本特征，即通过当前状态到目标状态的代价(Cost)预估，引导车辆更快的收敛到目标状态。</p>
<h1 id="搜索空间离散化"><a href="#搜索空间离散化" class="headerlink" title="搜索空间离散化"></a>搜索空间离散化</h1><p>传统的开放空间(Open Space)中的A*路径搜索的算法，一般将空间划分为小网格，使用网格中心作为A*路径规划的节点，在这些节点中寻求一条规避障碍物的路径。求解的路径只保证连通性，不保证车辆实际可行。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-bea58d1b8d2533fbdf1d62f79fd9f434_1440w.jpg" alt="传统A\*算法 VS Hybird A\*算法。图片来源：参考材料2"></p>
<p>Hybird A*算法同时考虑空间连通性和车辆朝向，将二维平面空间和角度同时进行二维离散化。论文《Practical Search Techniques in Path Planning  for Autonomous Driving》中设置的二维网格大小为1m x 1m，角度分辨率为$5^o$。在(X,Y,$\theta$)三个维度上进行搜索树(Search Tree)扩展时，Hybird A*将车辆的运动学约束引入其中，路径节点可以是二维小网格内的任意一点，保证了搜索出的路径一定是车辆实际可以行驶的。</p>
<h1 id="Hybird-A-搜索树扩展"><a href="#Hybird-A-搜索树扩展" class="headerlink" title="Hybird A*搜索树扩展"></a>Hybird A*搜索树扩展</h1><h2 id="满足车辆运动学约束"><a href="#满足车辆运动学约束" class="headerlink" title="满足车辆运动学约束"></a>满足车辆运动学约束</h2><p>搜索树扩展过程需要基于车辆运动模型，不同类型的车辆运动模型有差异，这里以以前提到的Simple Car Model为例。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-a57bc605a69cd10576c903cc66fa535f_1440w.jpg" alt="Simple Car车辆运动模型。图片来源：Planning Algorithm-http://planning.cs.uiuc.edu/node658.htm"></p>
<p>Simple Car Model的车辆运动学约束的实现如下，其中(x,y, yaw)是车辆的当前姿态；distance是车辆在当前行驶方向上前进的距离；steer是方向盘与车辆行驶方向的夹角；函数返回的是满足车辆运动学约束的下一个姿态点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move</span>(<span class="params">x, y, yaw, distance, steer, L=WB</span>):</span></span><br><span class="line">    x += distance * cos(yaw)</span><br><span class="line">    y += distance * sin(yaw)</span><br><span class="line">    yaw += pi_2_pi(distance * tan(steer) / L)  <span class="comment"># distance/2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y, yaw</span><br></pre></td></tr></table></figure>

<h2 id="车辆控制空间离散化"><a href="#车辆控制空间离散化" class="headerlink" title="车辆控制空间离散化"></a>车辆控制空间离散化</h2><p>车辆的控制输入主要有两个：方向盘转角(Steering Angle)和运动方向(direction)。将方向盘转角从最小转角(Min Steering Angle)到最大转角(Max Steering Angle)按照一定间隔进行采样；车辆的运动方向只有两个：向前运动和向后运动。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> steer <span class="keyword">in</span> np.concatenate((np.linspace(-MAX_STEER, MAX_STEER, N_STEER),[<span class="number">0.0</span>])):</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> [<span class="number">1</span>, -<span class="number">1</span>]:</span><br><span class="line">        <span class="keyword">yield</span> [steer, d]</span><br></pre></td></tr></table></figure>

<h2 id="对运动空间进行扩展探索"><a href="#对运动空间进行扩展探索" class="headerlink" title="对运动空间进行扩展探索"></a>对运动空间进行扩展探索</h2><p>对运动空间进行扩展探索的过程就是以车辆的控制参数(Steering Angle和Direction)为输入，从车辆的当前姿态为输入，不断采样生成增量扩展的搜索树的过程。</p>
<p>在生成搜索树的过程中，有两个细节：</p>
<p>1）对采样扩展的结果进行碰撞检测，并剔除不满足碰撞检测的扩展。碰撞检测的过程不仅考虑障碍物的位置和形状，还需要考虑车辆自身的位置和形状；</p>
<p>2）最大程度的保证采样扩展的起点和终点不在同一个网格中。可以将采样扩展的长度设置为比对角线长度大一点；</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-e8aacf720df0170dc592f10c958d99aa_1440w.jpg" alt="Search Tree With Reed-Shepp Expansion，黄绿色的是增量扩展的搜索树，紫色的是从当前位置到目标位置的Reed-Shepp扩展路径。图片来源：参考材料3"></p>
<p>采样扩展的示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x, y, yaw = current.xlist[-<span class="number">1</span>], current.ylist[-<span class="number">1</span>], current.yawlist[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">arc_l = XY_GRID_RESOLUTION * <span class="number">1.5</span></span><br><span class="line">xlist, ylist, yawlist = [], [], []</span><br><span class="line"><span class="keyword">for</span> dist <span class="keyword">in</span> np.arange(<span class="number">0</span>, arc_l, MOTION_RESOLUTION):</span><br><span class="line">    x, y, yaw = move(x, y, yaw, MOTION_RESOLUTION * direction, steer)</span><br><span class="line">    xlist.append(x)</span><br><span class="line">    ylist.append(y)</span><br><span class="line">    yawlist.append(yaw)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> check_car_collision(xlist, ylist, yawlist, ox, oy, kdtree):</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<h1 id="搜索代价预估-Heuristics"><a href="#搜索代价预估-Heuristics" class="headerlink" title="搜索代价预估(Heuristics)"></a>搜索代价预估(Heuristics)</h1><p>Hybird A*算法依赖如下两种Heuristics：Non Holonomic Without Obstacles和Obstacles Without Holonomic。</p>
<h2 id="Non-Holonomic-Without-Obstacles"><a href="#Non-Holonomic-Without-Obstacles" class="headerlink" title="Non Holonomic Without Obstacles"></a>Non Holonomic Without Obstacles</h2><p>Non Holonomic Without Obstacles只考虑车辆运动的非完整约束特性，而不考虑障碍物对车辆运动的限制，即认为车辆在完全没有障碍物的开放空间上运动。<br>Heuristics Cost = Max(non holonomic without obstacles cost, 2D Euclidean distance)</p>
<p>之所以使用Non Holonomic Without Obstacles Cost和2D Euclidean distance的原因在于，它可以对靠近目标附近的错误Heading搜索进行大量有效的剪枝。<br>Non Holonomic Without Obstacles Cost的计算过程中，对车辆的运动方向变化、车辆转向角度变化、车辆方向盘转角大小等行为施加一定的惩罚，保证车辆按照预期的行为进行运动。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-958aecbb37fe68605a3ef8bdac80c20f_1440w.jpg" alt="Hybird A\*算法在停车场的路径规划效果。图片来源：参考材料3"></p>
<h2 id="Obstacles-Without-Holonomic"><a href="#Obstacles-Without-Holonomic" class="headerlink" title="Obstacles Without Holonomic"></a>Obstacles Without Holonomic</h2><p>Obstacles Without Holonomic只考虑环境中的障碍物，不考虑车辆的运动约束。这种情况的处理就非常常见了，先基于已知环境和已知障碍物构建网格地图，再采用动态规划算法(Dynamic Programming)计算每个网格到达目的地所在网格的Cost(Cost一般使用欧式距离就够了)。</p>
<p>使用该Heuristic的好处是，可以提前发现所有的U型障碍物和Dead Ends，从而引导车辆尽早避开这些区域。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95/v2-c3a5ebd96864055c8428555c729320fd_b.jpg" alt="动态规划算法(Dynamic Programming)"></p>
<h1 id="Analytic-Expansions"><a href="#Analytic-Expansions" class="headerlink" title="Analytic Expansions"></a>Analytic Expansions</h1><p>前面提到的Hybird A*算法中对运动空间(X, Y, $\theta$)和车辆控制参数(Steering Angle)进行了离散化处理，这就决定了它永远不可能精确的到达连续变化的目标姿态。</p>
<p>为了解决这一问题，论文《Practical Search Techniques in Path Planning  for Autonomous Driving》中提出使用基于Reed Shepp模型的Analytic  Expansions，即选出一些节点，使用Reed Shepp曲线计算从该节点到目标姿态的路径，如果该路径在已知的环境中不与任何障碍物发生碰撞，则将其作为可选的行驶路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">paths = rs.calc_paths(sx, sy, syaw, gx, gy, gyaw,</span><br><span class="line">                        max_curvature, step_size=MOTION_RESOLUTION)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> paths:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">best_path, best = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> paths:</span><br><span class="line">    <span class="keyword">if</span> check_car_collision(path.x, path.y, path.yaw, ox, oy, kdtree):</span><br><span class="line">        cost = calc_rs_path_cost(path)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> best <span class="keyword">or</span> best &gt; cost:</span><br><span class="line">            best = cost</span><br><span class="line">            best_path = path</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> best_path</span><br></pre></td></tr></table></figure>

<p>到目前为止，我们通过Hybird A*算法得到了一条实际可行驶的运动路径，但这样的路径往往需要进一步的优化才能得到更好的预期驾驶行为。这种优化分为两个步骤：</p>
<p>1） 应用非线性优化算法(non-linear optimization)对路径的长度(length)和平滑性(smoothness)进行优化；</p>
<p>2） 对优化后的路径进行非参数化的插值(non-parametric interpolation)。</p>
<p>如何对规划出的路径进行继续优化下周继续研究！</p>
<h1 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h1><p>1、Explaining the Hybrid A Star pathfinding algorithm for selfdriving cars.(<a href="https://blog.habrador.com/2015/11/explaining-hybrid-star-pathfinding.html">https://blog.habrador.com/2015/11/explaining-hybrid-star-pathfinding.html</a>)<br>2、Udacity A* in Action-Artificial Intelligence for Robotics(<a href="https://www.youtube.com/watch?v=qXZt-B7iUyw">https://www.youtube.com/watch?v=qXZt-B7iUyw</a>)<br>3、Practical Search Techniques in Path Planning for Autonomous Driving(<a href="https://ai.stanford.edu/~ddolgov/papers/dolgov/_gpp/_stair08.pdf">https://ai.stanford.edu/~ddolgov/papers/dolgov\_gpp\_stair08.pdf</a>)<br>4、文中代码出处(<a href="https://github.com/gyq18/PythonRobotics/blob/9a9ea3b3d7cc2f5e4cb10b384610964044f17583/PathPlanning/HybridAStar/hybrid/_a/_star.py">https://github.com/gyq18/PythonRobotics/blob/9a9ea3b3d7cc2f5e4cb10b384610964044f17583/PathPlanning/HybridAStar/hybrid\_a\_star.py</a>)</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>运动规划</tag>
        <tag>Hybird A Star</tag>
        <tag>Hybird A*</tag>
        <tag>Motion Planning</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶运动规划-Hybird A*算法(续)</title>
    <url>/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95%E7%BB%AD/</url>
    <content><![CDATA[<p>Hybird A*算法保证生成的路径是车辆可实际行驶的，但它仍然包含很多不必要的车辆转向操作，我们可以对其进行进一步的平滑和优化。</p>
<h1 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h1><p>对于Hybird A*生成的车辆轨迹序列：${(x_1, y_1),(x_2, y_2),…,(x_N, y_N)}$,论文【1】中提出如下的目标优化函数(Objective Function)：</p>
<p>$$<br>\begin{array}<br>{l}w_{\rho} \sum_{i=1}^{N} \rho_{V}\left(x_{i}, y_{i}\right)+ w_{o} \sum_{i=1}^{N} \sigma_{o}\left(\left\mathbf{x}<em>{i}-\mathbf{o}</em>{i}\right-d_{\max }\right)+ w_{\kappa} \sum_{i=1}^{N-1} \sigma_{\kappa}\left(\frac{\Delta \phi_{i}}{\left\Delta \mathbf{x}<em>{i}\right}-\kappa</em>{\max }\right)+ w_{s} \sum_{i=1}^{N-1}\left(\Delta \mathbf{x}<em>{i+1}-\Delta \mathbf{x}</em>{i}\right)^{2}<br>\end{array}<br>$$</p>
<p>该优化函数是Voronoi Term、Obstacle Term、Curvature Term和Smoothness Term四个部分的加权平均：第一个部分引导车辆尽可能的避开障碍物区域；第二个部分惩罚车辆与障碍物的碰撞行为；第三部分约束规划的每个点的最大曲率，并提供车辆非完整约束的保证；第四个部分是轨迹的平滑性约束。$w_{\rho}$、$w_{o}$、$w_{\kappa}$、$w_{s}$分别是这四个部分的权重因子。</p>
<h2 id="oronoi-Term"><a href="#oronoi-Term" class="headerlink" title="oronoi Term"></a>oronoi Term</h2><p>Voronoi Term中引入了Voronoi Field的概念，Voronoi Field是机器人Motion Planning领域两种经典算法Voronoi Diagram和Potential Field的结合。</p>
<p>此处采用Voronoi Field的定义如下：</p>
<p>$$\begin{aligned}\rho_{V}(x, y)=&amp;\left(\frac{\alpha}{\alpha+d_{\mathcal{O}}(x, y)}\right)\left(\frac{d_{\mathcal{V}}(x, y)}{d_{\mathcal{O}}(x, y)+d_{\mathcal{V}}(x, y)}\right) \\&amp; \frac{\left(d_{\mathcal{O}}-d_{\mathcal{O}}^{\max }\right)^{2}}{\left(d_{\mathcal{O}}^{\max }\right)^{2}}\end{aligned}$$</p>
<p>其中$d_{\mathcal{O}}$和$d_{\mathcal{V}}$分别是路径点(x,y)到最近障碍物的距离和到最近Voronoi Diagram的边的距离。越靠近障碍物，$\rho_{V}(x, y)$的值越大，越接近1；越靠近Voronoi Edge，$\rho_{V}(x, y)$的值越接近0。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95%E7%BB%AD/v2-8f93cbb765626ef162a1d0ae24a1ca84_1440w.jpg"></p>
<p>上图图左一为Voronoi Field的实际效果，上图右一是标准Potential Field的实际效果。可以看到，Voronoi Field对狭窄通道的效果要明显优于Potential Field。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95%E7%BB%AD/v2-b2b52c6d9e0c597c882a9016b574c22c_1440w.jpg" alt="实际停车场的Voronoi Field和Junior的规划路径"></p>
<h2 id="Obstacle-Term"><a href="#Obstacle-Term" class="headerlink" title="Obstacle Term"></a>Obstacle Term</h2><p>Obstacle Term中$x_i$是路径点坐标位置，$o_i$是附近障碍物的位置，$d_{max}$是决定Obstacle Term是否影响路径Cost的阈值。当路径点距离障碍物的距离小于$d_{max}$时，Obstacle Term才会对轨迹的Cost进行惩罚。距离障碍物越近，$x_i - o_i$的值越小，Obstacle Term的值就越大，整个轨迹的Cost也就越大。这样就达到了使得平滑后的路径远离障碍物的效果。</p>
<p>这里$\sigma_{o}$一般使用二次函数。即:</p>
<p>$$\sigma_{o} \left(\left {\mathbf{x}}<em>{i}-{\mathbf{o}}</em>{i}\right-d_{\max }\right) = \left(\left {\mathbf{x}}<em>{i}-{\mathbf{o}}</em>{i}\right-d_{\max }\right)^2$$</p>
<h2 id="Curvature-Term"><a href="#Curvature-Term" class="headerlink" title="Curvature Term"></a>Curvature Term</h2><p>对于一系列的点$X_i={x_i, y_i}, i \in [1,N]$，$\Delta X_i = X_i - X_{i-1}$，即为规划路径的方向向量；$\Delta \phi_i$为路径点的方向角变化。</p>
<p>$\kappa = \Delta \phi / \Delta X_i$为$X_i$处的曲率。与Obstacle Term类似，Curvature Term也设置了一个最大允许的路径曲率$\kappa_{max}$，当曲率大于$\kappa_{max}$时，Curvature Term才会对路径的Cost施加惩罚。</p>
<h2 id="Smoothness-Term"><a href="#Smoothness-Term" class="headerlink" title="Smoothness Term"></a>Smoothness Term</h2><p>平滑项利用当前点前后两个方向向量的差值来衡量，方向向量既可以衡量方向的改变，也可以体现轨迹点的分布变换。</p>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>确定Objective Function函数之后，就可以利用Conjugate Gradient(CG，共轭梯度法)或者Gradient Descent求解最优路径。</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95%E7%BB%AD/v2-58534926e1b30883118f3532096bb90b_1440w.jpg"></p>
<p>代码参见:<br><a href="https://github.com/teddyluo/hybrid-a-star-annotation/blob/master/src/smoother.cpp">https://github.com/teddyluo/hybrid-a-star-annotation/blob/master/src/smoother.cpp</a></p>
<p>平滑后的路径如下：</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95%E7%BB%AD/v2-e42b1b6553416fa555ceaa4426b26f34_1440w.jpg" alt="Hybrid-A\*(红色路径) VS CG path(蓝色路径)"></p>
<h1 id="Non-Parametric-Interpolation"><a href="#Non-Parametric-Interpolation" class="headerlink" title="Non-Parametric Interpolation"></a>Non-Parametric Interpolation</h1><p>对路径进行非线性优化后，我们得到一条比Hybird A*算法路线更加平滑的路径，但是这条路径仍然由一段段的折线组成。在论文【1】中提到在它们的实现中组成路径的折线大约在0.5m-1m，这些折线仍然会导致车辆会出现非常生硬的转向，所以需要使用插值算法进一步平滑路径。</p>
<p>参数化的插值算法对噪声非常敏感，比如当路径中两个顶点非常接近时，三次样条曲线(Cubic Spline)算法的输出就会产生非常大的震荡。</p>
<p>【1】中提出通过固定原始路径顶点，然后在固定顶点之间插入新的顶点，最后使用Conjugate Gradient(CG，共轭梯度法)最小化曲率的非参数插值(Non-Parametric Interpolation)方法对曲线进一步平滑，平滑效果如下：</p>
<p><img src="/2020/10/30/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92-hybird-a%E7%AE%97%E6%B3%95%E7%BB%AD/v2-d264c9cf18e4de3687aab429920e9253_1440w.jpg"></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>1、Practical Search Techniques in Path Planning for Autonomous Driving。Dmitri Dolgov，Sebastian Thrun，Michael Montemerlo，James Diebel.<br>2、Path Planning in Unstructured Environments A Real-time Hybrid A* Implementation for Fast and Deterministic Path Generation for the KTH Research Concept Vehicle.</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>运动规划</tag>
        <tag>Hybird A Star</tag>
        <tag>Hybird A*</tag>
        <tag>Motion Planning</tag>
        <tag>Objective Function</tag>
        <tag>运动规划目标函数</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶运动规划(Motion Planning)</title>
    <url>/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/</url>
    <content><![CDATA[<h1 id="什么是Motion-Planning"><a href="#什么是Motion-Planning" class="headerlink" title="什么是Motion Planning"></a>什么是Motion Planning</h1><p>Motion Planning是在遵循道路交通规则的前提下，将自动驾驶车辆从当前位置导航到目的地的一种方法。</p>
<p>在实际开放道理场景下，自动驾驶要处理的场景非常繁杂：空旷的道路场景、与行人、障碍物共用道理的场景、空旷的十字路口、繁忙的十字路口、违反交通规则的行人/车辆、正常行驶的车辆/行人等等。场景虽然复杂，但都可以拆解为一系列简单行为(behavior)的组合:</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-18-57-56-1024x468.png"></p>
<span id="more"></span>

<p>将这些简单的行为(behavior)组合起来，就可以完成复杂的驾驶行为。</p>
<h1 id="Motion-Planning的约束条件-constraints"><a href="#Motion-Planning的约束条件-constraints" class="headerlink" title="Motion Planning的约束条件(constraints)"></a>Motion Planning的约束条件(constraints)</h1><p>Motion Planning是一个复杂的问题，它的执行过程需要满足很多约束条件：</p>
<h2 id="车辆运动学约束"><a href="#车辆运动学约束" class="headerlink" title="车辆运动学约束"></a>车辆运动学约束</h2><p>车辆运动受到运动学约束，比如它不能实现瞬时侧向移动，前驱的车辆必须依赖前轮的转向才能实现变道、转向等操作，在弯道上不能速度过快等等。通常我们采用单车模型(Bicycle Model)对车辆运动进行建模。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-17-52.png"></p>
<h2 id="静态障碍物-Static-Obstacle-约束"><a href="#静态障碍物-Static-Obstacle-约束" class="headerlink" title="静态障碍物(Static Obstacle)约束"></a>静态障碍物(Static Obstacle)约束</h2><p>静态障碍物(Static Obstacle)是道路上静止的车辆、路面中间的石墩子等车辆不可行驶的区域。Motion Planning需要避开这些静态障碍物，避免与它们发生碰撞。解决碰撞的思路大概有两种：</p>
<p>1）将静态障碍物(Static Obstacle)在网格占位图中表示出来，然后检测规划路线是否与静态障碍物区域相交。</p>
<p>2）将车辆的轮廓扩大，比如扩展成一个圆形，然后检测障碍物是否与Circle发生碰撞。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-26-13.png"></p>
<h2 id="动态障碍物约束"><a href="#动态障碍物约束" class="headerlink" title="动态障碍物约束"></a>动态障碍物约束</h2><p>Motion Planning要实时处理行人、车辆等各种运动的障碍物，避免与障碍物发生碰撞事故。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-37-33-1024x584.png"></p>
<h2 id="道路交通规则约束"><a href="#道路交通规则约束" class="headerlink" title="道路交通规则约束"></a>道路交通规则约束</h2><p>车辆在道路上行驶必须要遵守车道线约束规则(比如左转专用道只能左转、实线不能变道、路口必须遵守红绿灯的指示)和各种标志标牌的指示。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-19-55-29-1024x281.png"></p>
<h1 id="Motion-Planning的优化目标"><a href="#Motion-Planning的优化目标" class="headerlink" title="Motion Planning的优化目标"></a>Motion Planning的优化目标</h1><p>了解Motion Planning的约束条件之后，需要构造目标优化函数，然后最小化目标函数，从而获得在当前环境下的最优运动轨迹。目标函数的种类有很多，下面枚举一些常用的目标函数。</p>
<p>1）关注路径长度(Path Length)，寻求到达目的地的最短路径。</p>
<p>$s_f = \int^{s_f}_{s_i}{\sqrt{1+ (\frac{dy}{dx})^2}dx}$</p>
<p>2）关注通行时间(Travel Time)，寻求到达目的地的最短时间。</p>
<p>$T_f = \int^{s_f}_{0} {\frac{1}{v(s)}ds}$</p>
<p>3）惩罚偏离参考轨迹和参考速度的行为。</p>
<p>$\int^{s_f}_{0} {x(s) - x_{ref}(s)ds}$</p>
<p>$\int^{s_f}_{0} {v(s) - v_{ref}(s)ds}$</p>
<p>4）考虑轨迹平滑性(Smoothness)</p>
<p>$\int^{s_f}_{0} {\dddot{x}(s)^2ds}$</p>
<p>5）考虑曲率约束(Curvature)</p>
<p>$\int^{s_f}_{0} {k(s)^2ds}$</p>
<p>通过组合设计自己的目标优化函数，从而获得较好的Planning效果。</p>
<h1 id="分级运动规划器-Hierarchical-Motion-Planning"><a href="#分级运动规划器-Hierarchical-Motion-Planning" class="headerlink" title="分级运动规划器(Hierarchical Motion Planning)"></a>分级运动规划器(Hierarchical Motion Planning)</h1><p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-37-52-1024x324.png"></p>
<p>Motion Planning是一个异常复杂的问题，所以通常我们把它切分为一系列的子问题(Sub Problem)。比如Mission Planner、Behavior Planner、Local Planner、Vehicle Control等。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-40-07.png"></p>
<h2 id="Mission-Planner"><a href="#Mission-Planner" class="headerlink" title="Mission Planner"></a>Mission Planner</h2><p>Mission Planner关注High-Level的地图级别的规划；通过Graph Based的图搜索算法实现自动驾驶路径的规划。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-20-51-30.png"></p>
<h2 id="Behavior-Planner"><a href="#Behavior-Planner" class="headerlink" title="Behavior Planner"></a>Behavior Planner</h2><p>Behavior Planner主要关注交通规则、其它道路交通参与者(自行车、行人、社会车辆)等等，决定在在当前场景下应该采取何种操作(如停车让行、加速通过、避让行人等等)。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-01-29.png"></p>
<p>Behavior Planner的实现方式比较常见的有几种：<strong>有限状态机(Finite State Machines)、规则匹配系统(Rule Based System)、强化学习系统(Reinforcement Learning)。</strong></p>
<p>有限状态机中的State是各个行为决策，根据对外界环境的感知和交通规则的约束在各个状态之间转换。比如在路口红绿灯的场景，当路口交通灯为红色不可通行时，车辆会首先切换到Decelerate to Stop状态，然后在路口停止线完全停下来，进入Stop状态，并持续在Stop状态等待，直至交通灯变为绿色允许车辆通行，车辆进入Track Speed状态，继续前行。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-15-18.png"></p>
<p>Rule-Based System是通过一系列的分级的规则匹配来决定下一步的决策行为。比如交通灯绿色-&gt;通行；交通灯红色-&gt;停车等待。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-25-58-1024x174.png"></p>
<p>基于强化学习的Behavior Planner系统如下：</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-31-32-1024x498.png"></p>
<h2 id="Local-Planner"><a href="#Local-Planner" class="headerlink" title="Local Planner"></a>Local Planner</h2><p>Local Planner关注如何生成舒适的、碰撞避免的行驶路径和舒适的运动速度，所以Local Planner又可以拆分为两个子问题：<strong>Path Planner和Velocity Profile Generation</strong>。Path Planner又分为Sampling-Based Planner、Variational Planner和Lattice Planner。</p>
<p>最经典的Sampling-Based Planner算法是Rapidly Exploring Random Tree，RRT算法。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-36-30.png"></p>
<p>Variational Planner根据Cost Function进行优化调整，从而避开障碍物，生成安全的轨迹。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-45-12-1024x464.png"></p>
<p>Lattice Planner将空间搜索限制在对车辆可行的Action Space。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-48-25.png"></p>
<p><strong>Velocity Profile Generation</strong>要考虑到限速、速度的平滑性等。</p>
<p><img src="/2020/01/18/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92motion-planning/Screenshot-from-2020-01-18-21-52-05-1024x529.png"></p>
<p>Vehicle Control将Planner的规划结果转化为车辆的运动行为。</p>
<h1 id="待阅读材料"><a href="#待阅读材料" class="headerlink" title="待阅读材料"></a>待阅读材料</h1><ul>
<li><p>  P. Polack, F. Altche, B. Dandrea-Novel, and A. D. L. Fortelle, “<a href="https://ieeexplore.ieee.org/abstract/document/7995816">The kinematic bicycle model: A consistent model for planning feasible trajectories for autonomous vehicles</a>” 2017 IEEE Intelligent Vehicles Symposium (IV), 2017. Gives an overview of the kinematic bicycle model.</p>
</li>
<li><p>  S. Karaman and E. Frazzoli, “<a href="http://amav.gatech.edu/sites/default/files/papers/icra2013.Karaman.Frazzoli.submitted.pdf">Sampling-based optimal motion planning for non-holonomic dynamical systems</a>,” 2013 IEEE International Conference on Robotics and Automation, 2013. Introduces the RRT* algorithm as an example of sampling-based planning.</p>
</li>
<li><p>  N. Ratliff, M. Zucker, J. A. Bagnell, and S. Srinivasa, “<a href="https://kilthub.cmu.edu/articles/CHOMP_Gradient_Optimization_Techniques_for_Efficient_Motion_Planning/6552254/1">CHOMP: Gradient optimization techniques for efficient motion planning</a>,” 2009 IEEE International Conference on Robotics and Automation, 2009. Introduces the CHOMP algorithm as an example of applying calculus of variations to planning.</p>
</li>
<li><p>  M. Pivtoraiko, R. A. Knepper, and A. Kelly, “<a href="https://ri.cmu.edu/pub_files/2009/3/ross.pdf">Differentially constrained mobile robot motion planning in state lattices</a>,” Journal of Field Robotics, vol. 26, no. 3, pp. 308-333, 2009. Introduces the state lattice planning method.</p>
</li>
</ul>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1、Course自动驾驶课程： <a href="https://www.coursera.org/learn/motion-planning-self-driving-cars/home/welcome">Motion Planning for Self-Driving Cars</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶运动规划</tag>
        <tag>Motion Planning</tag>
        <tag>运动规划目标函数</tag>
        <tag>Hierarchical Motion Planning</tag>
        <tag>Motion Planner</tag>
        <tag>Motion Planning Objective Function</tag>
        <tag>分级运动规划器</tag>
        <tag>车辆动力学</tag>
        <tag>车辆运动学</tag>
        <tag>运动规划约束</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉-Camera标定</title>
    <url>/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/</url>
    <content><![CDATA[<h4 id="1、像素坐标系与图像坐标系之间的关系"><a href="#1、像素坐标系与图像坐标系之间的关系" class="headerlink" title="1、像素坐标系与图像坐标系之间的关系"></a>1、像素坐标系与图像坐标系之间的关系</h4><p>假设每一个像素在u轴和v轴方向上的物理尺寸为dx和dy</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad600029917f48fb5be.jpg" alt="计算机视觉-Camera标定(1)"></p>
<span id="more"></span>

<h4 id="2、图像坐标系到相机坐标系"><a href="#2、图像坐标系到相机坐标系" class="headerlink" title="2、图像坐标系到相机坐标系"></a>2、图像坐标系到相机坐标系</h4><p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad800028bebb494b0ce.jpg" alt="计算机视觉-Camera标定(1)"></p>
<h4 id="3、世界坐标系到相机坐标系"><a href="#3、世界坐标系到相机坐标系" class="headerlink" title="3、世界坐标系到相机坐标系"></a>3、世界坐标系到相机坐标系</h4><p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad70000db53d3553707.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad900013a3bb610ed89.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p>于是，从世界坐标系到像素坐标系的转换关系：</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad900013b9553cafc2f.jpg" alt="计算机视觉-Camera标定(1)"></p>
<h4 id="4、其他情况"><a href="#4、其他情况" class="headerlink" title="4、其他情况"></a>4、其他情况</h4><p>考虑像素坐标系坐标轴不垂直的情况(实际相机由于制造工艺上的问题，导致物理成像坐标轴不是绝对垂直)，如下图所示，假设O1在UV坐标系下的坐标为(u0, v0),像素的物理尺寸仍然为dx，dy，则有</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/472b0002dfc7ab409f46.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/47290002d8c8c581f478.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p>矩阵形式如下：</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/47290002d94419e50800.jpg" alt="计算机视觉-Camera标定(1)"></p>
<p>世界坐标系与像素坐标系转换关系：</p>
<p><img src="/2019/12/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-camera%E6%A0%87%E5%AE%9A/4ad600029ca1c9969969.jpg" alt="计算机视觉-Camera标定(1)"></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>computer vision</tag>
        <tag>坐标系变换</tag>
        <tag>相机标定</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>一位财务自由人士的投资修行(转帖)</title>
    <url>/2021/04/04/%E4%B8%80%E4%BD%8D%E8%B4%A2%E5%8A%A1%E8%87%AA%E7%94%B1%E4%BA%BA%E5%A3%AB%E7%9A%84%E6%8A%95%E8%B5%84%E4%BF%AE%E8%A1%8C(%E8%BD%AC%E5%B8%96)/</url>
    <content><![CDATA[<blockquote>
<p>文章来源：<a href="https://xueqiu.com/6465851184/176004126%E3%80%82">https://xueqiu.com/6465851184/176004126。</a> 暂不知道文章的作者是谁，却写出了我这大半年投资的所想所悟，大道至简，尊重常识，才是投资的真谛。</p>
</blockquote>
<p>投资，首先它是一种收入方式的转变，从自己挑水吃，到打造自己的自来水管，从主动收入到被动收入的过渡。当然，这条自来水管刚刚开始的时候很弱小，要它足够大供应足够多的水，那么是必须花费你很多的时间精力去打造。但不管怎么样，只要大方向正确，再掌握一点基本简单的技巧，日久天长，每天挖一点，每天加深一点，每天进步一步，总有一天会建成自己的大运河。</p>
<p>收入方式的改变，随之而来是带来了生活方式的改变。大家为了养家糊口，很多不愿意做的事也必须要做，哪怕这些事情可能损害自己的健康，甚至有一些违反法律，不就是为了家人过上好的生活，为了多赚一些钱。这些人有几重身份，一边是别人的孩子要照顾老人，另一边是又是孩子的父母。一代人要照顾两三代人，这种种重担压得人喘不过气来。这些人从来都是牺牲自我的消费享受，去支付老人的药费，小孩的学费，这些人是平凡的，却也是伟大的，是一种伟大的自我牺牲精神，照顾好家庭，就已经是最大的慈善，无上的功德，我对此非常的敬佩，这也是我一直追求的价值观。</p>
<span id="more"></span>

<p>我非常心疼这些人，我也曾经是这里的一份子，为这种牺牲自我成就他人的精神而感动。所以我真心希望有缘之人，看到我的文章，能够早点开悟，以正确的方式，开始投资，越早越好，很可能我的文章，会不知不觉间改变很多家庭的命运，这可能是我的使命，也是我本人的无上功德。</p>
<p>本人便是一个极端的例子，从早年为了赚钱，忙得要死。到现在投资的自来水管打造成功了，被动收入足够大，不需要再付出时间精力，所以我现在的状态是闲得要死。可以有大量的时间做自己想要做的事情，每天最烦恼的就是时间要如何打发。因为时间太多了，所以生活上也越来越追求精致。当然我说的不是奢侈生活，奢侈生活需要钱，而精致的生活需要的是时间精神。</p>
<p>比如我喝水，一定只喝温水。比如我吃早餐，每天到市场买新鲜的番薯玉米，回家自己蒸熟，再喝上一杯自制的新鲜无添加豆浆，一杯豆浆融合了十多种杂粮。在我心中，这就是我每天生活一个完美的开启。</p>
<p>其实这些精致的生活，并不需要花费钱，我的生活消费可能比很多上班族还低。精致在于精神境界的追求上，而非物质金钱上。一个早餐下来，又新鲜又健康，花费不过两元。而大家为了赶时间上班，随便在街头上吃的，又不健康又贵。</p>
<p>诚然，这种精致的生活追求，是需要花费大量的时间与精力的。一个上班族赶时间哪有精力顾上这些呢？这是一个现实的问题。但投资让收入方式从主动到被动的改变，生活方式也随之改变。投资会让你慢慢释放出时间，依靠出卖自己的时间获取收入这部分的比例渐渐在降低。我有一个习惯保持了很多年了，就是每天都在买入。不管钱多钱少，我每天都在买入。没钱的时候，10元100元也要买。这与钱多钱少没有关系，这是一种誓要用投资改变命运的信仰。也许这是我投资的强迫症吧。没有办法，这是以前养成的习惯，以前总是想快点财务自由，用尽每一分钱（闲钱）来投资。每过一天，我都感到我的河道又挖深了一点点，水流又多了一点点。不以善小而不为，10元也是一个种子基金，与其将它花费在我不需要的地方，不如此时此刻将它播种下去，久以时日，必建奇功。</p>
<p>所以虽然现在我已经财务自由，但这种良好的习惯，将会伴随我一辈子。当你也养成这样的习惯，珍视每一元，每一元的投资都能够让你离财务自由更进一小步，每天进步一点点，量变到质的飞跃，怎么可能会不自由！财务自由只是迟早的事，关键是你有没有改变命运的勇气，与持之以恒的耐心。投资永远不可能一夜暴富，它是以年为单位的人生战略规划，每十年财富上一个台阶。</p>
<p>当有一天，你被动收入超过了主动收入，你就会深深体会到某一种的自由感，某一种长久的快乐。这种快乐不是你买了车，不是你买了什么名牌包包，不是这类消费型的快乐，而是一种持久的成就感。多年的付出，终于从量变积累到质变的一天，被动收入终于超越主动收入。这时候我们便有时间精力慢下来，重新审视一下我们剩余的人生，可以开始规划选择一条自己更愿意走的路。所以这种快乐幸福感，实质是对自我人生的一种掌控感，我的命运越来越被我掌控，照着我规划的路线走下去，只要时间足够长，最终你会得到绝大部份你想要的生活方式。这不仅仅是一种财务上的自由，更是一种精神上的自由。你开始学会掌握自己的欲望，你会发现减少不必要的消费，其实带来更大的快乐，是一种自律的快乐。真正的快乐，从来不需要依靠消费来产生，这是一种精神上的自给自足。从掌控欲望，到改写命运。</p>
<p>从此不再需要为钱而工作，而是为兴趣而工作。如同我现在卖书，讲学，有人说，如果你这么富有，为何还要卖书，这不是骗子？我不生气，只是为这些人感到可怜，从来没有尝试过为兴趣而工作，或者说现实生活的压力不允许他们为兴趣而工作。所以我对他们不感到生气，反而有种慈悲可怜之感，希望这些人也早日过上为兴趣而工作的生活吧。有钱不是不需要工作，而是不需要为钱而工作。</p>
<p>投资带来的幸福的烦恼就是——太闲了！所以我现在形成了一种人生最舒服的状态。在主动收入方面，我放弃了为钱而工作，我是依靠兴趣而工作，这个工作同样会产生收入，但这是我快乐的收入，为之努力而不觉得累，反正越干越精神，越干越享受。因为享受，也让我不是为了钱而工作，而是为了提升我的技术，我在这个主动收入上的技能，不断地精进。工作上不断地精进，也在不断提升我的主动收入。所以虽然我不为钱而工作，但我的主动收入却也越来越高。另外，主动收入可以足够应付我平常的生活消费，我完全不需要依靠卖股票来维生，甚至还有结余，还能支持不断地买入股票——手有余粮心自然不慌。所以我在主动收入上，已经形成了良性循环——为兴趣而赚钱，再在此基础上，主动收入又与投资形成互补促进，这种状态实在是太舒服了。</p>
<p>事业（建立在兴趣基础上的主动收入）投资（不断自动增长的被动收入）两不误，我认为这是人生最完美的收入方式。</p>
<p>从一开始选择投资，便已经意味着你在人生战略上领先了一步。</p>
<p>如同很多人一辈子练习画画，一辈子练习书法，而我只不过是一辈子在股票投资领域上不断练习，没有什么本质的不同，功多自然艺熟，没有什么特别，没有什么秘诀，就是专注一个领域时间足够久而已。成为投资专家，并不比其他任何领域的专家更高贵，我认为本质是一样的，分工不同而已。</p>
<p>但有一点我是很幸运，如果一个人练习了十年的书法成为书法专家，未必一定能够过上财富生活，写好书法到转换成财富变现中间还隔着很多的道道，可能需要包装，需要宣传，需要名气。而我练习了十年的投资成为投资专家，投资到变现，中间没有什么距离，就直接财富变现了。投资嘛，投资做得好，自然是要赚钱的，不管我有没有名气，不管别人怎么骂我讨厌我，我依然不会受任何影响，赚着属于我的利润。没有好的收益，没有财富，算哪门子的投资专家？钱，虽然很粗俗，但它就是衡量投资是否成功的一个重要因素，就是这么直白。所以这是我幸运的地方，我与一位书法家同样付出一样多的努力最终成为各自己领域的专家，但由于我的领域是投资，可以直接变现，而他可能比我曲折很多倍。并不是我比他聪明或更努力，而是不同领域不同特性造就的结果。选择大于努力。</p>
<p>一般来说，想要生活得成功，需要两种能力，一种是具体的技能，另外一种是在现今商业社会将这种技能变现成财富的能力。而投资神奇之处，是技能与变现是重叠的，所以投资家比其它领域的人，事半功倍。一开始就选择投资作为一个终身职业，意味着比其他人少付出了一半的努力。</p>
<p>我这辈子最幸运的是，在二十岁就想明白了很多事情，知道自己想得到什么，也知道自己应该放弃什么，将我有限的精力极致专注在某一个很小很小的点上。然后余下八十年专心执行二十岁所制定的路线，每天进步一点点，等待时间足够长，我便得到我所有想要的事，包括家庭幸福，投资成就，身体健康。我获得让绝大多数人羡慕甚至是嫉妒的成绩——不仅仅是金钱。并不是我多聪明，相反我的绝对智商只能处于中下水平，但我真正聪明甚至是称得上人生智慧的做法，将我有限的智商极致专注在一个极小的领域——世界有很多行业，我只专注于投资。投资有很多品种，我只专注于股票。股票有上万个，我只专注于代表中国经济发展方向的十来个。投资方法千万种，我只永远低估的时候买入，高估卖出，只用一招，别无二式。就是这样，不断地做减法，不断地专注。</p>
<p>一般人是每天都有新鲜事，我是一辈子重复一件事，怎么可能不成功？怎么可能不成为投资家？在这个小领域中，我成为了这个领域的神，这是简单的物理法则，压力作用在足够小的面积上，便能切金断玉。而我在其他领域，只是一个白痴。但我不介意，人生需要有取舍，在一个小小的领域做到极致，已经足够成就一生传奇。而非常幸运的是，投资做的好，就意味着金钱，在现今这个商业化社会，拥有金钱，在其他领域虽然我是白痴，但也不再需要我亲力亲为，我完全可以假手于人，借助其他领域的专家为我效力，而我更专注于投资一域，享受并赚取更多的财富，形成一种非常高效的良性循环。</p>
<p>当然很多人也知道投资是个好东东，也同样在投资上付出巨大的努力，而不见得有好的收获。一方面可能是这些努力用错了方向，只是看上去很努力而已，没有用在关键的地方，自然是白费气力。很多人将心神放在了消息和股价不可预测的涨跌上，而且还常常被这些不可预测的因素，弄得自己头晕眼花，失去了方向。另一方面，我觉得更多的原因，在于无法克服自身人性的弱点。很多人在股票投资上付出巨大的努力而没有回报，而我却一直认为投资是一件很简单的事，我只付出了微不足道的努力，便在投资上取得巨大的收获。我将这些归功于我的性格，我的确是天生适合做投资的料。</p>
<p>投资对于我来说，已经是呼吸一般自然的事情，没有什么困难。事实上投资的方法一点也不难，投资很简单，只是人性很复杂。是人性的复杂，将原本很简单的投资，污染成复杂的事。一切皆是庸人自扰。</p>
<p>投资要做得好，必须要回归到简单的本源。所以这就是我的天生优势——我的智商实在是非常的普通平庸，无法理解复杂的事物，所以我天生凡事尽量往简单处想。在现实生活中做生意，我是完败的等待着被淘汰的那一批人。做生意对我来说太复杂的，每天都有无穷无尽的事情要处理，各种关系要处理。经营上还时刻要创新，产品要创新，服务要创新，变着花样讨好消费者，而且还有激烈的价格竞争，这一切对于我来说，都太复杂了，以我低下的智商，实在是无法掌控。</p>
<p>所以我的想法很简单，与其自己创造一个好生意，不如发现并买入一个好生意。反正本质是一样的：拥有一个好生意为我赚取钞票。就是这么简单的事。自己创造一个好生意，成功率只有万分之一。但发现一个好生意，成功率却是99.99%——直接挑选那些行业龙头不就行了吗！好生意，就是印钞机。我没有能力创造，但我有能力发现。这是出生于这个时代的幸运。假如我早出生三十年，我将一无事处，我的人生将暗淡无光。</p>
<p>做生意，时刻要变，不变就等待着被时代所淘汰。但即使你变，最终还是会被时代所淘汰。有时候你什么都没有做错，只是错在太老而已。没有新时代的基因。</p>
<p>但做投资，万世不变。无论现在时兴苹果还是华为，无论全社会兴起吃老干妈还是鱼子酱，总是有那么一批赚大钱的公司，总是有行业的领导者，总是有那1%的优等生。所以无论时代如何变革，我的投资永恒不变——永恒投资于那1%的优等生。人类精英中的精英，暴利中的暴利，时代的印钞机。</p>
<p>买入并持有顶层1%的精英企业，就是投资中最简单而最有效的事。</p>
<p>本质上，投资应该是永远持有这些精英企业，不做任何波段，指数化投资将是一个非常好的选择。长期的指数化投资，收益将达到10%左右。在此基础上，我进化了一点点。因为指数，其实也是由一个个具体股票组成。所以指数投资的一个缺点就是，你将被动投资一堆比较平庸的公司。</p>
<p>所以我的进化是，在指数化投资的基础上，去除了指数中平庸的公司，只投资于最优秀的公司，指数其实已经足够优秀，但我要优中选优。比如指数中的保险公司有好几个，那么我就只投资最优秀中国平安。指数中银行股有一大堆，我只投资最优秀的招商银行。指数中地产股也有好几个，我只投资最好的万科。指数中消费股也很多，我只投资茅台、伊利。指数中还有一堆不好的行业，重资产的企业，我也一一去除了。最终就剩下几家最优秀最赚钱的企业，这就是我进化版的指数化投资。</p>
<p>这样做减法下来，我的“超级指数”，大概不会超过20个股票。我将永恒持有这些股票。在这里要特别的强调，我所谓的永恒持有不是永远持有某一个具体的股票，我永恒持有的是始终代表经济最高层次的1%。这是一个动态的变化过程。以前我的名单中有苏宁，有国美，但随着他们不适应新时代的发展，他们已经从我的名单中去除。</p>
<p>又随着新时代的到来，腾讯、阿里、百度这些高科技公司也进入到我的候选观察名单当中。所以这是一种动态的静态投资哲学。动态的是，我的名单永远跟随着时代变化而进化。静态的是，我永远只投资代表人类未来符合时代的精英企业，这是我万世不变的法则。不变的是投资的原则，变的是与时俱进的赚钱生意。</p>
<p>通过这种简单的进化，长期投资收益将从原有10%提升到15%。这是了不起的进化，却又是那么的简单。10%年收益复利70年是789倍，而15%复利是17735倍。也就是说，现在我30岁，假如我能够努力活到一百岁，届时我的财富很大概率将比现在增长一万倍。这足够改写我整个家族的发展史。而且我相信，绝大部分人，也可以这样分享中国经济的成果，很多家庭可以依靠这个简单的方法，最终改写命运。</p>
<p>而这种进化是那么的简单，用一些简单的原则，过滤一下平庸的公司就可以了，只投资那么一眼就看到的优秀公司，只投资好行业中的优秀公司。所以，投资实在是很简单的事，付出一点点努力，却能够有巨大的回报。</p>
<p>原则上，我的投资战略，就是永恒持有这些优秀公司组合，不做任何波段。但很快，我发现了我的天赋——懒。可以说，我是懒出了境界，懒到与世无争，懒到淡泊一切。我只是持有这些优秀公司，获得15%的年收益就已经很满足了。即使最终无法得到15%，得到10%也满意。我就是这么懒，就是这么不想努力，就是这么没有进取之心。但这却演变成为我投资上一个极大的天赋：拥有极端的耐心，等待复利的增长。</p>
<p>而市场上普通的交易者，是心浮气躁，希望一夜暴富。而我却是极端的耐心，计划用一百年的时间，来进行复利的增长。最后导致一个结果是，像我这类极端耐心的人，常常不动如山，就是埋头持有，不闻不问，目标很简单很明确，心如止水面对市场上各种波动。这种天赋导致我可以赚取市场上超额的收益，长期大幅跑赢指数。</p>
<p>因为我不动，所以我不会犯错，至少致命的错误不会犯，就是运气好的时候，可能可以以10PE以下买到优秀公司。运气差一点，PE高一点点买优秀公司，收益低一点点，其实我也完全不在意。买贵一点，买便宜一点，其实都是在一个合理区间，这算不上是错误，应该是投资上的必然结果，肯定会遇到的，所以我很坦然。但是同样市场对面的千千万万的投资者，却没有我这种坦然，他们很急，很焦虑，很多动作。动作越大，错误越大。他们常常想跑赢市场，想赚取超越市场的收益，最后的结果是自己消灭了自己，自己吓死了自己。而我这种耐心到极致的人，常常等到各种买入的良机。金融危机买阿里，三聚氰氨买蒙牛，地产调控买保利，银行坏帐买民生，利率下降买平安。然后等待危机过去，股价收复上涨，我便功成身退，再次进入耐心的潜伏期。</p>
<p>这种结果是很让我哭笑不得的，也算是意外的收获。我的出发点，只是永恒持有优秀公司，获取优秀公司的平均成长收益就满足了。而市场对面千万的交易者，他们想超越市场，在他们瞎折腾的过程中，引起市场大幅的波动，要么恐惧的导致股价低的不可思异，要么贪婪的导致股价高的不可思异。对于我这种在旁边冷眼旁观的观察者而言，实质就是低估的时候轻轻的买入，高估的时候安静的离开，轻轻松松，我从来没有想过，股票投资是如此的暴利，市场上愚蠢的钱是如此之多，捡钱是如此的简单。余生几十年，我不过是简单重复这个捡钱的过程而已。</p>
<p>其实，只要大家和我一样，长期持有优秀公司，那么市场的波动就会变得非常小。就不会出现这种超额收益的现象。最终大家都是平均分享上市公司的发展成果。但大家贪婪，总是不满足，总是想自己会更聪明战胜其他人，最终导致的结果是自己打败了自己。</p>
<p>我从来没有战胜市场，只是其他人自己打败了自己。所有人都自己把自己累死，我这个一动不动的人，反而成为幸存者，活到最后，捡起大家遗落的钱。</p>
<p>我从来没有想过赢，没有想过要超越谁，我只是埋头做好自己，埋头持有好公司。但市场却自己常常犯错，市场之愚蠢，不可思异。</p>
<p>所以我对投资的领悟，已经超脱了对行业与公司的基本研究——这些研究其实也很简单，简单关注一下就行了。买万科还是买保利？买美的还是买格力？买茅台还是买五粮液？我根本不在乎，花了一大堆精力去研究，也许收益率就提升两三个点左右吧。甚至是常常自己迷失在自以为是的研究当中。花费极大的精力将行业企业研究到极致（还不一定研究方向就是正确的，多数时候不如傻傻持有），不如保持人性上保持简单纯粹，赚人性的钱，一不小心就是动不动就是数以倍计。因为这根本不花费我任何精力，也不会犯任何错。企业研究有时候可以是越研究越错，毕竟信息不对称。而人性的修练，怎么练都是对的，就是保持简单平静就好了。</p>
<p>永远追求做一个好人，做一个平静的人，永远是对的。而这对于我来说，根本不花精力，原因我的性格天生如此，不喜欢争，人多的地方永远不去，吵的地方待不久。所以我赚人性的钱轻轻松松，动不动就是以翻倍为结果，利润远远超越赚研究行业企业的辛苦血汗钱。当然，两者皆赚，并不矛盾。只是一个需要付出极大的努力，最终才能提升获得一点点的收益，甚至可能弄巧反拙。而修心养性，却是提升收益的另一个维度。如同你无论多么努力踩自行车就是这个速度，而汽车只需要轻轻一点油门。修心养性不仅能提升投资上的收益，更重要的是提升整个生命的质量。</p>
<p>德位相配，修练到有足够的财富承受能力，拥有巨大的财富，然后你又发现，其实你根本不需要财富，一样过得自由快乐。如同我没有想赚超额的利润，利润却自己送上门来。</p>
<p>这就是我和一般投资者的本质差异吧。而且这种差异是自带的天赋，一般人也难以模仿。因为大多数人的人性不适合。佛法说静极生慧，而投资上是静极生暴利。说白了就是不犯错的人，去捡那些犯错扔下的钱。简单到极致，便不会犯错，也可以持久重复。</p>
<p>退可守，守着优秀的公司，每年10%-15%的收益，非常的满足，这让我首先立于不败之地。进可攻，静待市场犯错，每两三年便有一次翻倍的暴利机会。用我简单到极致的纯粹，去碰撞市场上各种复杂的人性，不胜而胜。</p>
<p>所以总结下来，长期投资是分了三个阶段。</p>
<p>第一阶段，投资指数，赚市场平均增长的钱，长期下来收益率10%。</p>
<p>第二阶段，指数的进化版，优中选优，好行业好公司，长期下来收益率15%-20%。</p>
<p>投资来到这里，我觉得已经可以了，剩下的就是坚持而已。所以我说投资很简单。任何说投资很复杂的人，都是贪婪之人，他们想追求第三个阶段。</p>
<p>第三阶段，就是在第二阶段的基础上，低估买入，高估卖出，赚人性的钱。按经验看，一般3-5年倍便会出现收益翻倍的大波动，收益率可以达到年化的30%，甚至有时达到50%。这里就可以看出人性的魔力，一个公司的正常增长率长期下来不过15%左右，就人性的波动，动不动就是翻倍。</p>
<p>想赚第三阶段的钱需要天赋，而且机会可遇不可求。凡求者，必败。所以我从来不强求。我只立足于第二阶段。因为不求，所以无欲则刚，立于不败之地。等得起，机会自然会来，所以我常常做到了第三阶段。我能够做到了，是因为我不求。很多人，只是“假装不求”，实质上还是在强求，自我欺骗。能够做到这点，要么是具有天赋的性格，像我这种性格，懒出境界的人，与世无争。要么就是人世间事情经历多了，终于大彻大悟。求的最高境界是不求，不求，自然会来。所以芒格说，四十岁以下，没有真正的价值投资者。价值投资，除了极少数天赋异禀的人之外，大多数的确需要经历足够多的人生历练，才会有彻悟。</p>
<p>所以第三阶段，说简单也简单，说困难也非常难。方法很简单，心法很困难。方法就是这样了，很难再改进。但心法是一个更高的维度，稍稍把人性的弱点修正一下，在投资上就会有非凡的表现。把人性修行好，在投资的战场上，当相于你拥有了飞机坦克，对与拿着长矛的人交战，这是碾压式的合法的抢劫。</p>
<p>当你投资不再想着赚钱的时候，就自然会赚很多钱。假如你练习书法，是为了将来能够写一字值千金，那么你的书法永远写不好。只有把投资当成是一种修行之人，当成是一种人生信仰，投资才真正做到极致。投资做好了，结果不就是会赚到大钱吗？所以我的出发点，只是想把投资做好，享受这个投资的乐趣，而非想着钱，这有本质的不同。想着钱的人，总是该买的时候不敢买，害怕继续跌。该卖的时候贪婪着不想卖，害怕继续升。买了不应该买的，害怕出黑马。想赚尽市场的钱，眼红别人赚得比自己多。</p>
<p>而我只是享受发现一个低估的股票，低估了，便买入，高估了，便卖出，然后继续寻找下一个。每次发现一个低估的股票的时候，如同一个小孩子发现一个新的玩具般高兴。从来没有恐惧与贪婪之心。所以买入之后，下跌便下跌了，我也不再乎。卖出之后常常继续升，我也不眼红。我只是享受这一件玩具而已。</p>
<p>现在投资，已经成为我的一个修行工具。如同有些人练太极，有些人写书法，有些人打座静心，而我只是做投资而已，道是一样的。所谓投资上的修行，我的理解就是把自己给做好，做一个好人，做一个淡然之人。平常把每一件小事给做好，就是我的日常修行。从喝水开始，从吃饭开始，从睡觉开始，从跟家人好好说话开始，我已经就是修行。</p>
<p>我的修行不需要去深山闭关，吃好饭睡好觉，做每一件事保持平静之心，就是修行的最高境界。我没有刻意去修行，却又时时刻刻在修行。无处，却又无处不在。简单到极致，空无一物，却又万法加身。</p>
<p>道，从来没有离开生活。生活之道，才是最高的修行。</p>
<p>我的人生，从来没有什么大事，都是一件件平凡小事，每一件小事，我都力求做到极致，这就是我人生修行的最高境界。自律到信仰，信仰就是自律到极致，自律到自然，成为本能，无须刻意，已无执意，一举手一投足，皆发自内心，又顺应自然，合乎于道。自律没有成为本能之前，是痛苦的，是刻意为之，是“时时勤拂拭，莫使有尘埃”。当自律成为了本能，就不再有痛苦，反而因为自律，因为战胜自我的人性，会产生极大的快乐。成为信仰之后，根本无须刻意，无须强求。自律到极致，就无须再自律。别人眼中的自律、勤奋、节约，在我眼中不过如呼吸一般自然存在。“本来无一物，何处惹尘埃”。</p>
<p>人性修练到这个境界，投资赚钱不过如探囊取物一般的简单。任何认为投资困难的人，本质上是人性还没有到境界。不就是耐心等待嘛，等待低估买入，等待公司成长，等待市场高估，一切不过是慢慢来，慢慢等，就完成了，有什么难的？对于我来说，已经不存在耐心这个概念，我根本没有想过自己是否耐心，别人眼中的耐心，在我这里，不过是一种本能般自然的事。</p>
<p>山不是山，水不是水，投资也不再是那个投资。投资信仰变成了人生信仰，我得到比金钱重要千万倍的东西。投资除了带给我金钱之外，也改变了我的人生观。让家庭更和谐，心灵更自由，身体更健康，朋友更尊重，这绝对不是用钱就能买来的财富，这是比金钱更高维度的财富。投资无法解决所有问题，甚至我越来越觉得，不能解决大部分问题。有钱有时间有精力，只是第一个条件，有了条件跨进一个新的维度，下一步其实更关键，你拥有金钱时间之后，你选择做什么？别以为有钱了家人就会爱戴你，别以为有钱了朋友就会尊重你, 别以为有钱了你的心灵就会获得平静，别以为有钱了健康就会随之而来，还差一万光年。</p>
<p>年轻的时候一无所有，我想当然的认为，有了一百万，家人就会更尊敬我更爱我，就不会产生矛盾。很快，我赚了一生的第一个一百万，但情况并没有改善，矛盾依旧。我想，可能因为我还不够富有，如果能够有一千万，一定能够改善了。过了几年，资产又增长了十倍，我突然醒悟到，家人的爱，与你拥有多少钱毫无关系。很多人甚至越有钱，家庭矛盾越大。因为你越有钱，你自以为自己能力越大，越认为自己了不起，越认为其他人应该尊重你应该礼让于你，越是自我为中心，矛盾便越大。所以我醒悟了，我开始追求金钱之外的修行。投资在一般人眼中只是为了赚钱，我是为了修行。这是借由投资的修行，真正从财务自由，到达了人生自由。</p>
<p>全心全意无限包容地去爱家人，孩子的成长全程陪伴在身边。首先我要把自己做好，首先我要主动去爱，主动去付出，言传身教。我不再求了，我不再自以为中心，然后又慢慢的，身边所有的人，又受到我的感染，最终结果又变成了以我为中心，但此中心已非彼中心，以前别人是因为怕我，现在别人是因为爱我尊重我，以我为精神领导。家庭非常和谐的统一在一起。整个家庭，从我的父母，到我的妻子，再到我的三个孩子，他们都非常清楚明白，投资便是今后百年的发展战略。一旦形成这种高度统一的凝聚力，家庭，就升华成家族。我们每个人心中，深刻理解了这个家族百年发展的大战略，再没有任何阻力阻碍家庭的百年复利战略</p>
]]></content>
      <categories>
        <category>人生感悟</category>
      </categories>
      <tags>
        <tag>投资理财</tag>
        <tag>人生感悟</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶定位算法-直方图滤波(Histogram Filter)定位</title>
    <url>/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/</url>
    <content><![CDATA[<h1 id="直方图滤波-Histogram-Filter-的算法思想"><a href="#直方图滤波-Histogram-Filter-的算法思想" class="headerlink" title="直方图滤波(Histogram Filter)的算法思想"></a>直方图滤波(Histogram Filter)的算法思想</h1><p>直方图滤波的算法思想在于：它把整个状态空间dom(x(t))切分为互不相交的部分$b_1、b_2、…,b_{n-1}$，使得：</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/1524143870919032686cb02.jpeg" alt="无人驾驶高精度定位技术(1)-直方图滤波Histogram Filter"></p>
<p>然后定义一个新的状态空间$y_t \in \{0,…,n−1\}$，当且仅当$x(t)∈b_i$时，$y_t=i$,由于$y_t$是一个离散的状态空间，我们就可以采用离散贝叶斯算法计算$bel(y_t)$。$bel(y_t)$是对$bel(x_t)$的近似，它给出x(t)在每一个$b_i$的概率，$b_i$划分的越精细，计算的结果就越精确，当然精确的代价是计算成本的上升。</p>
<span id="more"></span>

<h1 id="1D直方图滤波在自动驾驶定位的应用"><a href="#1D直方图滤波在自动驾驶定位的应用" class="headerlink" title="1D直方图滤波在自动驾驶定位的应用"></a>1D直方图滤波在自动驾驶定位的应用</h1><p>如下图所示，无人驾驶汽车在一维的宽度为5m的世界重复循环，因为世界是循环的，所以如果无人驾驶汽车到了最右侧，再往前走一步，它就又回到了最左侧的位置。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/15242324358146dc9241c7f.png"></p>
<p>自动驾驶汽车上安装有Sensor可以检测车辆当前所在位置的颜色，但Sensor本身的存在一定检测错误率，即Sensor对颜色的检测不是100%准确的；</p>
<p>无人驾驶汽车以自认为1m/step的恒定速度向右运动，车辆运动本身也存在误差，即向车辆发出的控制命令是向右移动2m，而实际的车辆运动结果可能是待在原地不动，可能向右移动1m，也可能向右移动3m。</p>
<h2 id="数学模型"><a href="#数学模型" class="headerlink" title="数学模型"></a><strong>数学模型</strong></h2><p>从数学的语言来描述这个问题：机器人只有一个状态变量：位置，$pos(t) \in [0,5)$。应用直方图滤波(Histogram Filter)，对状态空间做如下分解:</p>
<p>$$<br>b_0 \in [0,1),<br>b_1 \in [1,2),<br>b_2 \in [2,3),<br>b_3 \in [3,4),<br>b_4 \in [4,5)<br>$$</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/15242324358146dc9241c7f.jpeg" alt="无人驾驶高精度定位技术(1)-直方图滤波Histogram Filter"></p>
<p>于是得到一个新的状态空间$y(t) \in {0,…,4}$，它是对连续状态空间的近似，在某一时刻车辆只能在这些离散状态中的一个。</p>
<p>虽然车辆自认为在向右运动，每一步运动1m，我们假设存在5%的概率，无人驾驶汽车仍待在原地没动；存在90%的概率车辆在向右移动1m；存在5%的概率无人驾驶汽车在向右运动2m。</p>
<p>$$<br>\begin{aligned}<br>\text{P}(y_t &amp;= (x+2) mod 5 y_{t-1} = x) = 0.05 \\<br>\text{P}(y_t &amp;= (x+1) mod 5 y_{t-1} = x = 0.9 \\<br>\text{P}(y_t &amp;= xy_{t-1} = x)=0.05 \\<br>\end{aligned}<br>$$</p>
<p>无人驾驶汽车的Sensor假设90%的概率感知结果是正确的，还有10%的情况下它感知的结果是错误的。</p>
<p>$$<br>\begin{aligned}<br>\text{P}(\text{MeasuredColor}_t = \text{Blue} y_t = 0,2,3) &amp;= 0.9 \\<br>\text{P}(\text{MeasuredColor}_t = \text{Orange} y_t = 0,2,3) &amp;= 0.1 \\<br>\text{P}(\text{MeasuredColor}_t = \text{Blue} y_t = 1,4) &amp;= 0.1 \\<br>\text{P}(\text{MeasuredColor}_t = \text{Orange} y_t = 1,4) &amp;= 0.9 \\<br>\end{aligned}<br>$$</p>
<h2 id="利用直方图滤波-Histogram-Filter-进行车辆定位的过程"><a href="#利用直方图滤波-Histogram-Filter-进行车辆定位的过程" class="headerlink" title="利用直方图滤波(Histogram Filter)进行车辆定位的过程"></a>利用直方图滤波(Histogram Filter)进行车辆定位的过程</h2><p>我们用一个5维的向量来描述t时刻，无人驾驶汽车位于第1个格子、第2个格子、第3个格子、第4个格子、第5个格子的概率。</p>
<p>$$\text{bel}(y_t) = (\text{bel}_{t,1}, \text{bel}_{t,2}, \text{bel}_{t,3}, \text{bel}_{t,4}, \text{bel}_{t,5})$$</p>
<p>无人驾驶汽车对自己所在位置一无所知，假设它连续三次【运动-向右走一步】-【观测】,三次观测结果分别是orange、blue、orange，我们一步步无人驾驶汽车是如何通过【运动】-【观测】过程逐步确认自己的位置的。</p>
<p><strong>t=0时刻</strong></p>
<p>没有任何先验知识，无人车不知道自己在哪里,所以在各个位置概率相等：</p>
<p>$$\text{bel}(y_0) = (0.2, 0.2, 0.2, 0.2, 0.2)$$</p>
<p><strong>t=1时刻</strong></p>
<p>首先向右走1m，用运动模型进行位置预测</p>
<p>$$<br>\begin{aligned}<br>\overline{\text{bel}}(y_1) &amp;= \sum_{y_0} \text{P}(y_1 y_0) P(y_0) \\<br>&amp;=(0.05,0.9,0.05,0,0) * 0.2+(0,0.05,0.9,0.05,0) * 0.2 \\<br>&amp;+(0,0.05,0.9,0.05) * 0.2+(0.05,0,0,0.05,0.9) * 0.2 \\<br>&amp;+(0.9,0.05,0,0,0.05) * 0.2 \\<br>&amp;=(0.2,0.2,0.2,0.2,0.2)<br>\end{aligned}<br>$$</p>
<p>可以看出无人车虽然向前运动一步，但它仍然对自己所在位置一无所知。这也和我们的认知相同，刚开始完全不知道自己在哪里，走了一步自然也完全不知道自己在哪里。</p>
<p>再用更新模型通过Sensor感知环境，更新当前位置的置信度。</p>
<p>$$<br>\begin{aligned}<br>\text{bel}(y_1) &amp;= \eta \text{P}(\text {MeasuredColor}_1 = \text{orange} {y_1}) \overline {\text {bel}} (y_1) \\<br>&amp; = \eta (0.1,0.9,0.1,0.1,0.9) {*} (0.2,0.2,0.2,0.2,0.2) \\<br>&amp; = \eta (0.02,0.18,0.02,0.02,0.18) \\<br>&amp; = (0.04762,0.42857,0.04762,0.04761,0.42857)<br>\end{aligned}<br>$$</p>
<p>orange的颜色感知信息使得传感器认为自己很可能位于第二个和第五个方格中。</p>
<p><strong>t=2时刻</strong>  </p>
<p>运动模型-向右走1m</p>
<p>$$<br>\begin{aligned}<br>\overline{\text{bel}}(y_2) &amp;= \sum_{y_1} \text{P}(y_2 y_1) \text{P}(y_1) \\<br>&amp;= (0.39048,0.08571,0.39048,0.06667,0.06667) \\<br>\end{aligned}<br>$$</p>
<p>更新模型-sensor环境感知</p>
<p>$$<br>\begin{aligned}<br>\text{bel}(y_2) &amp;= \eta \text{P}(\text{MeasuredColor}_2 = \text{blue} y_2) \overline{\text{bel}}(y_2) \\<br>&amp;=\eta (0.1,0.9,0.1,0.1,0.9) {*} (0.39048,0.08571,0.39048,0.06667,0.06667) \\<br>&amp;=(0.45165,0.01102,0.45165,0.07711,0.00857) \\<br>\end{aligned}<br>$$</p>
<p><strong>t=3时刻</strong></p>
<p>运动模型-向右走1m</p>
<p>$$<br>\begin{aligned}<br>\overline{\text{bel}}(y_3) &amp;= \sum_{y_2} \text{P}(y_3 y_2) \text{P}(y_2) \\<br>&amp;= =(0.03415,0.40747,0.05508,0.41089,0.09241) \\<br>\end{aligned}<br>$$</p>
<p>感知更新模型-sensor环境感知</p>
<p>$$<br>\begin{aligned}<br>\text{bel}(y_3) &amp;= \eta \text{P}(\text{MeasuredColor}_3 = \text{orange} y_3) \overline{\text{bel}}(y_3) \\<br>&amp;= \eta(0.1,0.9,0.1,0.1,0.9){*}(0.03415,0.40747,0.05508,0.41089,0.09241) \\<br>&amp;= (0.00683,0.73358,0.01102,0.08219,0.16637) \\<br>\end{aligned}<br>$$</p>
<p>可以看到经过三次的运动和观测后，无人驾驶汽车已经有73%的概率确认自己位于第二个网格中，事实再经过三次的运动观测，无人驾驶汽车可以有94%的概率确认自己的位置。</p>
<h1 id="2D直方图滤波在自动驾驶定位中的应用-一"><a href="#2D直方图滤波在自动驾驶定位中的应用-一" class="headerlink" title="2D直方图滤波在自动驾驶定位中的应用(一)"></a>2D直方图滤波在自动驾驶定位中的应用(一)</h1><p>1D的直方图滤波可以很好的帮助我们理解直方图滤波的原理以及在如何应用在自动驾驶的定位过程中。但是1D的直方图滤波在实际应用中几乎是不存在的，所以我们从更偏向应用的角度，看看2D直方图滤波在自动驾驶定位中是如何工作的。</p>
<h2 id="定义二维地图"><a href="#定义二维地图" class="headerlink" title="定义二维地图"></a>定义二维地图</h2><p>首先定义一张二维地图，R和G代表地图块的颜色：R为红色，G为绿色。每个地图块的大小根据实际应用而定，比如0.0125m*0.125m、0.025m*0.025m等。地图块越小，定位精度越高，但是地图数据量和计算量也就越大；反之，地图块越大，定位精度越低，但数据量和计算量也相应较低。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grid &#x3D; [</span><br><span class="line">    [R,G,G,G,R,R,R],</span><br><span class="line">    [G,G,R,G,R,G,R],</span><br><span class="line">    [G,R,G,G,G,G,R],</span><br><span class="line">    [R,R,G,R,G,G,G],</span><br><span class="line">    [R,G,R,G,R,R,R],</span><br><span class="line">    [G,R,R,R,G,R,G],</span><br><span class="line">    [R,R,R,G,R,G,G],</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>t=0时刻，车辆不知道自己处于地图中的具体位置，转化为数学表述，就是车辆在各个地图块的置信度相同，代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_beliefs</span>(<span class="params">grid</span>):</span></span><br><span class="line">    height = <span class="built_in">len</span>(grid)</span><br><span class="line">    width = <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">    area = height * width</span><br><span class="line">    belief_per_cell = <span class="number">1.0</span> / area</span><br><span class="line">    beliefs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        row = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">            row.append(belief_per_cell)</span><br><span class="line">        beliefs.append(row)</span><br><span class="line">    <span class="keyword">return</span> beliefs</span><br></pre></td></tr></table></figure>

<p>初始置信度如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020  </span><br><span class="line">0.020  0.020  0.020  0.020  0.020  0.020  0.020</span><br></pre></td></tr></table></figure>

<p>置信度的可视化如下，红色星星位置为车辆的真实初始实际位置，蓝色圈大小代表置信度的高低，蓝色圈越大，置信度越高，蓝色圈越小，置信度越低。t=0时刻，车辆不确定自己的位置，所以各个位置的置信度相等。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-06-22-50-31.png"></p>
<h2 id="运动更新"><a href="#运动更新" class="headerlink" title="运动更新"></a>运动更新</h2><p>车辆运动模型简化为x、y两个方向的运动，同时由于运动的不确定性，需要对运动后的位置增加概率性信息。</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move</span>(<span class="params">dy, dx, beliefs, blurring</span>):</span></span><br><span class="line">    height = <span class="built_in">len</span>(beliefs)</span><br><span class="line">    width = <span class="built_in">len</span>(beliefs[<span class="number">0</span>])</span><br><span class="line">    new_G = [[<span class="number">0.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(width)] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(height)]</span><br><span class="line">    <span class="keyword">for</span> i, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(beliefs):</span><br><span class="line">        <span class="keyword">for</span> j, cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(row):</span><br><span class="line">            new_i = (i + dy ) % height</span><br><span class="line">            new_j = (j + dx ) % width</span><br><span class="line">            new_G[<span class="built_in">int</span>(new_i)][<span class="built_in">int</span>(new_j)] = cell</span><br><span class="line">    <span class="keyword">return</span> blur(new_G, blurring)</span><br></pre></td></tr></table></figure>

<h2 id="观测更新"><a href="#观测更新" class="headerlink" title="观测更新"></a>观测更新</h2><p>观测更新的过程中，当观测的Color等于地图块的Color时，hit=1， bel=beliefs[i][j] * p_hit；当观测到的Color不等于地图块的Color时，hit=0， bel=beliefs[i][j] * p_miss。</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sense</span>(<span class="params">color, grid, beliefs, p_hit, p_miss</span>):</span></span><br><span class="line">    new_beliefs = []</span><br><span class="line"> </span><br><span class="line">    height = <span class="built_in">len</span>(grid)</span><br><span class="line">    width = <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop through all grid cells</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        row = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">            hit = (color == grid[i][j])</span><br><span class="line">            row.append(beliefs[i][j] * (hit * p_hit + (<span class="number">1</span>-hit) * p_miss))</span><br><span class="line">        new_beliefs.append(row)</span><br><span class="line">        </span><br><span class="line">    s = <span class="built_in">sum</span>(<span class="built_in">map</span>(<span class="built_in">sum</span>, new_beliefs))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">            new_beliefs[i][j] = new_beliefs[i][j] / s</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> new_beliefs</span><br></pre></td></tr></table></figure>

<h2 id="运行定位流程"><a href="#运行定位流程" class="headerlink" title="运行定位流程"></a>运行定位流程</h2><p>单次直方图滤波定位过程中，先进行观测更新，再进行运动更新。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, num_steps=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">self.sense()</span><br><span class="line">dy, dx = self.random_move()</span><br><span class="line">self.move(dy,dx)</span><br></pre></td></tr></table></figure>

<p>设置运动更新的不确定度为0.1，观测更新的错误率：每隔100次观测出现一次观测错误，车辆的真实初始位置为(3,3),注意，这个真实位置车辆自己并不知道，我们只是为了仿真而设置的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blur = <span class="number">0.1</span></span><br><span class="line">p_hit = <span class="number">100.0</span></span><br><span class="line">init_pos = (<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">simulation = sim.Simulation(grid, blur, p_hit, init_pos)</span><br><span class="line"></span><br><span class="line">simulation.run(<span class="number">1</span>)</span><br><span class="line">simulation.show_beliefs()</span><br><span class="line">show_rounded_beliefs(simulation.beliefs)</span><br></pre></td></tr></table></figure>

<p>经过一次直方图滤波定位之后，各个位置的置信度已经发生了变化。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.003  0.002  0.036  0.002  0.037  0.003  0.038  </span><br><span class="line">0.003  0.037  0.002  0.002  0.001  0.002  0.037  </span><br><span class="line">0.038  0.038  0.003  0.036  0.002  0.002  0.003  </span><br><span class="line">0.038  0.004  0.038  0.003  0.037  0.038  0.038  </span><br><span class="line">0.003  0.038  0.039  0.038  0.003  0.037  0.003  </span><br><span class="line">0.038  0.038  0.038  0.003  0.037  0.003  0.003  </span><br><span class="line">0.038  0.003  0.002  0.002  0.038  0.038  0.038</span><br></pre></td></tr></table></figure>

<p>置信度的可视化效果如下。可以看到，车辆已经对自己的置信度有了一定的认知，但是还是有大量的可能位置需要进一步确认。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-06-23-25-12.png"></p>
<p>连续执行直方图滤波100次，各个位置置信度的数值如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.008  0.000  0.000  0.000  0.000  0.016  0.016  </span><br><span class="line">0.032  0.001  0.000  0.000  0.001  0.032  0.833  </span><br><span class="line">0.016  0.000  0.000  0.000  0.000  0.025  0.017  </span><br><span class="line">0.001  0.000  0.000  0.000  0.000  0.000  0.000  </span><br><span class="line">0.000  0.000  0.000  0.000  0.000  0.000  0.000  </span><br><span class="line">0.000  0.000  0.000  0.000  0.000  0.000  0.000  </span><br><span class="line">0.000  0.000  0.000  0.000  0.000  0.000  0.000</span><br></pre></td></tr></table></figure>

<p>置信度的可视化效果如下,可以看到，车辆已经83.3%的概率可以确定自己所处的位置了。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-06-23-32-51.png"></p>
<h1 id="2D直方图滤波在自动驾驶定位中的应用-二"><a href="#2D直方图滤波在自动驾驶定位中的应用-二" class="headerlink" title="2D直方图滤波在自动驾驶定位中的应用(二)"></a>2D直方图滤波在自动驾驶定位中的应用(二)</h1><p>车辆状态的定义如下：</p>
<p>$$<br>\begin{aligned}<br>{x}_{t} &amp;=<br>\begin{bmatrix}<br>\text{x} \\<br>\text{y} \\<br>\theta \\<br>\text{v} \\<br>\end{bmatrix}<br>\end{aligned}<br>$$</p>
<p>其中，(x,y)是车辆的位置，$\theta$是车辆的朝向，v是车辆的运动速度，我们假设车辆是匀速运动的，$\omega$是车辆运动的角速度。</p>
<p>车辆运动方程如下，其实就是:</p>
<p>$x_{t+1} = x_t + v * cos(\theta) * \Delta t$<br>$y_{t+1} = y_t + v * sin(\theta) * \Delta t$<br>$\theta_{t+1} = \theta_t + \omega * \Delta t$</p>
<p>写成矩阵形式:</p>
<p>$$<br>x_{t+1} =<br>\begin{bmatrix}<br>1.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\<br>0.0 &amp; 1.0 &amp; 0.0 &amp; 0.0 \\<br>0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\<br>0.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\<br>\end{bmatrix} *<br>\begin{bmatrix}<br>\text{x} \\<br>\text{y} \\<br>\theta \\<br>\text{v}<br>\end{bmatrix}  +<br>\begin{bmatrix}<br>\Delta t cos(\theta) &amp; 0.0 \\<br>\Delta t sin(\theta) &amp; 0.0 \\<br>0.0 &amp; \Delta t \\<br>1.0 &amp; 0.0 \\<br>\end{bmatrix} *<br>\begin{bmatrix}<br>v \\<br>\omega \\<br>\end{bmatrix}<br>$$</p>
<p>车辆的运动模型代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">motion_model</span>(<span class="params">x, u</span>):</span></span><br><span class="line">    F = np.array([[<span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    B = np.array([[DT * math.cos(x[<span class="number">2</span>, <span class="number">0</span>]), <span class="number">0</span>],</span><br><span class="line">                  [DT * math.sin(x[<span class="number">2</span>, <span class="number">0</span>]), <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0.0</span>, DT],</span><br><span class="line">                  [<span class="number">1.0</span>, <span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line">    x = F @ x + B @ u</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="运动更新-1"><a href="#运动更新-1" class="headerlink" title="运动更新"></a>运动更新</h2><p>运动更新的过程与前面谈到的车辆运动模型一致，车辆运动有不确定性，所以增加了Gaussian Filter用来处理不确定性。还有一个细节，就是车辆运动距离和直方图滤波的分块地图之间的转换关系：</p>
<p>x_shift = $\Delta$ x / map_x_resolution</p>
<p>y_shift = $\Delta$ y / map_y_resolution</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># grid_map是网格地图，u=(v,w)是车辆运动的控制参数，yaw是车辆朝向</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">motion_update</span>(<span class="params">grid_map, u, yaw</span>):</span></span><br><span class="line">    <span class="comment"># DT是时间间隔</span></span><br><span class="line">    grid_map.dx += DT * math.cos(yaw) * u[<span class="number">0</span>]</span><br><span class="line">    grid_map.dy += DT * math.sin(yaw) * u[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># grid_map.xy_reso是地图分辨率</span></span><br><span class="line">    x_shift = grid_map.dx // grid_map.xy_reso</span><br><span class="line">    y_shift = grid_map.dy // grid_map.xy_reso</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">abs</span>(x_shift) &gt;= <span class="number">1.0</span> <span class="keyword">or</span> <span class="built_in">abs</span>(y_shift) &gt;= <span class="number">1.0</span>:  <span class="comment"># map should be shifted</span></span><br><span class="line">        grid_map = map_shift(grid_map, <span class="built_in">int</span>(x_shift), <span class="built_in">int</span>(y_shift))</span><br><span class="line">        grid_map.dx -= x_shift * grid_map.xy_reso</span><br><span class="line">        grid_map.dy -= y_shift * grid_map.xy_reso</span><br><span class="line">    <span class="comment"># MOTION_STD是车辆运动不确定性的标准差</span></span><br><span class="line">    grid_map.data = gaussian_filter(grid_map.data, sigma=MOTION_STD)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grid_map</span><br></pre></td></tr></table></figure>

<h2 id="观测更新-1"><a href="#观测更新-1" class="headerlink" title="观测更新"></a>观测更新</h2><p>这个例子中通过测量车辆到LandMark的距离来确定自身的位置，LandMark的位置都是已知的。</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/Screenshot-from-2019-12-07-11-55-06.png"></p>
<p>2D 直方图滤波定位算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_gaussian_observation_pdf</span>(<span class="params">gmap, z, iz, ix, iy, std</span>):</span></span><br><span class="line">    <span class="comment"># predicted range</span></span><br><span class="line">    x = ix * gmap.xy_reso + gmap.minx</span><br><span class="line">    y = iy * gmap.xy_reso + gmap.miny</span><br><span class="line">    d = math.sqrt((x - z[iz, <span class="number">1</span>]) ** <span class="number">2</span> + (y - z[iz, <span class="number">2</span>]) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># likelihood</span></span><br><span class="line">    pdf = (<span class="number">1.0</span> - norm.cdf(<span class="built_in">abs</span>(d - z[iz, <span class="number">0</span>]), <span class="number">0.0</span>, std))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pdf</span><br><span class="line"></span><br><span class="line">    <span class="comment">#z=[(车辆到Landmark的测量距离，Landmark的x坐标，Landmark的y坐标),...]，z是所有Landmark测量距离和位置的集合，std是测量误差的标准差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">observation_update</span>(<span class="params">gmap, z, std</span>):</span></span><br><span class="line">    <span class="keyword">for</span> iz <span class="keyword">in</span> <span class="built_in">range</span>(z.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> ix <span class="keyword">in</span> <span class="built_in">range</span>(gmap.xw):</span><br><span class="line">            <span class="keyword">for</span> iy <span class="keyword">in</span> <span class="built_in">range</span>(gmap.yw):</span><br><span class="line">                gmap.data[ix][iy] *= calc_gaussian_observation_pdf(</span><br><span class="line">                    gmap, z, iz, ix, iy, std)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 概率归一化</span></span><br><span class="line">    gmap = normalize_probability(gmap)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gmap</span><br></pre></td></tr></table></figure>

<h2 id="运行定位流程-1"><a href="#运行定位流程-1" class="headerlink" title="运行定位流程"></a>运行定位流程</h2><p>设置地图和测量相关参数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DT = <span class="number">0.1</span>  <span class="comment"># time tick [s]</span></span><br><span class="line">MAX_RANGE = <span class="number">10.0</span>  <span class="comment"># maximum observation range</span></span><br><span class="line">MOTION_STD = <span class="number">1.0</span>  <span class="comment"># standard deviation for motion gaussian distribution</span></span><br><span class="line">RANGE_STD = <span class="number">3.0</span>  <span class="comment"># standard deviation for observation gaussian distribution</span></span><br></pre></td></tr></table></figure>
<p>grid map param</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">XY_RESO = <span class="number">0.5</span>  <span class="comment"># xy grid resolution</span></span><br><span class="line">MINX = -<span class="number">15.0</span></span><br><span class="line">MINY = -<span class="number">5.0</span></span><br><span class="line">MAXX = <span class="number">15.0</span></span><br><span class="line">MAXY = <span class="number">25.0</span></span><br></pre></td></tr></table></figure>
<p>Landmark Position</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RF_ID = np.array([[<span class="number">10.0</span>, <span class="number">0.0</span>],</span><br><span class="line">                 [<span class="number">10.0</span>, <span class="number">10.0</span>],</span><br><span class="line">                 [<span class="number">0.0</span>, <span class="number">15.0</span>],</span><br><span class="line">                 [-<span class="number">5.0</span>, <span class="number">20.0</span>]])</span><br></pre></td></tr></table></figure>
<p>车辆的初始位置(for simulation)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xTrue = np.zeros((<span class="number">4</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>通过Observation模拟自动驾驶车辆对各个LandMark的观测结果和车辆速度的误差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">observation</span>(<span class="params">xTrue, u, RFID</span>):</span></span><br><span class="line">    xTrue = motion_model(xTrue, u)</span><br><span class="line"></span><br><span class="line">    z = np.zeros((<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(RFID[:, <span class="number">0</span>])):</span><br><span class="line"></span><br><span class="line">        dx = xTrue[<span class="number">0</span>, <span class="number">0</span>] - RFID[i, <span class="number">0</span>]</span><br><span class="line">        dy = xTrue[<span class="number">1</span>, <span class="number">0</span>] - RFID[i, <span class="number">1</span>]</span><br><span class="line">        d = math.sqrt(dx ** <span class="number">2</span> + dy ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> d &lt;= MAX_RANGE:</span><br><span class="line">            <span class="comment"># add noise to range observation</span></span><br><span class="line">            dn = d + np.random.randn() * NOISE_RANGE</span><br><span class="line">            zi = np.array([dn, RFID[i, <span class="number">0</span>], RFID[i, <span class="number">1</span>]])</span><br><span class="line">            z = np.vstack((z, zi))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add noise to speed</span></span><br><span class="line">    ud = u[:, :]</span><br><span class="line">    ud[<span class="number">0</span>] += np.random.randn() * NOISE_SPEED</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xTrue, z, ud</span><br></pre></td></tr></table></figure>

<p>执行车辆定位流程:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> SIM_TIME &gt;= time:</span><br><span class="line">    time += DT</span><br><span class="line">    print(<span class="string">&quot;Time:&quot;</span>, time)</span><br><span class="line"></span><br><span class="line">    u = calc_input()</span><br><span class="line"></span><br><span class="line">    yaw = xTrue[<span class="number">2</span>, <span class="number">0</span>]  <span class="comment"># Orientation is known</span></span><br><span class="line">    xTrue, z, ud = observation(xTrue, u, RF_ID)</span><br><span class="line"></span><br><span class="line">    grid_map = histogram_filter_localization(grid_map, u, z, yaw)</span><br></pre></td></tr></table></figure>

<p>定位效果如下:</p>
<p><img src="/2019/12/07/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E4%B9%9D-%E7%9B%B4%E6%96%B9%E5%9B%BE%E6%BB%A4%E6%B3%A2histogram-filter%E5%AE%9A%E4%BD%8D/animation.gif" alt="3"></p>
<p>图片来源:<a href="https://github.com/AtsushiSakai/PythonRobotics">https://github.com/AtsushiSakai/PythonRobotics</a></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1.<a href="http://www.deepideas.net/robot-localization-histogram-filter/">http://www.deepideas.net/robot-localization-histogram-filter/</a><br>2.<a href="https://github.com/AtsushiSakai/PythonRobotics">https://github.com/AtsushiSakai/PythonRobotics</a></p>
<p>文中完整代码路径:</p>
<p>第3节中的代码github路径:<a href="https://github.com/iamshnoo/localization">https://github.com/iamshnoo/localization</a>，如果无法访问，可以直接通过以下链接下载。</p>
<p><a href="2D_Histogram_Filter.zip">2D_Histogram_Filter</a><a href="2D_Histogram_Filter.zip">下载</a></p>
<p>第4节中的代码的github路径:<a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/histogram_filter/histogram_filter.py">https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/histogram_filter/histogram_filter.py</a> 如果github无法访问，可以通过以下链接下载:</p>
<p><a href="histogram_filter.zip">histogram_filter</a><a href="histogram_filter.zip">下载</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>自动驾驶定位</tag>
        <tag>直方图滤波</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶定位算法-粒子滤波(Particle Filter)</title>
    <url>/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/</url>
    <content><![CDATA[<p>自动驾驶对定位的精度的要求在厘米级的，如何实现厘米级的高精度定位呢？一种众所周知的定位方法是利用全球定位系统(GPS)，利用多颗卫星的测量结果，通过三角测量(Triangulation)机制确定目标的位置，GPS定位的原理见《自动驾驶硬件系统-Global Navigation Satellite Systems》，但是GPS并不总是提供高精度定位数据，在GPS信号强的情况下，定位精度在1~3m范围内，在GPS信号弱的情况下，定位精度下降到10~50m范围内。虽然依赖于RTK，可以将卫星定位的精度提高到厘米级，但是在GPS信号弱的场景下，定位精度仍然不能满足应用需求。所以仅仅使用GPS不能实现高可靠的高精度定位的。</p>
<p>为了达到厘米级(3-10cm)的定位精度，除了GPS之外，还需要借助于其它传感器，如激光传感器(LIDAR)、径向距离和角度传感器(Radial distance and angle sensor，RADAR)、视觉传感器(Camera)等，然后利用特定的算法将这些信息融合起来。粒子滤波(Particle Filter)就是利用这些传感器产生的数据进行高精定位的一种常用方法。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/Particle_filters.gif" alt="Particle Filters From Udacity Lecture"></p>
<span id="more"></span>

<h1 id="粒子滤波-Particle-Filter-的算法思想"><a href="#粒子滤波-Particle-Filter-的算法思想" class="headerlink" title="粒子滤波(Particle Filter)的算法思想"></a>粒子滤波(Particle Filter)的算法思想</h1><p>相对之前提到的[标准卡尔曼滤波]，粒子滤波(Particle Filter)没有线性高斯分布的假设；相对于[直方图滤波]，粒子滤波(Particle Filter)不需要对状态空间进行区间划分。粒子滤波算法采用很多粒子对置信度$\text{bel}(x_t)$进行近似,每个粒子都是对t时刻机器人实际状态的一个猜测。</p>
<p>$\chi_t := { \text{p}_t^{[1]}, \text{p}_t^{[2]}, …, \text{p}_t^{[n]} }$,其中$\text{p}_t^{[i]} \in \text{dom}(x_t)$</p>
<p>越接近t时刻的Ground Truth状态描述的粒子，权重越高。</p>
<p>粒子更新的过程类似于达尔文的自然选择(Natural Selection)机制，与当前Sensor测量状态越匹配的粒子，有更大的机会生存下来，与Sensor测量结果不符的粒子会被淘汰掉，最终粒子都会集中在正确的状态附近。</p>
<h1 id="粒子滤波算法在自动驾驶定位中的应用"><a href="#粒子滤波算法在自动驾驶定位中的应用" class="headerlink" title="粒子滤波算法在自动驾驶定位中的应用"></a>粒子滤波算法在自动驾驶定位中的应用</h1><p>以放置在封闭环境(Close Environment)中的自动驾驶车辆(Kidnapped Vehicle)为例，看它是如何通过粒子滤波(Particle Filter)确定自己所在的位置的。</p>
<p><img src="http://p1.pstatp.com/large/pgc-image/1525514459134fc35d872c9" alt="无人驾驶高精度定位技术(3)-粒子滤波"></p>
<h2 id="先看看自动驾驶车辆有什么？"><a href="#先看看自动驾驶车辆有什么？" class="headerlink" title="先看看自动驾驶车辆有什么？"></a>先看看自动驾驶车辆有什么？</h2><p>1)地图(Map)。每一辆自动驾驶汽车都配备了一幅高精度的地图，地图中包含了大量的地标(Landmark)的位置信息。地标(Landmark)是特定区域内的稳定特征，它们能够在相当长的一段时间内都保持不变，比如路边建筑物、杆状物等。</p>
<p>2)GPS。使用安装在车辆内部的GPS传感器设备，可以提供车辆在地图中的大概的位置范围。基于此局部性范围，仅选择全局地图的一部分用以匹配计算，从而避免大量的冗余计算，提升计算的时效性，从而达到实时定位的目的。</p>
<p>3)Lidar/Radar等传感器。安装在车辆上的Lidar/Radar传感器将测量其与地图路标(Landmark)之间的距离，从而进一步确定车辆的位置。但是Lidar/Radar的测量结果也不准确，存在测量噪声。</p>
<h2 id="将所有传感器信息组合起来"><a href="#将所有传感器信息组合起来" class="headerlink" title="将所有传感器信息组合起来"></a>将所有传感器信息组合起来</h2><p>通过粒子滤波(Particle Filter)将这些所有信息组合在一起，用于实现实时的高精度定位。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/particle-filter-localization-algorithm-1024x576.png"></p>
<p>图片来源:<a href="https://github.com/sohonisaurabh/CarND-Kidnapped-Vehicle-Project">https://github.com/sohonisaurabh/CarND-Kidnapped-Vehicle-Project</a></p>
<p>粒子滤波(Particle Filter)的主要步骤如下:</p>
<p>1）Initialisation Step：在初始化步骤中，根据GPS坐标输入估算位置，估算位置是存在噪声的，但是可以提供一个范围约束。</p>
<p>2）Prediction Step：在Prediction过程中，对所有粒子(Particles)增加车辆的控制输入(速度、角速度等)，预测所有粒子的下一步位置。</p>
<p>3）Update Step：在Update过程中，根据地图中的Landmark位置和对应的测量距离来更新所有粒子(Particles)的权重。</p>
<p>4）Resample Step：根据粒子(Particles)的权重，对所有粒子(Particles)进行重采样，权重越高的粒子有更大的概率生存下来，权重越小的例子生存下来的概率就越低，从而达到优胜劣汰的目的。</p>
<h2 id="粒子滤波代码实现"><a href="#粒子滤波代码实现" class="headerlink" title="粒子滤波代码实现"></a>粒子滤波代码实现</h2><p><strong>1）Initialisation Step：</strong></p>
<p>初始化阶段，车辆接收到来自GPS的带噪声的测量值，并将其用于初始化车辆的位置。GPS的测量值包括车辆的位置P(x,y)和朝向$\theta$，并且假设测量结果的噪声服从正态分布。</p>
<p>我们创建100个粒子，并用GPS的测量值初始化这些粒子的位置和朝向。粒子的个数是一个可调参数，可根据实际效果和实际需求调整。初始化时，所有粒子的权重相同。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ParticleFilter::init</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">double</span> y, <span class="keyword">double</span> theta, <span class="keyword">double</span> <span class="built_in">std</span>[])</span> </span>&#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Set the number of particles. Initialize all particles to first</span></span><br><span class="line">  <span class="comment">// position (based on estimates of</span></span><br><span class="line">  <span class="comment">//   x, y, theta and their uncertainties from GPS) and all weights to 1.</span></span><br><span class="line">  <span class="comment">// Add random Gaussian noise to each particle.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> Consult particle_filter.h for more information about this method (and</span></span><br><span class="line">  <span class="comment">// others in this file).</span></span><br><span class="line">  <span class="comment">// TODO Complete</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/*Number is particles are chosen in order to run the algorithm in almost real</span></span><br><span class="line"><span class="comment">  time and introduce lowest possible error in localization. This is a tunable</span></span><br><span class="line"><span class="comment">  parameter.*/</span></span><br><span class="line">  num_particles = <span class="number">20</span>;</span><br><span class="line">  default_random_engine gen;</span><br><span class="line"></span><br><span class="line">  <span class="function">normal_distribution&lt;<span class="keyword">double</span>&gt; <span class="title">dist_x</span><span class="params">(x, <span class="built_in">std</span>[<span class="number">0</span>])</span></span>;</span><br><span class="line">  <span class="function">normal_distribution&lt;<span class="keyword">double</span>&gt; <span class="title">dist_y</span><span class="params">(y, <span class="built_in">std</span>[<span class="number">1</span>])</span></span>;</span><br><span class="line">  <span class="function">normal_distribution&lt;<span class="keyword">double</span>&gt; <span class="title">dist_theta</span><span class="params">(theta, <span class="built_in">std</span>[<span class="number">2</span>])</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_particles; i++) &#123;</span><br><span class="line">    Particle current_particle;</span><br><span class="line">    current_particle.id = i;</span><br><span class="line">    current_particle.x = dist_x(gen);</span><br><span class="line">    current_particle.y = dist_y(gen);</span><br><span class="line">    current_particle.theta = dist_theta(gen);</span><br><span class="line">    current_particle.weight = <span class="number">1.0</span>;</span><br><span class="line"></span><br><span class="line">    particles.push_back(current_particle);</span><br><span class="line">    weights.push_back(current_particle.weight);</span><br><span class="line">  &#125;</span><br><span class="line">  is_initialized = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>2）Prediction Step</strong></p>
<p>初始化完成之后，对所有粒子执行车辆运动模型，预测每个粒子下一步出现的位置。粒子的位置更新是通过以下公式完成:</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/prediction-equations-1024x431.png"></p>
<p>车辆运动过程同样存在噪声，前面的文章提到到，比如车辆发出控制指令velocity=10m/s，但是由于设备测量误差、车轮打滑等原因，实际的运动速度可能大于10m/s，也可能小于10m/s，我们同样假设车辆运动噪声服从正态分布。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ParticleFilter::prediction</span><span class="params">(<span class="keyword">double</span> <span class="keyword">delta_t</span>, <span class="keyword">double</span> std_pos[],</span></span></span><br><span class="line"><span class="function"><span class="params">                                <span class="keyword">double</span> velocity, <span class="keyword">double</span> yaw_rate)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Add measurements to each particle and add random Gaussian noise.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> When adding noise you may find std::normal_distribution and</span></span><br><span class="line">  <span class="comment">// std::default_random_engine useful.</span></span><br><span class="line">  <span class="comment">//  http://en.cppreference.com/w/cpp/numeric/random/normal_distribution</span></span><br><span class="line">  <span class="comment">//  http://www.cplusplus.com/reference/random/default_random_engine/</span></span><br><span class="line">  <span class="comment">// TODO complete</span></span><br><span class="line"></span><br><span class="line">  default_random_engine gen;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_particles; i++) &#123;</span><br><span class="line">    <span class="keyword">double</span> particle_x = particles[i].x;</span><br><span class="line">    <span class="keyword">double</span> particle_y = particles[i].y;</span><br><span class="line">    <span class="keyword">double</span> particle_theta = particles[i].theta;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span> pred_x;</span><br><span class="line">    <span class="keyword">double</span> pred_y;</span><br><span class="line">    <span class="keyword">double</span> pred_theta;</span><br><span class="line">    <span class="comment">// Instead of a hard check of 0, adding a check for very low value of</span></span><br><span class="line">    <span class="comment">// yaw_rate</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">fabs</span>(yaw_rate) &lt; <span class="number">0.0001</span>) &#123;</span><br><span class="line">      pred_x = particle_x + velocity * <span class="built_in">cos</span>(particle_theta) * <span class="keyword">delta_t</span>;</span><br><span class="line">      pred_y = particle_y + velocity * <span class="built_in">sin</span>(particle_theta) * <span class="keyword">delta_t</span>;</span><br><span class="line">      pred_theta = particle_theta;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      pred_x = particle_x + (velocity / yaw_rate) *</span><br><span class="line">                                (<span class="built_in">sin</span>(particle_theta + (yaw_rate * <span class="keyword">delta_t</span>)) -</span><br><span class="line">                                 <span class="built_in">sin</span>(particle_theta));</span><br><span class="line">      pred_y = particle_y + (velocity / yaw_rate) *</span><br><span class="line">                                (<span class="built_in">cos</span>(particle_theta) -</span><br><span class="line">                                 <span class="built_in">cos</span>(particle_theta + (yaw_rate * <span class="keyword">delta_t</span>)));</span><br><span class="line">      pred_theta = particle_theta + (yaw_rate * <span class="keyword">delta_t</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">normal_distribution&lt;<span class="keyword">double</span>&gt; <span class="title">dist_x</span><span class="params">(pred_x, std_pos[<span class="number">0</span>])</span></span>;</span><br><span class="line">    <span class="function">normal_distribution&lt;<span class="keyword">double</span>&gt; <span class="title">dist_y</span><span class="params">(pred_y, std_pos[<span class="number">1</span>])</span></span>;</span><br><span class="line">    <span class="function">normal_distribution&lt;<span class="keyword">double</span>&gt; <span class="title">dist_theta</span><span class="params">(pred_theta, std_pos[<span class="number">2</span>])</span></span>;</span><br><span class="line"></span><br><span class="line">    particles[i].x = dist_x(gen);</span><br><span class="line">    particles[i].y = dist_y(gen);</span><br><span class="line">    particles[i].theta = dist_theta(gen);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>3）Update Step</strong></p>
<p>在预测过程中，我们将车辆的速度和角速度合并到粒子滤波器中，在更新过程中，我们将激光雷达(Lidar)/Radar对于Landmark的测量结果集成到粒子滤波(Particle Filter)中，用于更新所有粒子的权重。</p>
<p>Update Step包含三个主要步骤: a）Transformation；b）Association；c) Update Weights。</p>
<p>Transformation(坐标变换)：Lidar/Radar对Landmark的测量都是相对于车辆自身坐标系统的，需要先转换到地图坐标系。传感器设备都安装在车上，所以传感器的测量结果都是以车辆为坐标原点，X轴沿车辆方向，Y轴沿垂直于X轴的左侧方向。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/robot_axes_definition.jpg"></p>
<p>假设粒子(Particle)的坐标为$(x_p, y_p)$，Landmark在车辆坐标系中的坐标为$(x_c, y_c)$，Landmark转换到地图(Map)坐标系的坐标为$(x_m, y_m)$，车辆的Heading为$theta$，则从车辆坐标系到地图坐标系的变换矩阵如下:</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/1_uR0dYxOKWHhEPUT6YMGXGg-1024x207.png"></p>
<p>写成非矩阵形式:</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/1_nB_6uUWjDKC-pGWToRTQDQ.png"></p>
<p>通过Transformation我们已经将观测值的坐标转换到地图的坐标空间，下一步是将每个转换后的观测值与Map中的LandMark相关联。Associations主要将LandMark的测量结果匹配到Map中的LandMark。</p>
<p>如下图所示，L1，L2，…，L5是地图(Map)中的Landmark；OBS1、OBS2、OBS3是车辆的Observation。红色方块是车辆的GroundTruth位置，蓝色方块是车辆的预测位置。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/0_l09z27QssoJvqHLj.png"></p>
<p>我们可以看到地图有5个LandMark，它们分别被标识为L1、L2、L3、L4、L5，并且每个LandMark都有已知的地图位置。我们需要将每个转换后的观测值TOBS1、TOBS2、TOBS3与这5个标识符中的一个相关联。其中一个直观的做法就是每个转换后的观测LandMark坐标与最近的Map LandMark相关联。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/1_y92MO5zzieuqxH5-uHD0YQ-1024x514.png"></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ParticleFilter::dataAssociation</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;LandmarkObs&gt; predicted,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;LandmarkObs&gt; &amp;observations,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">double</span> sensor_range)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Find the predicted measurement that is closest to each observed</span></span><br><span class="line">  <span class="comment">// measurement and assign the</span></span><br><span class="line">  <span class="comment">//   observed measurement to this particular landmark.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> this method will NOT be called by the grading code. But you will</span></span><br><span class="line">  <span class="comment">// probably find it useful to</span></span><br><span class="line">  <span class="comment">//   implement this method and use it as a helper during the updateWeights</span></span><br><span class="line">  <span class="comment">//   phase.</span></span><br><span class="line">  <span class="comment">// TODO complete</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/*Associate observations in map co-ordinates to predicted landmarks using</span></span><br><span class="line"><span class="comment">nearest neighbor algorithm. Here, the number of observations may be less than</span></span><br><span class="line"><span class="comment">the total number of landmarks as some of the landmarks may be outside the range</span></span><br><span class="line"><span class="comment">of vehicle&#x27;s sensor.*/</span></span><br><span class="line">  <span class="keyword">int</span> i, j;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; observations.size(); i++) &#123;</span><br><span class="line">    <span class="comment">// Maximum distance can be square root of 2 times the range of sensor.</span></span><br><span class="line">    <span class="keyword">double</span> lowest_dist = sensor_range * <span class="built_in">sqrt</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">int</span> closest_landmark_id = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">double</span> obs_x = observations[i].x;</span><br><span class="line">    <span class="keyword">double</span> obs_y = observations[i].y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; predicted.size(); j++) &#123;</span><br><span class="line">      <span class="keyword">double</span> pred_x = predicted[j].x;</span><br><span class="line">      <span class="keyword">double</span> pred_y = predicted[j].y;</span><br><span class="line">      <span class="keyword">int</span> pred_id = predicted[j].id;</span><br><span class="line">      <span class="keyword">double</span> current_dist = dist(obs_x, obs_y, pred_x, pred_y);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (current_dist &lt; lowest_dist) &#123;</span><br><span class="line">        lowest_dist = current_dist;</span><br><span class="line">        closest_landmark_id = pred_id;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    observations[i].id = closest_landmark_id;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Update Weights：完成观测LandMark坐标转换之后和地图匹配之后，就可以更新粒子的权重了。由于粒子(Particle)对所有LandMark的观测都是独立的，所以粒子的总权重是所有观测LandMark的多元高斯概率密度的乘积。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/1_C8vytZC_jE0unb2TSKRUGw.png"></p>
<p>多元高斯概率密度函数</p>
<p>其中:$\theta_x$和$\theta_y$是x方向和y方向上传感器测量结果的标准差，x和y是Observation LandMark的x和y坐标，$\mu_x$和$\mu_y$是Map LandMark的x和y坐标。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ParticleFilter::updateWeights</span><span class="params">(<span class="keyword">double</span> sensor_range, <span class="keyword">double</span> std_landmark[],</span></span></span><br><span class="line"><span class="function"><span class="params">                                   <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;LandmarkObs&gt; observations,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   Map map_landmarks)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Update the weights of each particle using a mult-variate Gaussian</span></span><br><span class="line">  <span class="comment">// distribution. You can read</span></span><br><span class="line">  <span class="comment">//   more about this distribution here:</span></span><br><span class="line">  <span class="comment">//   https://en.wikipedia.org/wiki/Multivariate_normal_distribution</span></span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> The observations are given in the VEHICLE&#x27;S coordinate system. Your</span></span><br><span class="line">  <span class="comment">// particles are located</span></span><br><span class="line">  <span class="comment">//   according to the MAP&#x27;S coordinate system. You will need to transform</span></span><br><span class="line">  <span class="comment">//   between the two systems. Keep in mind that this transformation requires</span></span><br><span class="line">  <span class="comment">//   both rotation AND translation (but no scaling). The following is a good</span></span><br><span class="line">  <span class="comment">//   resource for the theory:</span></span><br><span class="line">  <span class="comment">//   https://www.willamette.edu/~gorr/classes/GeneralGraphics/Transforms/transforms2d.htm</span></span><br><span class="line">  <span class="comment">//   and the following is a good resource for the actual equation to implement</span></span><br><span class="line">  <span class="comment">//   (look at equation 3.33 http://planning.cs.uiuc.edu/node99.html</span></span><br><span class="line">  <span class="comment">//   TODO complete</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i, j;</span><br><span class="line">  <span class="comment">/*This variable is used for normalizing weights of all particles and bring</span></span><br><span class="line"><span class="comment">    them in the range of [0, 1]*/</span></span><br><span class="line">  <span class="keyword">double</span> weight_normalizer = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; num_particles; i++) &#123;</span><br><span class="line">    <span class="keyword">double</span> particle_x = particles[i].x;</span><br><span class="line">    <span class="keyword">double</span> particle_y = particles[i].y;</span><br><span class="line">    <span class="keyword">double</span> particle_theta = particles[i].theta;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*Step 1: Transform observations from vehicle co-ordinates to map</span></span><br><span class="line"><span class="comment">     * co-ordinates.*/</span></span><br><span class="line">    <span class="comment">// Vector containing observations transformed to map co-ordinates w.r.t.</span></span><br><span class="line">    <span class="comment">// current particle.</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;LandmarkObs&gt; transformed_observations;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Transform observations from vehicle&#x27;s co-ordinates to map co-ordinates.</span></span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; observations.size(); j++) &#123;</span><br><span class="line">      LandmarkObs transformed_obs;</span><br><span class="line">      transformed_obs.id = j;</span><br><span class="line">      transformed_obs.x = particle_x +</span><br><span class="line">                          (<span class="built_in">cos</span>(particle_theta) * observations[j].x) -</span><br><span class="line">                          (<span class="built_in">sin</span>(particle_theta) * observations[j].y);</span><br><span class="line">      transformed_obs.y = particle_y +</span><br><span class="line">                          (<span class="built_in">sin</span>(particle_theta) * observations[j].x) +</span><br><span class="line">                          (<span class="built_in">cos</span>(particle_theta) * observations[j].y);</span><br><span class="line">      transformed_observations.push_back(transformed_obs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*Step 2: Filter map landmarks to keep only those which are in the</span></span><br><span class="line"><span class="comment">     sensor_range of current particle. Push them to predictions vector.*/</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;LandmarkObs&gt; predicted_landmarks;</span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; map_landmarks.landmark_list.size(); j++) &#123;</span><br><span class="line">      Map::single_landmark_s current_landmark = map_landmarks.landmark_list[j];</span><br><span class="line">      <span class="keyword">if</span> ((<span class="built_in">fabs</span>((particle_x - current_landmark.x_f)) &lt;= sensor_range) &amp;&amp;</span><br><span class="line">          (<span class="built_in">fabs</span>((particle_y - current_landmark.y_f)) &lt;= sensor_range)) &#123;</span><br><span class="line">        predicted_landmarks.push_back(LandmarkObs&#123;</span><br><span class="line">            current_landmark.id_i, current_landmark.x_f, current_landmark.y_f&#125;);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*Step 3: Associate observations to lpredicted andmarks using nearest</span></span><br><span class="line"><span class="comment">     * neighbor algorithm.*/</span></span><br><span class="line">    <span class="comment">// Associate observations with predicted landmarks</span></span><br><span class="line">    dataAssociation(predicted_landmarks, transformed_observations,</span><br><span class="line">                    sensor_range);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*Step 4: Calculate the weight of each particle using Multivariate Gaussian</span></span><br><span class="line"><span class="comment">     * distribution.*/</span></span><br><span class="line">    <span class="comment">// Reset the weight of particle to 1.0</span></span><br><span class="line">    particles[i].weight = <span class="number">1.0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span> sigma_x = std_landmark[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">double</span> sigma_y = std_landmark[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">double</span> sigma_x_2 = <span class="built_in">pow</span>(sigma_x, <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">double</span> sigma_y_2 = <span class="built_in">pow</span>(sigma_y, <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">double</span> normalizer = (<span class="number">1.0</span> / (<span class="number">2.0</span> * M_PI * sigma_x * sigma_y));</span><br><span class="line">    <span class="keyword">int</span> k, l;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*Calculate the weight of particle based on the multivariate Gaussian</span></span><br><span class="line"><span class="comment">     * probability function*/</span></span><br><span class="line">    <span class="keyword">for</span> (k = <span class="number">0</span>; k &lt; transformed_observations.size(); k++) &#123;</span><br><span class="line">      <span class="keyword">double</span> trans_obs_x = transformed_observations[k].x;</span><br><span class="line">      <span class="keyword">double</span> trans_obs_y = transformed_observations[k].y;</span><br><span class="line">      <span class="keyword">double</span> trans_obs_id = transformed_observations[k].id;</span><br><span class="line">      <span class="keyword">double</span> multi_prob = <span class="number">1.0</span>;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (l = <span class="number">0</span>; l &lt; predicted_landmarks.size(); l++) &#123;</span><br><span class="line">        <span class="keyword">double</span> pred_landmark_x = predicted_landmarks[l].x;</span><br><span class="line">        <span class="keyword">double</span> pred_landmark_y = predicted_landmarks[l].y;</span><br><span class="line">        <span class="keyword">double</span> pred_landmark_id = predicted_landmarks[l].id;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (trans_obs_id == pred_landmark_id) &#123;</span><br><span class="line">          multi_prob = normalizer *</span><br><span class="line">                       <span class="built_in">exp</span>(<span class="number">-1.0</span> * ((<span class="built_in">pow</span>((trans_obs_x - pred_landmark_x), <span class="number">2</span>) /</span><br><span class="line">                                    (<span class="number">2.0</span> * sigma_x_2)) +</span><br><span class="line">                                   (<span class="built_in">pow</span>((trans_obs_y - pred_landmark_y), <span class="number">2</span>) /</span><br><span class="line">                                    (<span class="number">2.0</span> * sigma_y_2))));</span><br><span class="line">          particles[i].weight *= multi_prob;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    weight_normalizer += particles[i].weight;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*Step 5: Normalize the weights of all particles since resmapling using</span></span><br><span class="line"><span class="comment">   * probabilistic approach.*/</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; particles.size(); i++) &#123;</span><br><span class="line">    particles[i].weight /= weight_normalizer;</span><br><span class="line">    weights[i] = particles[i].weight;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>4）Resample Step</strong></p>
<p>重采样(Resample)是从旧粒子(Old Particles)中随机抽取新粒子(New Particles)并根据其权重进行替换的过程。重采样后，具有更高权重的粒子保留的概率越大，概率越小的粒子消亡的概率越大。</p>
<p>常用的Resample技术是Resampling Wheel。Resampling Wheel的算法思想如下：如下图所示的圆环，圆环的弧长正比与它的权重(即权重越大，弧长越长)。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/Screenshot-from-2019-12-22-09-06-34.png"></p>
<p>下一步就是利用类似于轮盘赌(Roulette Wheel)的方式，将圆环安装固定步长(Resampling Wheel中将圆环切分为2*max(weights))进行等分，选定一个方向进行n次随机旋转，每次旋转获得一个位置，这个位置就是被选中的粒子的索引。</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/Screenshot-from-2019-12-22-09-52-27.png"></p>
<p>搬运Youtube上的Udacity关于Resampling Wheel视频如下:</p>
<p>来源:<a href="https://www.youtube.com/watch?v=wNQVo6uOgYA">https://www.youtube.com/watch?v=wNQVo6uOgYA</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">p3 &#x3D; []</span><br><span class="line">index &#x3D; int(random.random()*N)</span><br><span class="line">beta &#x3D; 0.0</span><br><span class="line">mw &#x3D; max(w)</span><br><span class="line">for i in range(N):</span><br><span class="line">  beta +&#x3D; random.random()*2*mw</span><br><span class="line">  while w[index] &lt; beta:</span><br><span class="line">     beta &#x3D; beta - w[index]</span><br><span class="line">     index &#x3D; index + 1</span><br><span class="line"></span><br><span class="line">  p3.append(p[index]) </span><br></pre></td></tr></table></figure>

<p>Resample的C++代码如下</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ParticleFilter::resample</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Resample particles with replacement with probability proportional to</span></span><br><span class="line">  <span class="comment">// their weight. <span class="doctag">NOTE:</span> You may find std::discrete_distribution helpful here.</span></span><br><span class="line">  <span class="comment">//   http://en.cppreference.com/w/cpp/numeric/random/discrete_distribution</span></span><br><span class="line">  <span class="comment">//   TODO complete</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">vector</span>&lt;Particle&gt; resampled_particles;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a generator to be used for generating random particle index and beta</span></span><br><span class="line">  <span class="comment">// value</span></span><br><span class="line">  default_random_engine gen;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Generate random particle index</span></span><br><span class="line">  <span class="function">uniform_int_distribution&lt;<span class="keyword">int</span>&gt; <span class="title">particle_index</span><span class="params">(<span class="number">0</span>, num_particles - <span class="number">1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> current_index = particle_index(gen);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span> beta = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span> max_weight_2 = <span class="number">2.0</span> * *max_element(weights.begin(), weights.end());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; particles.size(); i++) &#123;</span><br><span class="line">    <span class="function">uniform_real_distribution&lt;<span class="keyword">double</span>&gt; <span class="title">random_weight</span><span class="params">(<span class="number">0.0</span>, max_weight_2)</span></span>;</span><br><span class="line">    beta += random_weight(gen);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (beta &gt; weights[current_index]) &#123;</span><br><span class="line">      beta -= weights[current_index];</span><br><span class="line">      current_index = (current_index + <span class="number">1</span>) % num_particles;</span><br><span class="line">    &#125;</span><br><span class="line">    resampled_particles.push_back(particles[current_index]);</span><br><span class="line">  &#125;</span><br><span class="line">  particles = resampled_particles;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最后把这些代码串起来，实现车辆定位的功能。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Set up parameters here</span></span><br><span class="line"><span class="keyword">double</span> <span class="keyword">delta_t</span> = <span class="number">0.1</span>;     <span class="comment">// Time elapsed between measurements [sec]</span></span><br><span class="line"><span class="keyword">double</span> sensor_range = <span class="number">50</span>; <span class="comment">// Sensor range [m]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> sigma_pos[<span class="number">3</span>] = &#123;</span><br><span class="line">    <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.01</span>&#125;; <span class="comment">// GPS measurement uncertainty [x [m], y [m], theta [rad]]</span></span><br><span class="line"><span class="keyword">double</span> sigma_landmark[<span class="number">2</span>] = &#123;</span><br><span class="line">    <span class="number">0.3</span>, <span class="number">0.3</span>&#125;; <span class="comment">// Landmark measurement uncertainty [x [m], y [m]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Read map data</span></span><br><span class="line">Map <span class="built_in">map</span>;</span><br><span class="line"><span class="keyword">if</span> (!read_map_data(<span class="string">&quot;../data/map_data.txt&quot;</span>, <span class="built_in">map</span>)) &#123;</span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Error: Could not open map file&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create particle filter</span></span><br><span class="line">ParticleFilter pf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (循环接收车辆的控制信息和Sense信息) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!pf.initialized()) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Sense noisy position data from the simulator</span></span><br><span class="line">    <span class="keyword">double</span> sense_x = <span class="built_in">std</span>::stod(j[<span class="number">1</span>][<span class="string">&quot;sense_x&quot;</span>].get&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt;());</span><br><span class="line">    <span class="keyword">double</span> sense_y = <span class="built_in">std</span>::stod(j[<span class="number">1</span>][<span class="string">&quot;sense_y&quot;</span>].get&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt;());</span><br><span class="line">    <span class="keyword">double</span> sense_theta = <span class="built_in">std</span>::stod(j[<span class="number">1</span>][<span class="string">&quot;sense_theta&quot;</span>].get&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt;());</span><br><span class="line"></span><br><span class="line">    pf.init(sense_x, sense_y, sense_theta, sigma_pos);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Predict the vehicle&#x27;s next state from previous (noiseless</span></span><br><span class="line">    <span class="comment">// control) data.</span></span><br><span class="line">    <span class="keyword">double</span> previous_velocity =</span><br><span class="line">        <span class="built_in">std</span>::stod(j[<span class="number">1</span>][<span class="string">&quot;previous_velocity&quot;</span>].get&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt;());</span><br><span class="line">    <span class="keyword">double</span> previous_yawrate =</span><br><span class="line">        <span class="built_in">std</span>::stod(j[<span class="number">1</span>][<span class="string">&quot;previous_yawrate&quot;</span>].get&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt;());</span><br><span class="line"></span><br><span class="line">    pf.prediction(<span class="keyword">delta_t</span>, sigma_pos, previous_velocity, previous_yawrate);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// receive noisy observation data from the simulator</span></span><br><span class="line">  <span class="comment">// sense_observations in JSON format</span></span><br><span class="line">  <span class="comment">// [&#123;obs_x,obs_y&#125;,&#123;obs_x,obs_y&#125;,...&#123;obs_x,obs_y&#125;]</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;LandmarkObs&gt; noisy_observations;</span><br><span class="line">  <span class="built_in">string</span> sense_observations_x = j[<span class="number">1</span>][<span class="string">&quot;sense_observations_x&quot;</span>];</span><br><span class="line">  <span class="built_in">string</span> sense_observations_y = j[<span class="number">1</span>][<span class="string">&quot;sense_observations_y&quot;</span>];</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; x_sense;</span><br><span class="line">  <span class="function"><span class="built_in">std</span>::<span class="built_in">istringstream</span> <span class="title">iss_x</span><span class="params">(sense_observations_x)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::copy(<span class="built_in">std</span>::istream_iterator&lt;<span class="keyword">float</span>&gt;(iss_x), <span class="built_in">std</span>::istream_iterator&lt;<span class="keyword">float</span>&gt;(),</span><br><span class="line">            <span class="built_in">std</span>::back_inserter(x_sense));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; y_sense;</span><br><span class="line">  <span class="function"><span class="built_in">std</span>::<span class="built_in">istringstream</span> <span class="title">iss_y</span><span class="params">(sense_observations_y)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::copy(<span class="built_in">std</span>::istream_iterator&lt;<span class="keyword">float</span>&gt;(iss_y), <span class="built_in">std</span>::istream_iterator&lt;<span class="keyword">float</span>&gt;(),</span><br><span class="line">            <span class="built_in">std</span>::back_inserter(y_sense));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; x_sense.size(); i++) &#123;</span><br><span class="line">    LandmarkObs obs;</span><br><span class="line">    obs.x = x_sense[i];</span><br><span class="line">    obs.y = y_sense[i];</span><br><span class="line">    noisy_observations.push_back(obs);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Update the weights and resample</span></span><br><span class="line">  pf.updateWeights(sensor_range, sigma_landmark, noisy_observations, <span class="built_in">map</span>);</span><br><span class="line">  pf.resample();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Calculate and output the average weighted error of the particle</span></span><br><span class="line">  <span class="comment">// filter over all time steps so far.</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;Particle&gt; particles = pf.particles;</span><br><span class="line">  <span class="keyword">int</span> num_particles = particles.size();</span><br><span class="line">  <span class="keyword">double</span> highest_weight = <span class="number">-1.0</span>;</span><br><span class="line">  Particle best_particle;</span><br><span class="line">  <span class="keyword">double</span> weight_sum = <span class="number">0.0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_particles; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (particles[i].weight &gt; highest_weight) &#123;</span><br><span class="line">      highest_weight = particles[i].weight;</span><br><span class="line">      best_particle = particles[i];</span><br><span class="line">    &#125;</span><br><span class="line">    weight_sum += particles[i].weight;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Udacity Simulator上的演示效果如下:</p>
<p><img src="/2019/12/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E5%8D%81%E4%B8%89-%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2particle-filter/particle_filter.gif"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1.<a href="http://www.deepideas.net/robot-localization-histogram-filter/">http://www.deepideas.net/robot-localization-histogram-filter/</a><br>2.<a href="https://github.com/sohonisaurabh/CarND-Kidnapped-Vehicle-Project">https://github.com/sohonisaurabh/CarND-Kidnapped-Vehicle-Project</a><br>3.<a href="https://medium.com/intro-to-artificial-intelligence/kidnapped-vehicle-project-using-particle-filters-udacitys-self-driving-car-nanodegree-aa1d37c40d49">https://medium.com/intro-to-artificial-intelligence/kidnapped-vehicle-project-using-particle-filters-udacitys-self-driving-car-nanodegree-aa1d37c40d49</a></p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>多传感器融合</tag>
        <tag>自动驾驶定位</tag>
        <tag>Particle Filter</tag>
        <tag>粒子滤波</tag>
      </tags>
  </entry>
  <entry>
    <title>自动驾驶高精地图-概述与分析</title>
    <url>/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h1 id="HD-Maps-Structure-Functionalities-Accuracy-And-Standards"><a href="#HD-Maps-Structure-Functionalities-Accuracy-And-Standards" class="headerlink" title="HD Maps: Structure, Functionalities, Accuracy And Standards"></a>HD Maps: Structure, Functionalities, Accuracy And Standards</h1><p>在自动驾驶系统的功能系统架构中，高精地图与定位功能紧密相关，与感知模块交互，并最终支持规划和控制模块。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8A%E5%8D%8810.59.57-1024x434.png" alt="Functional system architecture of an automated driving system."></p>
<p>不同级别的自动驾驶对地图的精细程度要求不同。高精地图不仅存储Road和Lane等详细信息，而且还存储Landmark信息和周围环境信息用于辅助车辆定位；车辆定位信息和高精地图信息又作为Perception模块的输入，实现为物理环境建模的目的；Planning&amp;Control模块依据Perception模块输出的环境模型规划和控制车辆的驾驶行为；同时Perception的建模信息又可以作为高精地图的输入，实现对高精地图的及时更新。</p>
<span id="more"></span>

<h2 id="HD-Map-Structure"><a href="#HD-Map-Structure" class="headerlink" title="HD Map Structure"></a>HD Map Structure</h2><p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8A%E5%8D%8811.21.29-1024x162.png" alt="Examples of layered structure of an HD Map"></p>
<p>如上图所示，虽然叫法不同，大部分地图厂商的高精地图都基本分为三层:Road Model, Lane Model 和Localization Model。</p>
<h2 id="HD-Map-Functionalities"><a href="#HD-Map-Functionalities" class="headerlink" title="HD Map Functionalities"></a>HD Map Functionalities</h2><p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8A%E5%8D%8811.26.32-1024x483.png" alt="functionality of an HD Map. (a) Road Model supports navigation; (b) Localisation Model enables perception using Lane Model: the ego vehicle understands the presence of lane markings and an obstacle; (c) Lane Model supports tactical planning, for example, lane-changing manoeuvre."></p>
<p>如上图所示，Road Model用于导航规划；Lane Model用于感知和考虑当前道路和交通状况的路线规划；Localization Model用于在地图中定位车辆，只有当车辆在地图中准确定位时，Lane Model才能帮助车辆感知。</p>
<h2 id="Map-Accuracy"><a href="#Map-Accuracy" class="headerlink" title="Map Accuracy"></a>Map Accuracy</h2><p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8A%E5%8D%8811.37.11-1024x384.png" alt="Relative accuracy defined in EDMap (2004). (a) Incorrect relative accuracies: &lt; 20 cm (orange) and &gt; 20 cm (red) respectively; (b) Correct relative accuracies: &lt; 20cm (orange) and &gt; 20cm (red) respectively."></p>
<p>绝对精度(Absolute Accuracy)：Map Geometry偏移Ground Truth Geometry的最大空间距离。</p>
<p>相对精度(Relative Accuracy)： 首先Align Map Geometry和Ground Truth Geometry，然后计算二者之间的最大空间距离。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8A%E5%8D%8811.47.29-1024x277.png" alt="The map accuracy parameters set by some notable map providers"></p>
<h2 id="HD-Map-Related-Standards"><a href="#HD-Map-Related-Standards" class="headerlink" title="HD Map Related Standards"></a>HD Map Related Standards</h2><p>ISO/TC 204 发布的Geographic Data File (GDF) 提供了一个用于地图信息存储和交换的基础版本。GDF 5.1将支持自动驾驶，并于2018年发布了Local Dynamic Map (LDM)标准，存储在LDM的信息包含天气、道路、交通状况、静态信息等。</p>
<p>许多政府和工业团体都在积极推动自动驾驶地图的标准化，比如，Open AutoDrive Forum (OADF) (openauto-drive.org) 作为cross-domain平台推动自动驾驶标准化。Traveller Information Services Association (TISA) (tisa.org) 正在讨论将Transport Protocol Experts Group (TPEGTM) 的交通信息精度提升到车道级别。Advanced Driver Assistance Systems Interface Specification (ADASIS) Forum (adasis.org) 发布ADASIS protocol V3以支持车内高精地图分发。Sensoris (sensor-is.org) 正在致力于基于车辆的传感器数据交换格式的标准化，以及车辆到云和云到云接口的研发。Navigation Data Standard (NDS, 2016) 和 OpenDRIVE(OpenDRIVE, 2015)是两大高精地图工业级标准，它们与GDF的对比如下:</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8A%E5%8D%8811.50.19-1024x653.png"></p>
<p>还有一些地区和国家也在研究自动驾驶的地图标准。日本在研究Dynamic Map(en.sip- adus.go.jp)；2018年5月，中国智能与互联汽车产业创新联盟（CAICV）自动驾驶地图工作组（caicv.org.cn）正式成立，该工作组的愿景是实现中国自动驾驶和高清地图的标准化。</p>
<h1 id="HD-Map-Models"><a href="#HD-Map-Models" class="headerlink" title="HD Map Models"></a>HD Map Models</h1><p>HD Map Model包含Road Model, Lane Model和Localization Model。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8B%E5%8D%8812.23.46-1024x953.png" alt="Road Model (green polylines and yellow nodes) on top of Lane Model."></p>
<h2 id="Road-Model"><a href="#Road-Model" class="headerlink" title="Road Model"></a>Road Model</h2><p>Road Model使用有序的形状点序列组成的线段表示，每个Road Section都包含Start Nodes和End Nodes。使用形状点序列定义Curved Road Geometry，优点是表达简单，增加中间点的密度即可以获得更好的精度，但缺点是需要存储大量的信息，Road Model的道路属性和丰富的语义信息可以为自动驾驶带来更多的先验知识。</p>
<p>此外，相对于传统的2D导航地图，高精地图的Road Model增加了高度信息，对于精度的要求也更高。不同地图提出的用于ADAS的元素级精度要求如下表:</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8B%E5%8D%8812.40.17-1024x482.png" alt="Examples of map content for ADAS and accuracy requirements"></p>
<h2 id="Lane-Model"><a href="#Lane-Model" class="headerlink" title="Lane Model"></a>Lane Model</h2><p>最著名的车道地图是DARPA城市挑战赛使用的Road Network Description File (RDNF)，但是RDNF是2D地图，并且也非常粗糙。</p>
<p>Bertha Drive项目使用的基于Lanelet的三维车道模型中，车道不仅包含高度精确的左右边界的可行驶区域，而且车道行驶所需要的交通规则信息。</p>
<p>通常情况下，Lane Model包含如下信息：</p>
<h3 id="Highly-accurate-geometry-model"><a href="#Highly-accurate-geometry-model" class="headerlink" title="Highly accurate geometry model"></a>Highly accurate geometry model</h3><p>Lane Geometry Mode在很大程度上决定了Lane Model的准确性、存储效率和可用性，该模型不仅涵盖车道中心线、车道边界和RoadMarkings，还应包含潜在的三维道路结构，如斜坡和立交桥等。此外，它还应该具备辅助车辆高效计算的能力。</p>
<h3 id="Lane-attributes"><a href="#Lane-attributes" class="headerlink" title="Lane attributes"></a>Lane attributes</h3><p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8B%E5%8D%8812.44.01-1024x274.png"></p>
<p>Lane Attribute主要包括车道中心线(沿车道中间的理论线)和具有不同形状、颜色和材质的车道边界。</p>
<h3 id="Traffic-Regulations-Road-Furniture-And-Parking"><a href="#Traffic-Regulations-Road-Furniture-And-Parking" class="headerlink" title="Traffic Regulations, Road Furniture And Parking"></a>Traffic Regulations, Road Furniture And Parking</h3><p>交通规则和相关信息/参数可以嵌入到其他属性中，例如车道的道路类型可以隐式地指示道路的默认限速。同时Lane Model也应具备分段车道属性的表达能力。与车道关联的停车位和道路设施也需要高度精确的位置信息。</p>
<h2 id="Lane-connectivity"><a href="#Lane-connectivity" class="headerlink" title="Lane connectivity"></a>Lane connectivity</h2><p>Lane connectivity描述了车道或车道的连接关系。交叉路口的拓扑和语义方面由Traffic Matrices处理，Traffic Matrices定义了符合交通规则的所有操作。</p>
<p>车道连通性几何信息通过连接出入口控制点的Virtual Lane表示，Virtual Lane可以使用与普通车道相同的几何车道模型(NDS, 2016)或者采用完全不同的车道模型表示方法(因为两个固定的控制点会影响车道曲线的连续性)。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8B%E5%8D%881.12.03.png" alt="An intersection with entry (blue dots) and exit (red dots) control points and the centrelines of virtual lanes (blue dashed arrow)."></p>
<p>下表列举了各个地图供应商和标准化组织提供的Lane Model内容和精度要求。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-20-%E4%B8%8B%E5%8D%881.21.23-1024x568.png" alt="Examples of Lane Model content and accuracy requirements"></p>
<h2 id="Localisation-Model"><a href="#Localisation-Model" class="headerlink" title="Localisation Model"></a>Localisation Model</h2><p>Localisation Model用于辅助车辆定位。根据所使用传感器方案的不同，定位技术分为Feature Based的方法和Dense Information Based的方法。</p>
<p><strong>Feature-based Localisation Model</strong></p>
<p>Feature-Based Localisation Model通常被存储为Graph，每一个Graph Node包含Image和3D Landmark，连接Graph Node的Edge是车辆Pose。Landmark通常使用特征描述子(Feature Descriptor)表达，以方便在Live Image和Map进行Feature匹配。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-15-%E4%B8%8B%E5%8D%881.16.52-1024x340.png" alt="Localisation Model examples. (a) Landmark map with green landmarks and orange vehicle pose; (b) Road marking map with blue line segments"></p>
<p>Feature Map易用高效，但是制作Offline Map和Online Localization都需要进行Feature Extraction，过程繁琐复杂。</p>
<p><strong>Dense Information-Based Localisation Model</strong></p>
<p>Dense Information-Based Localisation Model可以进一步分为Location Based(Grid Map)或View Based(如Point Cloud)。毫米波雷达和RGB-D相机也可以用于收集Dense Information进行制图，但LiDAR是目前测绘和自动驾驶公司(HERE、TomTom、Google等)最广泛使用的采集设备。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-15-%E4%B8%8B%E5%8D%881.45.18-1024x323.png" alt="Examples of 2D grid map formats. (a) Reflectivity grid map; (b) Occupancy grid map (colour denotes the distance to road centre) ."></p>
<p>2D Grid Map探索X-Y平面(地面)和X-Z平面(垂直于地面)，如上左图所示，假设道路表面平坦，每一个Grid Cell被激光雷达的平均反射值填充，或者由激光雷达反射值的高斯分布填充，两种定位方法的相对精度均在10厘米量级。另一种方法是使用Occupancy Grid Map，如上右图所示，每一个Grid Cell被由距道路中心距离和网格被占用概率联合定义的累积概率填充，通过该手段将道路的3D信息压缩为2D栅格地图，从而达到40cm以内的绝对定位精度。2D Grid Map的缺点在于对于环境变化不具鲁棒性。</p>
<p>2.5D地图是包含高度信息的2D Map。Wolcott和Eustice(2014)在2D X-Y Reflectivity Grid Map中添加Z信息，以描绘道路的高度变化；Morales等人(2010)将Estimated Height添加到2D Map中心线地图中，以在室外林地环境中实现定位；此外，Wolcott和Eustice(2015)提出了对Z高度和反射率值进行建模，以捕获结构和外观变化，并使用多分辨率搜索进行优化。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-15-%E4%B8%8B%E5%8D%883.18.52-1024x286.png" alt="continued effort of improvement。a) 2D reflectivity grid map；b) 2D probabilistic grid map;  
(c) 2D probabilistic grid map with height attribute; (d) 2·5D grid map. Only the i-th cell is annotated. Note: i= the index of the cell; z= height; r= reflectivity value; μ = mean; G = Gaussian distribution."></p>
<p>3D点云地图通常由latitude、longitude、altitude、ntensity信息组成的点序列组成，并且可以用Camera Data赋予点云数据纹理。3D点云地图随着面积的增加，其内存占用也呈现指数级增长，这给实际使用带来很多问题。</p>
<p>定位模型或高精地图面临的挑战之一是如何反映环境中的结构、季节或光照条件变化。Churchill和New-man(2013) 提出了考虑“在不同的时间、不同的天气和光照条件”场景差异的方案，Maddern等人(2015)提出的3D PointCloud Map也采用了这种Experience-Based Navigation的方法。Irie(2010)尝试将Grid Map和Feature整合到一个Map中，以提高对光照变化的鲁棒性。在未来，实时地图将是地图的终极目标。</p>
<h1 id="HD-Map-Mapping"><a href="#HD-Map-Mapping" class="headerlink" title="HD Map Mapping"></a>HD Map Mapping</h1><p>HDMap Mapping的主要方法是收集车辆的传感器数据(GNSS Receiver、IMU、Camera、Lidar、Wheel Odometry等)，然后依赖于Mobile Mapping System (MMS) 和 Simultaneous Localisation and Mapping (SLAM)等方法完成地图制作。</p>
<p>NMS依赖于GNSS/IMU进行Pose Estimation，地图采集车的所有传感器数据都包含精确的时间戳，在用于制图之前，需要对这些数据进行离线处理和校准，比如，通过合并同一场景的包含地理信息的图像来创建Dense Localisation Model，从包含地理信息的图像中提取Feature用于车道建模也是热门的研究领域。另外一个研究课题是如何用解析方程来拟合道路和车道曲线，Betaille等人(2010)提出clothoids曲线来拟合实际道路曲线； Gwon等人(2016)提出使用样条曲线拟合实际道路曲线，样条曲线的每一个曲线段为三次多项式。</p>
<p>SLAM最初是针对没有GNSS的环境开发的，其目的是利用Ego Motion Measurements 和Loop Closures建立一个全球一致的Environment Representation ，该Representation提供的Environment Topology和Metric Representation可以辅助进行自动驾驶定位。</p>
<p>MMS和SLAM都有自己的挑战。对于NMS而言，挑战在于GNSS在城市区域的可用性和准确性相对较低；对于SLAM而言，挑战在于随着采集区域的增加，计算成本和计算复杂度爆炸性增长。这些原因催生了对SLAM和GNSS/IMU相互集成的研究，通过增加其他信息来源，如可公开获取的航空影像和数字地图，而不是仅仅使用Loop Closure作为减小SLAM不确定性的唯一来源。</p>
<p>HDMap Mapping面临的挑战之一是实现全自动化，目前的方案是引入机器学习的方法从图像数据中提取道路网络语义信息；另一个挑战是实现大规模制图，解决方案是数据众包，利用来自同一条道路的车辆的传感器数据用于改进更新现有地图或制作新地图。</p>
<p>如果政府在规划、施工遵守标准施工方案，那么从设计和施工文件中获取HDMap信息也是一种潜在可行的方法。</p>
<h1 id="Vehicle-Localization-With-HD-Map-And-Numerical-Analysis"><a href="#Vehicle-Localization-With-HD-Map-And-Numerical-Analysis" class="headerlink" title="Vehicle Localization With HD Map And Numerical Analysis"></a>Vehicle Localization With HD Map And Numerical Analysis</h1><h2 id="Map-Relative-Localisation"><a href="#Map-Relative-Localisation" class="headerlink" title="Map Relative Localisation"></a>Map Relative Localisation</h2><p>Map Relative Localization是解决主车相对于HDMap的位置估计问题。自动驾驶要求非常精确的六自由度(DOF)定位，这对于定义主车视野(Field Of View，FOV)以有效利用车道模型(Lane Model)进行感知至关重要。Levinson和Thrun(2010)研究认为厘米级定位精度对于任何开放道路的自动驾驶都足够使用。自动驾驶车辆还需要较高的位置更新频率：对于某些系统，更新频率高达200HZ（Levinson等人，2007年）；10HZ，限速63公里/小时（Levinson和Thrun，2010年），20HZ，时速60公里（Cui等人，2014年）。</p>
<p>高精地图(HDMap)提供已知的环境信息，然后使用环境感知传感器感知的信息用于定位。通过将从实时图像或扫描中检测到的Feature注册到定位模型中，相对于地图的六自由度(DOF)车辆姿态估计可以达到10-20厘米精度，有了定位信息，就可以将车道模型(Lane Mode)引入到感知任务中，从而将复杂的静态环境感知问题转化为定位问题。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-16-%E4%B8%8A%E5%8D%888.30.57-1024x657.png" alt="Feature-based and appearance-based vehicle localisation flow chart."></p>
<p>上图显示了地图相对定位的一般流程。Appearance Based的方法避开了Feature Detection的步骤。“Data Association”将环境传感器测量信息与地图关联起来。当使用贝叶斯状态估计器时，如Kalman Filter(KF)以及其变种(Extended、Unscented等)、Particle Filter (PF)等，Pose Estimation过程对应于State Estimation。</p>
<p>使用KF时，Data Association对于Pose Estimation是必不可少的；当使用PF时它可以简化，但是可以分别考虑每个粒子。Map Relative Localisation使用的Data Association方法包括基于描述子的Feature Matching、Direct Points Comparison、Point Set Registration(Iterative Closest Point，ICP)；Appearance-based Matching使用Normalised Mutual Information、Maximum Likelihood Estimate、Normalised Information Distance、ICP和 Normal Distribution Transformation (NDT)进行Direct Registration。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-16-%E4%B8%8B%E5%8D%8811.42.16-1024x344.png" alt="(a) The 3D point cloud-based HD Map; (b) A section of the road in the map and one matched scan (in red)."></p>
<h2 id="Numerical-analysis"><a href="#Numerical-analysis" class="headerlink" title="Numerical analysis"></a>Numerical analysis</h2><p>本节分析使用NDT作为Data Association和主车Pose Estimation的HDMap-Based Localisation，并比较了使用GNSS和ICP的Matching效果。</p>
<p>Input Scan: $S={x_i}(i=1…N_s)$，Map:$M={y_i}(i=1…N_m)$，Scan和Map匹配的过程称为Scan Matching或者Registration，匹配的结果是Vehicle相对于Map的六自由度刚体变换参数:$p=(\psi, \theta, \phi, t_x, t_y, t_z)$。</p>
<p>ICP和NDT是两个广泛使用的Registration方法。ICP将Registration任务看做Source和Target的几何要素(点、线、面)的对应问题。NDT可以避免建立这种对应关系。</p>
<p>本节研究使用Autoware的真实数据集，从数据集中提取了大约3000个Scan作为Input Scan。Scan采用0.5m Voxel Grid进行下采样，然后分别使用NDT(Cell Size=0.5m)和ICP与Autoware的3D Point Cloud Map进行Match。</p>
<p>记初始Pose为$p_0$、Input Scan为S，地图为M，NDT过程分为如下两步：</p>
<p><strong>Step 1. Build NDT map</strong></p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-18-%E4%B8%8A%E5%8D%888.33.40-1024x510.png"></p>
<p>如上图所示，第一步是将Point Cloud分散到预先定义的固定大小的立方体${\beta_i}，=1,…,m$。假设任意一个立方体$\beta$包含的点集${z_k},k=1,…,n$。Mean Vector $\mu$和Covariance Matrix $\sum$定义如下:</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-18-%E4%B8%8A%E5%8D%888.45.43.png"></p>
<p>对于每一个$\beta_i$进行3D正态分布建模:$N(\mu, \sum)$，它的概率密度函数如下:</p>
<p>$prob(x)=\frac{1}{c}exp(-\frac{(x-\mu)^T \sum^{-1} (x-\mu)}{2})$</p>
<p><strong>Step 2. Align the input scan to the map.</strong></p>
<p>初始Pose:$p=p_0$，采用牛顿法迭代优化姿态p。</p>
<p>1.采用姿态p对Input Scan进行变换。如下，其中T是Transformation Function。</p>
<p>$x_i^{\prime} = T(p, x_i)$</p>
<p>将$x_i^{\prime}$映射到Map Cell，通过概率密度函数计算Score:</p>
<p>$s(p) = -\sum_{i=1}^{N_s} prob(T(p, x_i))$</p>
<p>2. 计算$\Delta p$</p>
<p>$H \Delta p = -g$</p>
<p>其中H是Hessian矩阵，g是梯度矩阵；</p>
<p>3. 更新姿态p</p>
<p>$p = p + \Delta p$</p>
<p>持续上述过程，直至$\Delta p$小于预定阈值。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-19-%E4%B8%8A%E5%8D%888.34.18-1024x337.png" alt="Comparing the coordinates from NDT, ICP and GNSS RTK (m)"></p>
<p>如上图，NDT、ICP、GNSS RTK的RMS Difference都在10cm以内。</p>
<p><img src="/2019/09/20/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E9%AB%98%E7%B2%BE%E5%9C%B0%E5%9B%BE-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%88%86%E6%9E%90/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-09-19-%E4%B8%8A%E5%8D%888.38.21-1024x420.png" alt="Comparison the trajectories between: (a) NDT and GNSS; (b) NDT and ICP."></p>
<p>如上图所示，NDT、ICP、GNSS RTK的水平坐标差异大多数都在$2\sigma$范围内(&lt;20cm)，但是也有一些超出了$4\sigma$范围，因此要应用在自动驾驶领域，需要对定位输出做严格的质量控制。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接:"></a>参考链接:</h1><p>Paper: High Definition Map for Automated Driving: Overview and Analysis</p>
<h1 id="该文章为翻译文章，原文出处："><a href="#该文章为翻译文章，原文出处：" class="headerlink" title="该文章为翻译文章，原文出处："></a>该文章为翻译文章，原文出处：</h1><p>Liu, R., Wang, J., &amp; Zhang, B. (n.d.). High Definition Map for Automated Driving: Overview and Analysis. Journal of Navigation, 1-18. doi:10.1017/S0373463319000638</p>
]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>高精地图</tag>
        <tag>自动驾驶定位</tag>
        <tag>NDS</tag>
        <tag>NDT</tag>
        <tag>定位模型</tag>
        <tag>相对定位</tag>
        <tag>绝对定位</tag>
        <tag>车道模型</tag>
        <tag>道路模型</tag>
      </tags>
  </entry>
</search>
