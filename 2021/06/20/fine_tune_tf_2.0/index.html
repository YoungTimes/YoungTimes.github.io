<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Week 2 Assignment: Zombie DetectionWelcome to this week’s programming assignment! You will use the Object Detection API and retrain RetinaNet to spot Zombies using just 5 training images. You will set">
<meta property="og:type" content="article">
<meta property="og:title" content="Fine Tune in Tensorflow 2.x">
<meta property="og:url" content="http://example.com/2021/06/20/fine_tune_tf_2.0/index.html">
<meta property="og:site_name" content="半杯茶的小酒杯">
<meta property="og:description" content="Week 2 Assignment: Zombie DetectionWelcome to this week’s programming assignment! You will use the Object Detection API and retrain RetinaNet to spot Zombies using just 5 training images. You will set">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://drive.google.com/uc?export=view&id=18Ck0qNSZy9F1KsUKWc4Jv7_x_1e_fXTN">
<meta property="og:image" content="http://example.com/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_15_1.png">
<meta property="og:image" content="http://example.com/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_34_0.png">
<meta property="og:image" content="http://example.com/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_131_1.jpg">
<meta property="og:image" content="http://example.com/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_131_3.jpg">
<meta property="og:image" content="http://example.com/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_131_5.jpg">
<meta property="article:published_time" content="2021-06-20T07:39:38.000Z">
<meta property="article:modified_time" content="2021-06-20T08:23:58.613Z">
<meta property="article:author" content="Young Times">
<meta property="article:tag" content="自动驾驶">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://drive.google.com/uc?export=view&id=18Ck0qNSZy9F1KsUKWc4Jv7_x_1e_fXTN">

<link rel="canonical" href="http://example.com/2021/06/20/fine_tune_tf_2.0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Fine Tune in Tensorflow 2.x | 半杯茶的小酒杯</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">半杯茶的小酒杯</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/20/fine_tune_tf_2.0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="Young Times">
      <meta itemprop="description" content="路要一步一步的走">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="半杯茶的小酒杯">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Fine Tune in Tensorflow 2.x
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-20 15:39:38 / 修改时间：16:23:58" itemprop="dateCreated datePublished" datetime="2021-06-20T15:39:38+08:00">2021-06-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" itemprop="url" rel="index"><span itemprop="name">自动驾驶</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/06/20/fine_tune_tf_2.0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/06/20/fine_tune_tf_2.0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>146k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2:13</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Week-2-Assignment-Zombie-Detection"><a href="#Week-2-Assignment-Zombie-Detection" class="headerlink" title="Week 2 Assignment: Zombie Detection"></a>Week 2 Assignment: Zombie Detection</h1><p>Welcome to this week’s programming assignment! You will use the Object Detection API and retrain <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.02002">RetinaNet</a> to spot Zombies using just 5 training images. You will setup the model to restore pretrained weights and fine tune the classification layers.</p>
<p><em><strong>Important:</strong></em> <em>This colab notebook has read-only access so you won’t be able to save your changes. If you want to save your work periodically, please click <code>File -&gt; Save a Copy in Drive</code> to create a copy in your account, then work from there.</em>  </p>
<img src='https://drive.google.com/uc?export=view&id=18Ck0qNSZy9F1KsUKWc4Jv7_x_1e_fXTN' alt='zombie'>

<span id="more"></span>

<h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2><ul>
<li><a href="#exercise-1">Exercise 1 - Import Object Detection API packages</a></li>
<li><a href="#exercise-2">Exercise 2 - Visualize the training images</a></li>
<li><a href="#exercise-3">Exercise 3 - Define the category index dictionary</a></li>
<li><a href="#exercise-4">Exercise 4 - Download checkpoints</a></li>
<li><a href="#exercise-5-1">Exercise 5.1 - Locate and read from the configuration file</a></li>
<li><a href="#exercise-5-2">Exercise 5.2 - Modify the model configuration</a></li>
<li><a href="#exercise-5-3">Exercise 5.3 - Modify model_config</a></li>
<li><a href="#exercise-5-4">Exercise 5.4 - Build the custom model</a></li>
<li><a href="#exercise-6-1">Exercise 6.1 - Define Checkpoints for the box predictor</a></li>
<li><a href="#exercise-6-2">Exercise 6.2 - Define the temporary model checkpoint</a></li>
<li><a href="#exercise-6-2">Exercise 6.3 - Restore the checkpoint</a></li>
<li><a href="#exercise-7">Exercise 7 - Run a dummy image to generate the model variables</a></li>
<li><a href="#exercise-8">Exercise 8 - Set training hyperparameters</a></li>
<li><a href="#exercise-9">Exercise 9 - Select the prediction layer variables</a></li>
<li><a href="#exercise-10">Exercise 10 - Define the training step</a></li>
<li><a href="#exercise-11">Exercise 11 - Preprocess, predict, and post process an image</a></li>
</ul>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>You’ll start by installing the Tensorflow 2 <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/object_detection">Object Detection API</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># uncomment the next line if you want to delete an existing models directory</span></span><br><span class="line">!rm -rf ./models/</span><br><span class="line"></span><br><span class="line"><span class="comment"># clone the Tensorflow Model Garden</span></span><br><span class="line">!git clone --depth <span class="number">1</span> https://github.com/tensorflow/models/</span><br></pre></td></tr></table></figure>

<pre><code>Cloning into &#39;models&#39;...
remote: Enumerating objects: 2713, done.[K
remote: Counting objects: 100% (2713/2713), done.[K
remote: Compressing objects: 100% (2252/2252), done.[K
remote: Total 2713 (delta 690), reused 1244 (delta 427), pack-reused 0[K
Receiving objects: 100% (2713/2713), 32.69 MiB | 34.65 MiB/s, done.
Resolving deltas: 100% (690/690), done.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># install the Object Detection API</span></span><br><span class="line">!cd models/research/ &amp;&amp; protoc object_detection/protos/*.proto --python_out=. &amp;&amp; cp object_detection/packages/tf2/setup.py . &amp;&amp; python -m pip install .</span><br></pre></td></tr></table></figure>

<pre><code>Processing /content/models/research
Collecting avro-python3
  Downloading https://files.pythonhosted.org/packages/cc/97/7a6970380ca8db9139a3cc0b0e3e0dd3e4bc584fb3644e1d06e71e1a55f0/avro-python3-1.10.2.tar.gz
Collecting apache-beam
[?25l  Downloading https://files.pythonhosted.org/packages/ac/c9/395a9759dfbf9e87203a69c33b2e94f10d566d9391bddb6f99facafe64c3/apache_beam-2.30.0-cp37-cp37m-manylinux2010_x86_64.whl (9.6MB)
[K     |████████████████████████████████| 9.6MB 16.9MB/s 
[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)
Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)
Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)
Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.23)
Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)
Collecting tf-slim
[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)
[K     |████████████████████████████████| 358kB 47.9MB/s 
[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)
Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)
Collecting lvis
  Downloading https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl
Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)
Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)
Collecting tf-models-official
[?25l  Downloading https://files.pythonhosted.org/packages/96/08/81bbc275e8e9c6d1e03dd26daec3a67f45e6322804cbce3d51f93eae1961/tf_models_official-2.5.0-py2.py3-none-any.whl (1.6MB)
[K     |████████████████████████████████| 1.6MB 46.8MB/s 
[?25hRequirement already satisfied: grpcio&lt;2,&gt;=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (1.34.1)
Requirement already satisfied: oauth2client&lt;5,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (4.1.3)
Requirement already satisfied: numpy&lt;1.21.0,&gt;=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (1.19.5)
Requirement already satisfied: pymongo&lt;4.0.0,&gt;=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (3.11.4)
Requirement already satisfied: typing-extensions&lt;3.8.0,&gt;=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (3.7.4.3)
Collecting dill&lt;0.3.2,&gt;=0.3.1.1
[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)
[K     |████████████████████████████████| 153kB 56.1MB/s 
[?25hRequirement already satisfied: crcmod&lt;2.0,&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (1.7)
Requirement already satisfied: pyarrow&lt;4.0.0,&gt;=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (3.0.0)
Requirement already satisfied: pydot&lt;2,&gt;=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (1.3.0)
Collecting hdfs&lt;3.0.0,&gt;=2.1.0
  Downloading https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl
Collecting fastavro&lt;2,&gt;=0.21.4
[?25l  Downloading https://files.pythonhosted.org/packages/52/d1/8f5c8611026f0ddcd86a8e2f965998e0c159af980c31efba72342c69f3e4/fastavro-1.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2MB)
[K     |████████████████████████████████| 2.3MB 43.8MB/s 
[?25hRequirement already satisfied: httplib2&lt;0.20.0,&gt;=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (0.17.4)
Requirement already satisfied: pytz&gt;=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (2018.9)
Requirement already satisfied: python-dateutil&lt;3,&gt;=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (2.8.1)
Collecting requests&lt;3.0.0,&gt;=2.24.0
[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)
[K     |████████████████████████████████| 61kB 8.5MB/s 
[?25hCollecting future&lt;1.0.0,&gt;=0.18.2
[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)
[K     |████████████████████████████████| 829kB 39.8MB/s 
[?25hRequirement already satisfied: protobuf&lt;4,&gt;=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (3.12.4)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;object-detection==0.1) (0.10.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;object-detection==0.1) (2.4.7)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;object-detection==0.1) (1.3.1)
Requirement already satisfied: absl-py&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim-&gt;object-detection==0.1) (0.12.0)
Requirement already satisfied: setuptools&gt;=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools-&gt;object-detection==0.1) (57.0.0)
Requirement already satisfied: opencv-python&gt;=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis-&gt;object-detection==0.1) (4.1.2.30)
Collecting sacrebleu
[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)
[K     |████████████████████████████████| 61kB 8.7MB/s 
[?25hCollecting tensorflow-addons
[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)
[K     |████████████████████████████████| 686kB 51.0MB/s 
[?25hRequirement already satisfied: psutil&gt;=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (5.4.8)
Requirement already satisfied: google-api-python-client&gt;=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (1.12.8)
Collecting py-cpuinfo&gt;=3.3.0
[?25l  Downloading https://files.pythonhosted.org/packages/e6/ba/77120e44cbe9719152415b97d5bfb29f4053ee987d6cb63f55ce7d50fadc/py-cpuinfo-8.0.0.tar.gz (99kB)
[K     |████████████████████████████████| 102kB 16.1MB/s 
[?25hCollecting seqeval
[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)
[K     |████████████████████████████████| 51kB 8.3MB/s 
[?25hRequirement already satisfied: google-cloud-bigquery&gt;=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (1.21.0)
Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (4.0.1)
Requirement already satisfied: tensorflow-hub&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (0.12.0)
Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (0.4.0)
Collecting tensorflow-model-optimization&gt;=0.4.1
[?25l  Downloading https://files.pythonhosted.org/packages/78/8f/f6969dc64709c5c5e22cfd7057a83adbc927e6855a431b234168222cbf03/tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211kB)
[K     |████████████████████████████████| 215kB 54.5MB/s 
[?25hRequirement already satisfied: kaggle&gt;=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (1.5.12)
Collecting sentencepiece
[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)
[K     |████████████████████████████████| 1.2MB 34.4MB/s 
[?25hCollecting opencv-python-headless
[?25l  Downloading https://files.pythonhosted.org/packages/c3/35/bfc76533f2274cd3da4e2cf255cd13ab9d7f6fc8990c06911e7f8fcc2130/opencv_python_headless-4.5.2.54-cp37-cp37m-manylinux2014_x86_64.whl (38.2MB)
[K     |████████████████████████████████| 38.2MB 73kB/s 
[?25hRequirement already satisfied: tensorflow&gt;=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official-&gt;object-detection==0.1) (2.5.0)
Collecting pyyaml&gt;=5.1
[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)
[K     |████████████████████████████████| 645kB 37.9MB/s 
[?25hRequirement already satisfied: rsa&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client&lt;5,&gt;=2.0.1-&gt;apache-beam-&gt;object-detection==0.1) (4.7.2)
Requirement already satisfied: pyasn1-modules&gt;=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client&lt;5,&gt;=2.0.1-&gt;apache-beam-&gt;object-detection==0.1) (0.2.8)
Requirement already satisfied: pyasn1&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client&lt;5,&gt;=2.0.1-&gt;apache-beam-&gt;object-detection==0.1) (0.4.8)
Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs&lt;3.0.0,&gt;=2.1.0-&gt;apache-beam-&gt;object-detection==0.1) (0.6.2)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.24.0-&gt;apache-beam-&gt;object-detection==0.1) (1.24.3)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.24.0-&gt;apache-beam-&gt;object-detection==0.1) (2021.5.30)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.24.0-&gt;apache-beam-&gt;object-detection==0.1) (2.10)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.24.0-&gt;apache-beam-&gt;object-detection==0.1) (3.0.4)
Collecting portalocker==2.0.0
  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl
Requirement already satisfied: typeguard&gt;=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons-&gt;tf-models-official-&gt;object-detection==0.1) (2.7.1)
Requirement already satisfied: google-auth&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official-&gt;object-detection==0.1) (1.31.0)
Requirement already satisfied: uritemplate&lt;4dev,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official-&gt;object-detection==0.1) (3.0.1)
Requirement already satisfied: google-api-core&lt;2dev,&gt;=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official-&gt;object-detection==0.1) (1.26.3)
Requirement already satisfied: google-auth-httplib2&gt;=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official-&gt;object-detection==0.1) (0.0.4)
Requirement already satisfied: scikit-learn&gt;=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval-&gt;tf-models-official-&gt;object-detection==0.1) (0.22.2.post1)
Requirement already satisfied: google-resumable-media!=0.4.0,&lt;0.5.0dev,&gt;=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery&gt;=0.31.0-&gt;tf-models-official-&gt;object-detection==0.1) (0.4.1)
Requirement already satisfied: google-cloud-core&lt;2.0dev,&gt;=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery&gt;=0.31.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.0.3)
Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (0.1.6)
Requirement already satisfied: importlib-resources; python_version &lt; &quot;3.9&quot; in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (5.1.4)
Requirement already satisfied: attrs&gt;=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (21.2.0)
Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (2.3)
Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (4.41.1)
Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (1.1.0)
Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (1.0.0)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official-&gt;object-detection==0.1) (5.0.2)
Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (0.4.0)
Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.12.1)
Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (3.3.0)
Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (0.36.2)
Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.6.3)
Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.12)
Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (3.1.0)
Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.1.2)
Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (0.2.0)
Requirement already satisfied: tensorflow-estimator&lt;2.6.0,&gt;=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (2.5.0)
Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (2.5.0.dev2021032900)
Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (2.5.0)
Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&gt;=1.16.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official-&gt;object-detection==0.1) (4.2.2)
Requirement already satisfied: packaging&gt;=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official-&gt;object-detection==0.1) (20.9)
Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official-&gt;object-detection==0.1) (1.53.0)
Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.21.3-&gt;seqeval-&gt;tf-models-official-&gt;object-detection==0.1) (1.0.1)
Requirement already satisfied: zipp&gt;=3.1.0; python_version &lt; &quot;3.10&quot; in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version &lt; &quot;3.9&quot;-&gt;tensorflow-datasets-&gt;tf-models-official-&gt;object-detection==0.1) (3.4.1)
Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle&gt;=1.3.9-&gt;tf-models-official-&gt;object-detection==0.1) (1.3)
Requirement already satisfied: cached-property; python_version &lt; &quot;3.8&quot; in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.5.2)
Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (3.3.4)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (0.4.4)
Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.0.1)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (0.6.1)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.8.0)
Requirement already satisfied: importlib-metadata; python_version &lt; &quot;3.8&quot; in /usr/local/lib/python3.7/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (4.5.0)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (1.3.0)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.5-&gt;tensorflow&gt;=2.5.0-&gt;tf-models-official-&gt;object-detection==0.1) (3.1.1)
Building wheels for collected packages: object-detection, avro-python3, dill, future, py-cpuinfo, seqeval
  Building wheel for object-detection (setup.py) ... [?25l[?25hdone
  Created wheel for object-detection: filename=object_detection-0.1-cp37-none-any.whl size=1654779 sha256=ba5fae85d2e1c26555d9bfa64e76622376103577d62a8467921af47e84f99b1d
  Stored in directory: /tmp/pip-ephem-wheel-cache-g2fg_e35/wheels/94/49/4b/39b051683087a22ef7e80ec52152a27249d1a644ccf4e442ea
  Building wheel for avro-python3 (setup.py) ... [?25l[?25hdone
  Created wheel for avro-python3: filename=avro_python3-1.10.2-cp37-none-any.whl size=44011 sha256=4b31530234c7900feaaad2da9b4800ba7f1b5ded9038e77be7c0354d37ceb806
  Stored in directory: /root/.cache/pip/wheels/ee/ee/18/c466221ca6900e3efce2f4ea9c329288808679aecdcb2838d3
  Building wheel for dill (setup.py) ... [?25l[?25hdone
  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78545 sha256=1c6bc14973fd62ecd3401e0b15cd5ea3c5bce8136a2763a3f5bc197eca79465d
  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7
  Building wheel for future (setup.py) ... [?25l[?25hdone
  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=e1908aecd928be4a47ac1e616ec33b893b62ce231f8d0ff001b7f47e31bcd0c6
  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e
  Building wheel for py-cpuinfo (setup.py) ... [?25l[?25hdone
  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-cp37-none-any.whl size=22258 sha256=6c4f49d43bee7129fd120d9e0372742130661f1fdd143276c2463549ba5141ed
  Stored in directory: /root/.cache/pip/wheels/2e/15/f5/aa2a056d223903b52cf4870134e3a01df0c723816835dd08db
  Building wheel for seqeval (setup.py) ... [?25l[?25hdone
  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=41b599b49a39a159eed592382d9cb1062310aa6bdc07a98e17cdcd388c65c53b
  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f
Successfully built object-detection avro-python3 dill future py-cpuinfo seqeval
[31mERROR: multiprocess 0.70.12.2 has requirement dill&gt;=0.3.4, but you&#39;ll have dill 0.3.1.1 which is incompatible.[0m
[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you&#39;ll have requests 2.25.1 which is incompatible.[0m
[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you&#39;ll have folium 0.8.3 which is incompatible.[0m
[31mERROR: apache-beam 2.30.0 has requirement avro-python3!=1.9.2,&lt;1.10.0,&gt;=1.8.1, but you&#39;ll have avro-python3 1.10.2 which is incompatible.[0m
Installing collected packages: avro-python3, dill, requests, hdfs, fastavro, future, apache-beam, tf-slim, lvis, portalocker, sacrebleu, tensorflow-addons, py-cpuinfo, seqeval, tensorflow-model-optimization, sentencepiece, opencv-python-headless, pyyaml, tf-models-official, object-detection
  Found existing installation: dill 0.3.4
    Uninstalling dill-0.3.4:
      Successfully uninstalled dill-0.3.4
  Found existing installation: requests 2.23.0
    Uninstalling requests-2.23.0:
      Successfully uninstalled requests-2.23.0
  Found existing installation: future 0.16.0
    Uninstalling future-0.16.0:
      Successfully uninstalled future-0.16.0
  Found existing installation: PyYAML 3.13
    Uninstalling PyYAML-3.13:
      Successfully uninstalled PyYAML-3.13
Successfully installed apache-beam-2.30.0 avro-python3-1.10.2 dill-0.3.1.1 fastavro-1.4.1 future-0.18.2 hdfs-2.6.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.2.54 portalocker-2.0.0 py-cpuinfo-8.0.0 pyyaml-5.4.1 requests-2.25.1 sacrebleu-1.5.1 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.13.0 tensorflow-model-optimization-0.6.0 tf-models-official-2.5.0 tf-slim-1.1.0
</code></pre>
<h2 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a>Imports</h2><p>Let’s now import the packages you will use in this assignment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">from</span> six <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, Javascript</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image <span class="keyword">as</span> IPyImage</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  <span class="comment"># %tensorflow_version only exists in Colab.</span></span><br><span class="line">  %tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.get_logger().setLevel(<span class="string">&#x27;ERROR&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><a name='exercise-1'></a></p>
<h3 id="Exercise-1-Import-Object-Detection-API-packages"><a href="#Exercise-1-Import-Object-Detection-API-packages" class="headerlink" title="Exercise 1: Import Object Detection API packages"></a><strong>Exercise 1</strong>: Import Object Detection API packages</h3><p>Import the necessary modules from the <code>object_detection</code> package. </p>
<ul>
<li>From the <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/object_detection/utils">utils</a> package:<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/label_map_util.py">label_map_util</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/config_util.py">config_util</a>: You’ll use this to read model configurations from a .config file and then modify that configuration</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py">visualization_utils</a>: please give this the alias <code>viz_utils</code>, as this is what will be used in some visualization code that is given to you later.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/colab_utils.py">colab_utils</a></li>
</ul>
</li>
<li>From the <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/object_detection/builders">builders</a> package:<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/builders/model_builder.py">model_builder</a>: This builds your model according to the model configuration that you’ll specify.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE (Replace Instances of `None` with your code) ###</span></span><br><span class="line"><span class="comment"># import the label map utility module</span></span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> label_map_util</span><br><span class="line"></span><br><span class="line"><span class="comment"># import module for reading and updating configuration files.</span></span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> config_util</span><br><span class="line"></span><br><span class="line"><span class="comment"># import module for visualization. use the alias `viz_utils`</span></span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> visualization_utils <span class="keyword">as</span> viz_utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># import module for building the detection model</span></span><br><span class="line"><span class="keyword">from</span> object_detection.builders <span class="keyword">import</span> model_builder</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import module for utilities in Colab</span></span><br><span class="line"><span class="keyword">from</span> object_detection.utils <span class="keyword">import</span> colab_utils</span><br></pre></td></tr></table></figure>

<h2 id="Utilities"><a href="#Utilities" class="headerlink" title="Utilities"></a>Utilities</h2><p>You’ll define a couple of utility functions for loading images and plotting detections. This code is provided for you.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image_into_numpy_array</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load an image from file into a numpy array.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Puts image into numpy array to feed into tensorflow graph.</span></span><br><span class="line"><span class="string">    Note that by convention we put it into a numpy array with shape</span></span><br><span class="line"><span class="string">    (height, width, channels), where channels=3 for RGB.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    path: a file path.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    uint8 numpy array with shape (img_height, img_width, 3)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    img_data = tf.io.gfile.GFile(path, <span class="string">&#x27;rb&#x27;</span>).read()</span><br><span class="line">    image = Image.<span class="built_in">open</span>(BytesIO(img_data))</span><br><span class="line">    (im_width, im_height) = image.size</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.array(image.getdata()).reshape(</span><br><span class="line">        (im_height, im_width, <span class="number">3</span>)).astype(np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_detections</span>(<span class="params">image_np,</span></span></span><br><span class="line"><span class="function"><span class="params">                    boxes,</span></span></span><br><span class="line"><span class="function"><span class="params">                    classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                    scores,</span></span></span><br><span class="line"><span class="function"><span class="params">                    category_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                    figsize=(<span class="params"><span class="number">12</span>, <span class="number">16</span></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">                    image_name=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Wrapper function to visualize detections.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    image_np: uint8 numpy array with shape (img_height, img_width, 3)</span></span><br><span class="line"><span class="string">    boxes: a numpy array of shape [N, 4]</span></span><br><span class="line"><span class="string">    classes: a numpy array of shape [N]. Note that class indices are 1-based,</span></span><br><span class="line"><span class="string">          and match the keys in the label map.</span></span><br><span class="line"><span class="string">    scores: a numpy array of shape [N] or None.  If scores=None, then</span></span><br><span class="line"><span class="string">          this function assumes that the boxes to be plotted are groundtruth</span></span><br><span class="line"><span class="string">          boxes and plot all boxes as black with no classes or scores.</span></span><br><span class="line"><span class="string">    category_index: a dict containing category dictionaries (each holding</span></span><br><span class="line"><span class="string">          category index `id` and category name `name`) keyed by category indices.</span></span><br><span class="line"><span class="string">    figsize: size for the figure.</span></span><br><span class="line"><span class="string">    image_name: a name for the image file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    image_np_with_annotations = image_np.copy()</span><br><span class="line">    </span><br><span class="line">    viz_utils.visualize_boxes_and_labels_on_image_array(</span><br><span class="line">        image_np_with_annotations,</span><br><span class="line">        boxes,</span><br><span class="line">        classes,</span><br><span class="line">        scores,</span><br><span class="line">        category_index,</span><br><span class="line">        use_normalized_coordinates=<span class="literal">True</span>,</span><br><span class="line">        min_score_thresh=<span class="number">0.8</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> image_name:</span><br><span class="line">        plt.imsave(image_name, image_np_with_annotations)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.imshow(image_np_with_annotations)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Download-the-Zombie-data"><a href="#Download-the-Zombie-data" class="headerlink" title="Download the Zombie data"></a>Download the Zombie data</h2><p>Now you will get 5 images of zombies that you will use for training. </p>
<ul>
<li>The zombies are hosted in a Google bucket.</li>
<li>You can download and unzip the images into a local <code>training/</code> directory by running the cell below.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># uncomment the next 2 lines if you want to delete an existing zip and training directory</span></span><br><span class="line"><span class="comment"># !rm training-zombie.zip</span></span><br><span class="line"><span class="comment"># !rm -rf ./training</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># download the images</span></span><br><span class="line">!wget --no-check-certificate \</span><br><span class="line">    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training-zombie.<span class="built_in">zip</span> \</span><br><span class="line">    -O ./training-zombie.<span class="built_in">zip</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># unzip to a local directory</span></span><br><span class="line">local_zip = <span class="string">&#x27;./training-zombie.zip&#x27;</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;./training&#x27;</span>)</span><br><span class="line">zip_ref.close()</span><br></pre></td></tr></table></figure>

<pre><code>--2021-06-20 07:56:34--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training-zombie.zip
Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.62.128, 172.253.115.128, 172.253.122.128, ...
Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.62.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1915446 (1.8M) [application/zip]
Saving to: ‘./training-zombie.zip’

./training-zombie.z 100%[===================&gt;]   1.83M  --.-KB/s    in 0.01s   

2021-06-20 07:56:34 (167 MB/s) - ‘./training-zombie.zip’ saved [1915446/1915446]
</code></pre>
<p><a name='exercise-2'></a></p>
<h3 id="Exercise-2-Visualize-the-training-images"><a href="#Exercise-2-Visualize-the-training-images" class="headerlink" title="Exercise 2: Visualize the training images"></a><strong>Exercise 2</strong>: Visualize the training images</h3><p>Next, you’ll want to inspect the images that you just downloaded. </p>
<ul>
<li>Please replace instances of <code>None</code> below to load and visualize the 5 training images. </li>
<li>You can inspect the <em>training</em> directory (using the <code>Files</code> button on the left side of this Colab) to see the filenames of the zombie images. The paths for the images will look like this:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;training&#x2F;training-zombie1.jpg</span><br><span class="line">.&#x2F;training&#x2F;training-zombie2.jpg</span><br><span class="line">.&#x2F;training&#x2F;training-zombie3.jpg</span><br><span class="line">.&#x2F;training&#x2F;training-zombie4.jpg</span><br><span class="line">.&#x2F;training&#x2F;training-zombie5.jpg</span><br></pre></td></tr></table></figure>
<ul>
<li>To set file paths, you’ll use <a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/python-os-path-join-method/">os.path.join</a>.  As an example, if you wanted to create the path ‘./parent_folder/file_name1.txt’, you could write: </li>
</ul>
<p><code>os.path.join(&#39;parent_folder&#39;, &#39;file_name&#39;, str(1), &#39;.txt&#39;)</code></p>
<ul>
<li>You should see the 5 training images after running this cell. If not, please inspect your code, particularly the <code>image_path</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE (Replace Instances of `None` with your code) ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># assign the name (string) of the directory containing the training images</span></span><br><span class="line">train_image_dir = <span class="string">&#x27;./training&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># declare an empty list</span></span><br><span class="line">train_images_np = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># run a for loop for each image</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    <span class="comment"># define the path (string) for each image</span></span><br><span class="line">    image_path = os.path.join(train_image_dir, <span class="string">&#x27;training-zombie&#x27;</span> + <span class="built_in">str</span>(i) +<span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line">    print(image_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load images into numpy arrays and append to a list</span></span><br><span class="line">    train_images_np.append(load_image_into_numpy_array(image_path))</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># configure plot settings via rcParams</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.grid&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;xtick.labelsize&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;ytick.labelsize&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;xtick.top&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;xtick.bottom&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;ytick.left&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;ytick.right&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">14</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot images</span></span><br><span class="line"><span class="keyword">for</span> idx, train_image_np <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_images_np):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">5</span>, idx+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(train_image_np)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>./training/training-zombie1.jpg
./training/training-zombie2.jpg
./training/training-zombie3.jpg
./training/training-zombie4.jpg
./training/training-zombie5.jpg
</code></pre>
<p><img src="/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_15_1.png" alt="png"></p>
<p><a name='gt_boxes_definition'></a></p>
<h2 id="Prepare-data-for-training-Optional"><a href="#Prepare-data-for-training-Optional" class="headerlink" title="Prepare data for training (Optional)"></a>Prepare data for training (Optional)</h2><p>In this section, you will create your ground truth boxes. You can either draw your own boxes or use a prepopulated list of coordinates that we have provided below. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the list of ground truth boxes</span></span><br><span class="line">gt_boxes = []</span><br></pre></td></tr></table></figure>

<h4 id="Option-1-draw-your-own-ground-truth-boxes"><a href="#Option-1-draw-your-own-ground-truth-boxes" class="headerlink" title="Option 1: draw your own ground truth boxes"></a>Option 1: draw your own ground truth boxes</h4><p>If you want to draw your own, please run the next cell and the following test code. If not, then skip these optional cells.</p>
<ul>
<li>Draw a box around the zombie in each image. </li>
<li>Click the <code>next image</code> button to go to the next image</li>
<li>Click <code>submit</code> when it says “All images completed!!”. </li>
</ul>
<ul>
<li>Make sure to not make the bounding box too big. <ul>
<li>If the box is too big, the model might learn the features of the background (e.g. door, road, etc) in determining if there is a zombie or not. </li>
</ul>
</li>
<li>Include the entire zombie inside the box. </li>
<li>As an example, scroll to the beginning of this notebook to look at the bounding box around the zombie.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 1: draw your own ground truth boxes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># annotate the training images</span></span><br><span class="line">colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 1: draw your own ground truth boxes</span></span><br><span class="line"><span class="comment"># TEST CODE:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  <span class="keyword">assert</span>(<span class="built_in">len</span>(gt_boxes) == <span class="number">5</span>), <span class="string">&quot;Warning: gt_boxes is empty. Did you click `submit`?&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> AssertionError <span class="keyword">as</span> e:</span><br><span class="line">  print(e)</span><br><span class="line"></span><br><span class="line"><span class="comment"># checks if there are boxes for all 5 images</span></span><br><span class="line"><span class="keyword">for</span> gt_box <span class="keyword">in</span> gt_boxes:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      <span class="keyword">assert</span>(gt_box <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>), <span class="string">&quot;There are less than 5 sets of box coordinates. &quot;</span> \</span><br><span class="line">                                  <span class="string">&quot;Please re-run the cell above to draw the boxes again.\n&quot;</span> \</span><br><span class="line">                                  <span class="string">&quot;Alternatively, you can run the next cell to load pre-determined &quot;</span> \</span><br><span class="line">                                  <span class="string">&quot;ground truth boxes.&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">except</span> AssertionError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ref_gt_boxes = [</span><br><span class="line">        np.array([[<span class="number">0.27333333</span>, <span class="number">0.41500586</span>, <span class="number">0.74333333</span>, <span class="number">0.57678781</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.29833333</span>, <span class="number">0.45955451</span>, <span class="number">0.75666667</span>, <span class="number">0.61078546</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.40833333</span>, <span class="number">0.18288394</span>, <span class="number">0.945</span>, <span class="number">0.34818288</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.16166667</span>, <span class="number">0.61899179</span>, <span class="number">0.8</span>, <span class="number">0.91910903</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.28833333</span>, <span class="number">0.12543962</span>, <span class="number">0.835</span>, <span class="number">0.35052755</span>]]),</span><br><span class="line">      ]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> gt_box, ref_gt_box <span class="keyword">in</span> <span class="built_in">zip</span>(gt_boxes, ref_gt_boxes):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      <span class="keyword">assert</span>(np.allclose(gt_box, ref_gt_box, atol=<span class="number">0.04</span>)), <span class="string">&quot;One of the boxes is too big or too small. &quot;</span> \</span><br><span class="line">                                                          <span class="string">&quot;Please re-draw and make the box tighter around the zombie.&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">except</span> AssertionError <span class="keyword">as</span> e:</span><br><span class="line">      print(e)</span><br><span class="line">      <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p><a name='gt-boxes'></a></p>
<h4 id="Option-2-use-the-given-ground-truth-boxes"><a href="#Option-2-use-the-given-ground-truth-boxes" class="headerlink" title="Option 2: use the given ground truth boxes"></a>Option 2: use the given ground truth boxes</h4><p>You can also use this list if you opt not to draw the boxes yourself.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 2: use given ground truth boxes</span></span><br><span class="line"><span class="comment"># set this to `True` if you want to override the boxes you drew</span></span><br><span class="line">override = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bounding boxes for each of the 5 zombies found in each image. </span></span><br><span class="line"><span class="comment"># you can use these instead of drawing the boxes yourself.</span></span><br><span class="line">ref_gt_boxes = [</span><br><span class="line">        np.array([[<span class="number">0.27333333</span>, <span class="number">0.41500586</span>, <span class="number">0.74333333</span>, <span class="number">0.57678781</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.29833333</span>, <span class="number">0.45955451</span>, <span class="number">0.75666667</span>, <span class="number">0.61078546</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.40833333</span>, <span class="number">0.18288394</span>, <span class="number">0.945</span>, <span class="number">0.34818288</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.16166667</span>, <span class="number">0.61899179</span>, <span class="number">0.8</span>, <span class="number">0.91910903</span>]]),</span><br><span class="line">        np.array([[<span class="number">0.28833333</span>, <span class="number">0.12543962</span>, <span class="number">0.835</span>, <span class="number">0.35052755</span>]]),</span><br><span class="line">      ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># if gt_boxes is empty, use the reference</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> gt_boxes <span class="keyword">or</span> override <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">  gt_boxes = ref_gt_boxes</span><br><span class="line"></span><br><span class="line"><span class="comment"># if gt_boxes does not contain 5 box coordinates, use the reference </span></span><br><span class="line"><span class="keyword">for</span> gt_box <span class="keyword">in</span> gt_boxes:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      <span class="keyword">assert</span>(gt_box <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">      gt_boxes = ref_gt_boxes</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="View-your-ground-truth-box-coordinates"><a href="#View-your-ground-truth-box-coordinates" class="headerlink" title="View your ground truth box coordinates"></a>View your ground truth box coordinates</h4><p>Whether you chose to draw your own or use the given boxes, please check your list of ground truth box coordinates.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print the coordinates of your ground truth boxes</span></span><br><span class="line"><span class="keyword">for</span> gt_box <span class="keyword">in</span> gt_boxes:</span><br><span class="line">  print(gt_box)</span><br></pre></td></tr></table></figure>

<pre><code>[[0.27333333 0.41500586 0.74333333 0.57678781]]
[[0.29833333 0.45955451 0.75666667 0.61078546]]
[[0.40833333 0.18288394 0.945      0.34818288]]
[[0.16166667 0.61899179 0.8        0.91910903]]
[[0.28833333 0.12543962 0.835      0.35052755]]
</code></pre>
<p>Below, we add the class annotations. For simplicity, we assume just a single class, though it should be straightforward to extend this to handle multiple classes. We will also convert everything to the format that the training loop expects (e.g., conversion to tensors, one-hot representations, etc.).</p>
<p><a name='exercise-3'></a></p>
<h3 id="Exercise-3-Define-the-category-index-dictionary"><a href="#Exercise-3-Define-the-category-index-dictionary" class="headerlink" title="Exercise 3: Define the category index dictionary"></a><strong>Exercise 3</strong>: Define the category index dictionary</h3><p>You’ll need to tell the model which integer class ID to assign to the ‘zombie’ category, and what ‘name’ to associate with that integer id.</p>
<ul>
<li><p>zombie_class_id: By convention, class ID integers start numbering from 1,2,3, onward.</p>
<ul>
<li>If there is ever a ‘background’ class, it could be assigned the integer 0, but in this case, you’re just predicting the one zombie class.</li>
<li>Since you are just predicting one class (zombie), please assign <code>1</code> to the zombie class ID.</li>
</ul>
</li>
<li><p>category_index: Please define the <code>category_index</code> dictionary, which will have the same structure as this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;human_class_id : </span><br><span class="line">  &#123;&#39;id&#39;  : human_class_id, </span><br><span class="line">   &#39;name&#39;: &#39;human_so_far&#39;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Define <code>category_index</code> similar to the example dictionary above, except for zombies.</li>
<li>This will be used by the succeeding functions to know the class <code>id</code> and <code>name</code> of zombie images.</li>
</ul>
</li>
<li><p>num_classes: Since you are predicting one class, please assign <code>1</code> to the number of classes that the model will predict.</p>
<ul>
<li>This will be used during data preprocessing and again when you configure the model.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE (Replace instances of `None` with your code ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assign the zombie class ID</span></span><br><span class="line">zombie_class_id = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># define a dictionary describing the zombie class</span></span><br><span class="line">category_index = &#123;zombie_class_id:&#123;<span class="string">&#x27;id&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;zombie&#x27;</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify the number of classes that the model will predict</span></span><br><span class="line">num_classes = <span class="number">1</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CODE:</span></span><br><span class="line"></span><br><span class="line">print(category_index[zombie_class_id])</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;id&#39;: 1, &#39;name&#39;: &#39;zombie&#39;&#125;
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;zombie&#x27;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h3><p>You will now do some data preprocessing so it is formatted properly before it is fed to the model:</p>
<ul>
<li>Convert the class labels to one-hot representations</li>
<li>convert everything (i.e. train images, gt boxes and class labels) to tensors.</li>
</ul>
<p>This code is provided for you.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The `label_id_offset` here shifts all classes by a certain number of indices;</span></span><br><span class="line"><span class="comment"># we do this here so that the model receives one-hot labels where non-background</span></span><br><span class="line"><span class="comment"># classes start counting at the zeroth index.  This is ordinarily just handled</span></span><br><span class="line"><span class="comment"># automatically in our training binaries, but we need to reproduce it here.</span></span><br><span class="line"></span><br><span class="line">label_id_offset = <span class="number">1</span></span><br><span class="line">train_image_tensors = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># lists containing the one-hot encoded classes and ground truth boxes</span></span><br><span class="line">gt_classes_one_hot_tensors = []</span><br><span class="line">gt_box_tensors = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (train_image_np, gt_box_np) <span class="keyword">in</span> <span class="built_in">zip</span>(train_images_np, gt_boxes):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert training image to tensor, add batch dimension, and add to list</span></span><br><span class="line">    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(</span><br><span class="line">        train_image_np, dtype=tf.float32), axis=<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert numpy array to tensor, then add to list</span></span><br><span class="line">    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># apply offset to to have zero-indexed ground truth classes</span></span><br><span class="line">    zero_indexed_groundtruth_classes = tf.convert_to_tensor(</span><br><span class="line">        np.ones(shape=[gt_box_np.shape[<span class="number">0</span>]], dtype=np.int32) - label_id_offset)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># do one-hot encoding to ground truth classes</span></span><br><span class="line">    gt_classes_one_hot_tensors.append(tf.one_hot(</span><br><span class="line">        zero_indexed_groundtruth_classes, num_classes))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Done prepping data.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Done prepping data.
</code></pre>
<h2 id="Visualize-the-zombies-with-their-ground-truth-bounding-boxes"><a href="#Visualize-the-zombies-with-their-ground-truth-bounding-boxes" class="headerlink" title="Visualize the zombies with their ground truth bounding boxes"></a>Visualize the zombies with their ground truth bounding boxes</h2><p>You should see the 5 training images with the bounding boxes after running the cell below. If not, please re-run the <a href="#gt_boxes_definition">annotation tool</a> again or use the prepopulated <code>gt_boxes</code> array given.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># give boxes a score of 100%</span></span><br><span class="line">dummy_scores = np.array([<span class="number">1.0</span>], dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the figure size</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># use the `plot_detections()` utility function to draw the ground truth boxes</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">4</span>, idx+<span class="number">1</span>)</span><br><span class="line">    plot_detections(</span><br><span class="line">      train_images_np[idx],</span><br><span class="line">      gt_boxes[idx],</span><br><span class="line">      np.ones(shape=[gt_boxes[idx].shape[<span class="number">0</span>]], dtype=np.int32),</span><br><span class="line">      dummy_scores, category_index)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_34_0.png" alt="png"></p>
<h2 id="Download-the-checkpoint-containing-the-pre-trained-weights"><a href="#Download-the-checkpoint-containing-the-pre-trained-weights" class="headerlink" title="Download the checkpoint containing the pre-trained weights"></a>Download the checkpoint containing the pre-trained weights</h2><p>Next, you will download <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.02002">RetinaNet</a> and copy it inside the object detection directory.</p>
<p>When working with models that are at the frontiers of research, the models and checkpoints may not yet be organized in a central location like the TensorFlow Garden (<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a>).</p>
<ul>
<li>You’ll often read a blog post from the researchers, who will usually provide information on:<ul>
<li>how to use the model</li>
<li>where to download the models and pre-trained checkpoints.</li>
</ul>
</li>
</ul>
<p>It’s good practice to do some of this “detective work”, so that you’ll feel more comfortable when exploring new models yourself!  So please try the following steps:</p>
<ul>
<li>Go to the <a target="_blank" rel="noopener" href="https://blog.tensorflow.org/">TensorFlow Blog</a>, where researchers announce new findings.</li>
<li>In the search box at the top of the page, search for “retinanet”.</li>
<li>In the search results, click on the blog post titled “TensorFlow 2 meets the Object Detection API” (it may be the first search result).</li>
<li>Skim through this blog and look for links to either the checkpoints or to Colabs that will show you how to use the checkpoints.</li>
<li>Try to fill out the following code cell below, which does the following:<ul>
<li>Download the compressed SSD Resnet 50 version 1, 640 x 640 checkpoint.</li>
<li>Untar (decompress) the tar file</li>
<li>Move the decompressed checkpoint to <code>models/research/object_detection/test_data/</code></li>
</ul>
</li>
</ul>
<p>If you want some help getting started, please click on the “Initial Hints” cell to get some hints.</p>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Initial Hints</b></font>
</summary>
<p>
    
General Hints to get started
<ul>
    <li>The link to the blog is <a target="_blank" rel="noopener" href="https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html">TensorFlow 2 meets the Object Detection API</a> </li>
    <li>In the blog, you'll find the text "COCO pre-trained weights, which links to a list of checkpoints in GitHub titled 
      <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md">TensorFlow 2 Detection Model Zoo<a>.  
    </li>
    <li>
          If you read each checkpoint name, you'll find the one for SSD Resnet 50 version 1, 640 by 640.  If you hover your mouse over
    </li>
    <li>
        If you right-click on the desired checkpoint link, you can save the link address, and use it in the code cell below to get the checkpoint.
    </li>
    <li>For more hints, please click on the cell "More Hints"</li>
</ul>
</p>


<details>    
<summary>
    <font size="3" color="darkgreen"><b>More Hints</b></font>
</summary>
<p>
    
More Hints
<ul>
    <li> To see how to download the checkpoint, look in the blog for links to Colab tutorials.
    </li>
    <li>
        For example, the blog links to a Colab titled <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb">Intro to Object Detection Colab</a>
    </li>
    <li>
        In the Colab, you'll see the section titled "Build a detection model and load pre-trained model weights", which is followed by a code cell showing how to download, decompress, and relocate a checkpoint.  Use similar syntax, except use the URL to the ssd resnet50 version 1 640x640 checkpoint instead.
    </li>
    <li> If you're feeling stuck, please click on the cell "Even More Hints".
    </li>
</ul>
</p>


<details>    
<summary>
    <font size="3" color="darkgreen"><b>Even More Hints</b></font>
</summary>
<p>
    
Even More Hints
<ul>
    <li> The blog post also links to a notebook titled 
    <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb">
    Eager Few Shot Object Detection Colab</a>
    </li>
    <li> In this notebook, look for the section titled "Create model and restore weights for all but last layer".  
    The code cell below it shows how to download the exact checkpoint that you're interested in.
    </li>
    <li>You can also review the lecture videos for this week, which show the same code.</li>

</ul>
</p>


<p><a name='exercise-4'></a></p>
<h3 id="Exercise-4-Download-checkpoints"><a href="#Exercise-4-Download-checkpoints" class="headerlink" title="Exercise 4: Download checkpoints"></a>Exercise 4: Download checkpoints</h3><ul>
<li>Download the compressed SSD Resnet 50 version 1, 640 x 640 checkpoint.</li>
<li>Untar (decompress) the tar file</li>
<li>Move the decompressed checkpoint to <code>models/research/object_detection/test_data/</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">!wget http://download.tensorflow.org/models/object_detection/tf2/<span class="number">20200711</span>/ssd_resnet50_v1_fpn_640x640_coco17_tpu-<span class="number">8.</span>tar.gz</span><br><span class="line"></span><br><span class="line">!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-<span class="number">8.</span>tar.gz</span><br><span class="line"></span><br><span class="line">!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-<span class="number">8</span>/checkpoint models/research/object_detection/test_data/</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="comment"># Download the SSD Resnet 50 version 1, 640x640 checkpoint</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># untar (decompress) the tar file</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># copy the checkpoint to the test_data folder models/research/object_detection/test_data/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE</span></span><br></pre></td></tr></table></figure>

<pre><code>--2021-06-20 07:57:52--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz
Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.65.80, 2607:f8b0:4004:832::2010
Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.65.80|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 244817203 (233M) [application/x-tar]
Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’

ssd_resnet50_v1_fpn 100%[===================&gt;] 233.48M   149MB/s    in 1.6s    

2021-06-20 07:57:53 (149 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’ saved [244817203/244817203]
</code></pre>
<h2 id="Configure-the-model"><a href="#Configure-the-model" class="headerlink" title="Configure the model"></a>Configure the model</h2><p>Here, you will configure the model for this use case.</p>
<p><a name='exercise-5-1'></a></p>
<h3 id="Exercise-5-1-Locate-and-read-from-the-configuration-file"><a href="#Exercise-5-1-Locate-and-read-from-the-configuration-file" class="headerlink" title="Exercise 5.1: Locate and read from the configuration file"></a><strong>Exercise 5.1</strong>: Locate and read from the configuration file</h3><h4 id="pipeline-config"><a href="#pipeline-config" class="headerlink" title="pipeline_config"></a>pipeline_config</h4><ul>
<li>In the Colab, on the left side table of contents, click on the folder icon to display the file browser for the current workspace.  </li>
<li>Navigate to <code>models/research/object_detection/configs/tf2</code>.  The folder has multiple .config files.  </li>
<li>Look for the file corresponding to ssd resnet 50 version 1 640x640.</li>
<li>You can double-click the config file to view its contents. This may help you as you complete the next few code cells to configure your model.</li>
<li>Set the <code>pipeline_config</code> to a string that contains the full path to the resnet config file, in other words: <code>models/research/.../... .config</code></li>
</ul>
<h4 id="configs"><a href="#configs" class="headerlink" title="configs"></a>configs</h4><p>If you look at the module <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/config_util.py">config_util</a> that you imported, it contains the following function:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def get_configs_from_pipeline_file(pipeline_config_path, config_override&#x3D;None):</span><br></pre></td></tr></table></figure>
<ul>
<li>Please use this function to load the configuration from your <code>pipeline_config</code>.<ul>
<li><code>configs</code> will now contain a dictionary.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="comment"># define the path to the .config file for ssd resnet 50 v1 640x640</span></span><br><span class="line">pipeline_config = <span class="string">&#x27;models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the configuration file into a dictionary</span></span><br><span class="line">configs = config_util.get_configs_from_pipeline_file(pipeline_config)</span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"><span class="comment"># See what configs looks like</span></span><br><span class="line">configs</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;eval_config&#39;: metrics_set: &quot;coco_detection_metrics&quot;
 use_moving_averages: false,
 &#39;eval_input_config&#39;: label_map_path: &quot;PATH_TO_BE_CONFIGURED/label_map.txt&quot;
 shuffle: false
 num_epochs: 1
 tf_record_input_reader &#123;
   input_path: &quot;PATH_TO_BE_CONFIGURED/val2017-?????-of-00032.tfrecord&quot;
 &#125;,
 &#39;eval_input_configs&#39;: [label_map_path: &quot;PATH_TO_BE_CONFIGURED/label_map.txt&quot;
 shuffle: false
 num_epochs: 1
 tf_record_input_reader &#123;
   input_path: &quot;PATH_TO_BE_CONFIGURED/val2017-?????-of-00032.tfrecord&quot;
 &#125;
 ],
 &#39;model&#39;: ssd &#123;
   num_classes: 90
   image_resizer &#123;
     fixed_shape_resizer &#123;
       height: 640
       width: 640
     &#125;
   &#125;
   feature_extractor &#123;
     type: &quot;ssd_resnet50_v1_fpn_keras&quot;
     depth_multiplier: 1.0
     min_depth: 16
     conv_hyperparams &#123;
       regularizer &#123;
         l2_regularizer &#123;
           weight: 0.00039999998989515007
         &#125;
       &#125;
       initializer &#123;
         truncated_normal_initializer &#123;
           mean: 0.0
           stddev: 0.029999999329447746
         &#125;
       &#125;
       activation: RELU_6
       batch_norm &#123;
         decay: 0.996999979019165
         scale: true
         epsilon: 0.0010000000474974513
       &#125;
     &#125;
     override_base_feature_extractor_hyperparams: true
     fpn &#123;
       min_level: 3
       max_level: 7
     &#125;
   &#125;
   box_coder &#123;
     faster_rcnn_box_coder &#123;
       y_scale: 10.0
       x_scale: 10.0
       height_scale: 5.0
       width_scale: 5.0
     &#125;
   &#125;
   matcher &#123;
     argmax_matcher &#123;
       matched_threshold: 0.5
       unmatched_threshold: 0.5
       ignore_thresholds: false
       negatives_lower_than_unmatched: true
       force_match_for_each_row: true
       use_matmul_gather: true
     &#125;
   &#125;
   similarity_calculator &#123;
     iou_similarity &#123;
     &#125;
   &#125;
   box_predictor &#123;
     weight_shared_convolutional_box_predictor &#123;
       conv_hyperparams &#123;
         regularizer &#123;
           l2_regularizer &#123;
             weight: 0.00039999998989515007
           &#125;
         &#125;
         initializer &#123;
           random_normal_initializer &#123;
             mean: 0.0
             stddev: 0.009999999776482582
           &#125;
         &#125;
         activation: RELU_6
         batch_norm &#123;
           decay: 0.996999979019165
           scale: true
           epsilon: 0.0010000000474974513
         &#125;
       &#125;
       depth: 256
       num_layers_before_predictor: 4
       kernel_size: 3
       class_prediction_bias_init: -4.599999904632568
     &#125;
   &#125;
   anchor_generator &#123;
     multiscale_anchor_generator &#123;
       min_level: 3
       max_level: 7
       anchor_scale: 4.0
       aspect_ratios: 1.0
       aspect_ratios: 2.0
       aspect_ratios: 0.5
       scales_per_octave: 2
     &#125;
   &#125;
   post_processing &#123;
     batch_non_max_suppression &#123;
       score_threshold: 9.99999993922529e-09
       iou_threshold: 0.6000000238418579
       max_detections_per_class: 100
       max_total_detections: 100
     &#125;
     score_converter: SIGMOID
   &#125;
   normalize_loss_by_num_matches: true
   loss &#123;
     localization_loss &#123;
       weighted_smooth_l1 &#123;
       &#125;
     &#125;
     classification_loss &#123;
       weighted_sigmoid_focal &#123;
         gamma: 2.0
         alpha: 0.25
       &#125;
     &#125;
     classification_weight: 1.0
     localization_weight: 1.0
   &#125;
   encode_background_as_zeros: true
   normalize_loc_loss_by_codesize: true
   inplace_batchnorm_update: true
   freeze_batchnorm: false
 &#125;,
 &#39;train_config&#39;: batch_size: 64
 data_augmentation_options &#123;
   random_horizontal_flip &#123;
   &#125;
 &#125;
 data_augmentation_options &#123;
   random_crop_image &#123;
     min_object_covered: 0.0
     min_aspect_ratio: 0.75
     max_aspect_ratio: 3.0
     min_area: 0.75
     max_area: 1.0
     overlap_thresh: 0.0
   &#125;
 &#125;
 sync_replicas: true
 optimizer &#123;
   momentum_optimizer &#123;
     learning_rate &#123;
       cosine_decay_learning_rate &#123;
         learning_rate_base: 0.03999999910593033
         total_steps: 25000
         warmup_learning_rate: 0.013333000242710114
         warmup_steps: 2000
       &#125;
     &#125;
     momentum_optimizer_value: 0.8999999761581421
   &#125;
   use_moving_average: false
 &#125;
 fine_tune_checkpoint: &quot;PATH_TO_BE_CONFIGURED/resnet50.ckpt-1&quot;
 num_steps: 25000
 startup_delay_steps: 0.0
 replicas_to_aggregate: 8
 max_number_of_boxes: 100
 unpad_groundtruth_tensors: false
 fine_tune_checkpoint_type: &quot;classification&quot;
 use_bfloat16: true
 fine_tune_checkpoint_version: V2,
 &#39;train_input_config&#39;: label_map_path: &quot;PATH_TO_BE_CONFIGURED/label_map.txt&quot;
 tf_record_input_reader &#123;
   input_path: &quot;PATH_TO_BE_CONFIGURED/train2017-?????-of-00256.tfrecord&quot;
 &#125;&#125;
</code></pre>
<p><a name='exercise-5-2'></a></p>
<h3 id="Exercise-5-2-Get-the-model-configuration"><a href="#Exercise-5-2-Get-the-model-configuration" class="headerlink" title="Exercise 5.2: Get the model configuration"></a><strong>Exercise 5.2</strong>: Get the model configuration</h3><h4 id="model-config"><a href="#model-config" class="headerlink" title="model_config"></a>model_config</h4><ul>
<li>From the <code>configs</code> dictionary, access the object associated with the key ‘model’.</li>
<li><code>model_config</code> now contains an object of type <code>object_detection.protos.model_pb2.DetectionModel</code>.  </li>
<li>If you print <code>model_config</code>, you’ll see something like this:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ssd &#123;</span><br><span class="line">  num_classes: 90</span><br><span class="line">  image_resizer &#123;</span><br><span class="line">    fixed_shape_resizer &#123;</span><br><span class="line">      height: 640</span><br><span class="line">      width: 640</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  feature_extractor &#123;</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">  freeze_batchnorm: false</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="comment"># Read in the object stored at the key &#x27;model&#x27; of the configs dictionary</span></span><br><span class="line">model_config = configs[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE</span></span><br><span class="line"><span class="comment"># see what model_config looks like</span></span><br><span class="line">model_config</span><br></pre></td></tr></table></figure>




<pre><code>ssd &#123;
  num_classes: 90
  image_resizer &#123;
    fixed_shape_resizer &#123;
      height: 640
      width: 640
    &#125;
  &#125;
  feature_extractor &#123;
    type: &quot;ssd_resnet50_v1_fpn_keras&quot;
    depth_multiplier: 1.0
    min_depth: 16
    conv_hyperparams &#123;
      regularizer &#123;
        l2_regularizer &#123;
          weight: 0.00039999998989515007
        &#125;
      &#125;
      initializer &#123;
        truncated_normal_initializer &#123;
          mean: 0.0
          stddev: 0.029999999329447746
        &#125;
      &#125;
      activation: RELU_6
      batch_norm &#123;
        decay: 0.996999979019165
        scale: true
        epsilon: 0.0010000000474974513
      &#125;
    &#125;
    override_base_feature_extractor_hyperparams: true
    fpn &#123;
      min_level: 3
      max_level: 7
    &#125;
  &#125;
  box_coder &#123;
    faster_rcnn_box_coder &#123;
      y_scale: 10.0
      x_scale: 10.0
      height_scale: 5.0
      width_scale: 5.0
    &#125;
  &#125;
  matcher &#123;
    argmax_matcher &#123;
      matched_threshold: 0.5
      unmatched_threshold: 0.5
      ignore_thresholds: false
      negatives_lower_than_unmatched: true
      force_match_for_each_row: true
      use_matmul_gather: true
    &#125;
  &#125;
  similarity_calculator &#123;
    iou_similarity &#123;
    &#125;
  &#125;
  box_predictor &#123;
    weight_shared_convolutional_box_predictor &#123;
      conv_hyperparams &#123;
        regularizer &#123;
          l2_regularizer &#123;
            weight: 0.00039999998989515007
          &#125;
        &#125;
        initializer &#123;
          random_normal_initializer &#123;
            mean: 0.0
            stddev: 0.009999999776482582
          &#125;
        &#125;
        activation: RELU_6
        batch_norm &#123;
          decay: 0.996999979019165
          scale: true
          epsilon: 0.0010000000474974513
        &#125;
      &#125;
      depth: 256
      num_layers_before_predictor: 4
      kernel_size: 3
      class_prediction_bias_init: -4.599999904632568
    &#125;
  &#125;
  anchor_generator &#123;
    multiscale_anchor_generator &#123;
      min_level: 3
      max_level: 7
      anchor_scale: 4.0
      aspect_ratios: 1.0
      aspect_ratios: 2.0
      aspect_ratios: 0.5
      scales_per_octave: 2
    &#125;
  &#125;
  post_processing &#123;
    batch_non_max_suppression &#123;
      score_threshold: 9.99999993922529e-09
      iou_threshold: 0.6000000238418579
      max_detections_per_class: 100
      max_total_detections: 100
    &#125;
    score_converter: SIGMOID
  &#125;
  normalize_loss_by_num_matches: true
  loss &#123;
    localization_loss &#123;
      weighted_smooth_l1 &#123;
      &#125;
    &#125;
    classification_loss &#123;
      weighted_sigmoid_focal &#123;
        gamma: 2.0
        alpha: 0.25
      &#125;
    &#125;
    classification_weight: 1.0
    localization_weight: 1.0
  &#125;
  encode_background_as_zeros: true
  normalize_loc_loss_by_codesize: true
  inplace_batchnorm_update: true
  freeze_batchnorm: false
&#125;
</code></pre>
<p><a name='exercise-5-3'></a></p>
<h3 id="Exercise-5-3-Modify-model-config"><a href="#Exercise-5-3-Modify-model-config" class="headerlink" title="Exercise 5.3: Modify model_config"></a><strong>Exercise 5.3</strong>: Modify model_config</h3><ul>
<li>Modify num_classes from the default <code>90</code> to the <code>num_classes</code> that you set earlier in this notebook.<ul>
<li>num_classes is nested under ssd.  You’ll need to use dot notation ‘obj.x’ and NOT bracket notation obj[‘x’]` to access num_classes.</li>
</ul>
</li>
<li>Freeze batch normalization <ul>
<li>Batch normalization is not frozen in the default configuration.</li>
<li>If you inspect the <code>model_config</code> object, you’ll see that <code>freeze_batchnorm</code> is nested under <code>ssd</code> just like <code>num_classes</code>.</li>
<li>Freeze batch normalization by setting the relevant field to <code>True</code>.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="comment"># Modify the number of classes from its default of 90</span></span><br><span class="line">model_config.ssd.num_classes = num_classes</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze batch normalization</span></span><br><span class="line">model_config.ssd.freeze_batchnorm = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># See what model_config now looks like after you&#x27;ve customized it!</span></span><br><span class="line">model_config</span><br></pre></td></tr></table></figure>




<pre><code>ssd &#123;
  num_classes: 1
  image_resizer &#123;
    fixed_shape_resizer &#123;
      height: 640
      width: 640
    &#125;
  &#125;
  feature_extractor &#123;
    type: &quot;ssd_resnet50_v1_fpn_keras&quot;
    depth_multiplier: 1.0
    min_depth: 16
    conv_hyperparams &#123;
      regularizer &#123;
        l2_regularizer &#123;
          weight: 0.00039999998989515007
        &#125;
      &#125;
      initializer &#123;
        truncated_normal_initializer &#123;
          mean: 0.0
          stddev: 0.029999999329447746
        &#125;
      &#125;
      activation: RELU_6
      batch_norm &#123;
        decay: 0.996999979019165
        scale: true
        epsilon: 0.0010000000474974513
      &#125;
    &#125;
    override_base_feature_extractor_hyperparams: true
    fpn &#123;
      min_level: 3
      max_level: 7
    &#125;
  &#125;
  box_coder &#123;
    faster_rcnn_box_coder &#123;
      y_scale: 10.0
      x_scale: 10.0
      height_scale: 5.0
      width_scale: 5.0
    &#125;
  &#125;
  matcher &#123;
    argmax_matcher &#123;
      matched_threshold: 0.5
      unmatched_threshold: 0.5
      ignore_thresholds: false
      negatives_lower_than_unmatched: true
      force_match_for_each_row: true
      use_matmul_gather: true
    &#125;
  &#125;
  similarity_calculator &#123;
    iou_similarity &#123;
    &#125;
  &#125;
  box_predictor &#123;
    weight_shared_convolutional_box_predictor &#123;
      conv_hyperparams &#123;
        regularizer &#123;
          l2_regularizer &#123;
            weight: 0.00039999998989515007
          &#125;
        &#125;
        initializer &#123;
          random_normal_initializer &#123;
            mean: 0.0
            stddev: 0.009999999776482582
          &#125;
        &#125;
        activation: RELU_6
        batch_norm &#123;
          decay: 0.996999979019165
          scale: true
          epsilon: 0.0010000000474974513
        &#125;
      &#125;
      depth: 256
      num_layers_before_predictor: 4
      kernel_size: 3
      class_prediction_bias_init: -4.599999904632568
    &#125;
  &#125;
  anchor_generator &#123;
    multiscale_anchor_generator &#123;
      min_level: 3
      max_level: 7
      anchor_scale: 4.0
      aspect_ratios: 1.0
      aspect_ratios: 2.0
      aspect_ratios: 0.5
      scales_per_octave: 2
    &#125;
  &#125;
  post_processing &#123;
    batch_non_max_suppression &#123;
      score_threshold: 9.99999993922529e-09
      iou_threshold: 0.6000000238418579
      max_detections_per_class: 100
      max_total_detections: 100
    &#125;
    score_converter: SIGMOID
  &#125;
  normalize_loss_by_num_matches: true
  loss &#123;
    localization_loss &#123;
      weighted_smooth_l1 &#123;
      &#125;
    &#125;
    classification_loss &#123;
      weighted_sigmoid_focal &#123;
        gamma: 2.0
        alpha: 0.25
      &#125;
    &#125;
    classification_weight: 1.0
    localization_weight: 1.0
  &#125;
  encode_background_as_zeros: true
  normalize_loc_loss_by_codesize: true
  inplace_batchnorm_update: true
  freeze_batchnorm: true
&#125;
</code></pre>
<h2 id="Build-the-model"><a href="#Build-the-model" class="headerlink" title="Build the model"></a>Build the model</h2><p>Recall that you imported <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/builders/model_builder.py">model_builder</a>.  </p>
<ul>
<li>You’ll use <code>model_builder</code> to build the model according to the configurations that you have just downloaded and customized.</li>
</ul>
<p><a name='exercise-5.4'></a></p>
<h3 id="Exercise-5-4-Build-the-custom-model"><a href="#Exercise-5-4-Build-the-custom-model" class="headerlink" title="Exercise 5.4: Build the custom model"></a><strong>Exercise 5.4</strong>: Build the custom model</h3><h4 id="model-builder"><a href="#model-builder" class="headerlink" title="model_builder"></a>model_builder</h4><p>model_builder has a function <code>build</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def build(model_config, is_training, add_summaries&#x3D;True):</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>model_config: Set this to the model configuration that you just customized.</li>
<li>is_training: Set this to True.</li>
<li>You can keep the default value for the remaining parameter.</li>
<li>Note that it will take some time to build the model.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE (Replace instances of `None` with your code) ###</span></span><br><span class="line">detection_model = model_builder.build(</span><br><span class="line">      model_config=model_config, is_training=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">print(<span class="built_in">type</span>(detection_model))</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch&#39;&gt;
</code></pre>
<p><strong>Expected Output</strong>:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#x27;object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch&#x27;&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Restore-weights-from-your-checkpoint"><a href="#Restore-weights-from-your-checkpoint" class="headerlink" title="Restore weights from your checkpoint"></a>Restore weights from your checkpoint</h2><p>Now, you will selectively restore weights from your checkpoint.</p>
<ul>
<li>Your end goal is to create a custom model which reuses parts of, but not all of the layers of RetinaNet (currently stored in the variable <code>detection_model</code>.)<ul>
<li>The parts of RetinaNet that you want to reuse are:<ul>
<li>Feature extraction layers</li>
<li>Bounding box regression prediction layer</li>
</ul>
</li>
<li>The part of RetinaNet that you will not want to reuse is the classification prediction layer (since you will define and train your own classification layer specific to zombies).</li>
<li>For the parts of RetinaNet that you want to reuse, you will also restore the weights from the checkpoint that you selected.</li>
</ul>
</li>
</ul>
<h4 id="Inspect-the-detection-model"><a href="#Inspect-the-detection-model" class="headerlink" title="Inspect the detection_model"></a>Inspect the detection_model</h4><p>First, take a look at the type of the detection_model and its Python class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this to check the type of detection_model</span></span><br><span class="line">detection_model</span><br></pre></td></tr></table></figure>




<pre><code>&lt;object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch at 0x7f818055d290&gt;
</code></pre>
<h4 id="Find-the-source-code-for-detection-model"><a href="#Find-the-source-code-for-detection-model" class="headerlink" title="Find the source code for detection_model"></a>Find the source code for detection_model</h4><p>You’ll see that the type of the model is <code>object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch</code>.<br>Please practice some detective work and open up the source code for this class in GitHub repository.  Recall that at the start of this assignment, you cloned from this repository:  <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models">TensorFlow Models</a>.</p>
<ul>
<li>Navigate through these subfolders: models -&gt; research -&gt; object_detection.<ul>
<li>If you get stuck, go to this link: <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/object_detection">object_detection</a></li>
</ul>
</li>
<li>Take a look at this ‘object_detection’ folder and look for the remaining folders to navigate based on the class type of detection_model: object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch<ul>
<li>Hopefully you’ll find the meta_architectures folder, and within it you’ll notice a file named <code>ssd_meta_arch.py</code>.</li>
<li>Please open and view this <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py">ssd_meta_arch.py</a> file.</li>
</ul>
</li>
</ul>
<h4 id="View-the-variables-in-detection-model"><a href="#View-the-variables-in-detection-model" class="headerlink" title="View the variables in detection_model"></a>View the variables in detection_model</h4><p>Now, check the class variables that are in <code>detection_model</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vars</span>(detection_model)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;_activity_regularizer&#39;: None,
 &#39;_add_background_class&#39;: True,
 &#39;_add_summaries&#39;: True,
 &#39;_anchor_generator&#39;: &lt;object_detection.anchor_generators.multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator at 0x7f8180623fd0&gt;,
 &#39;_anchors&#39;: None,
 &#39;_auto_track_sub_layers&#39;: True,
 &#39;_autocast&#39;: True,
 &#39;_batched_prediction_tensor_names&#39;: ListWrapper([]),
 &#39;_box_coder&#39;: &lt;object_detection.box_coders.faster_rcnn_box_coder.FasterRcnnBoxCoder at 0x7f81805901d0&gt;,
 &#39;_box_predictor&#39;: &lt;object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f8180430810&gt;,
 &#39;_build_input_shape&#39;: None,
 &#39;_callable_losses&#39;: [],
 &#39;_classification_loss&#39;: &lt;object_detection.core.losses.SigmoidFocalClassificationLoss at 0x7f81803f3350&gt;,
 &#39;_classification_loss_weight&#39;: 1.0,
 &#39;_compute_dtype_object&#39;: tf.float32,
 &#39;_default_training_arg&#39;: None,
 &#39;_dtype_policy&#39;: &lt;Policy &quot;float32&quot;&gt;,
 &#39;_dynamic&#39;: False,
 &#39;_equalization_loss_config&#39;: EqualizationLossConfig(weight=0.0, exclude_prefixes=[]),
 &#39;_expected_loss_weights_fn&#39;: None,
 &#39;_expects_mask_arg&#39;: False,
 &#39;_expects_training_arg&#39;: False,
 &#39;_explicit_background_class&#39;: False,
 &#39;_extract_features_scope&#39;: &#39;ResNet50V1_FPN&#39;,
 &#39;_feature_extractor&#39;: &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f81805753d0&gt;,
 &#39;_freeze_batchnorm&#39;: True,
 &#39;_groundtruth_lists&#39;: DictWrapper(&#123;&#125;),
 &#39;_hard_example_miner&#39;: None,
 &#39;_image_resizer_fn&#39;: functools.partial(&lt;function resize_image at 0x7f818e46b0e0&gt;, new_height=640, new_width=640, method=0),
 &#39;_implicit_example_weight&#39;: 1.0,
 &#39;_inbound_nodes_value&#39;: [],
 &#39;_initial_weights&#39;: None,
 &#39;_inplace_batchnorm_update&#39;: True,
 &#39;_input_spec&#39;: None,
 &#39;_instrumented_keras_api&#39;: True,
 &#39;_instrumented_keras_layer_class&#39;: True,
 &#39;_instrumented_keras_model_class&#39;: False,
 &#39;_is_training&#39;: True,
 &#39;_localization_loss&#39;: &lt;object_detection.core.losses.WeightedSmoothL1LocalizationLoss at 0x7f81803f3390&gt;,
 &#39;_localization_loss_weight&#39;: 1.0,
 &#39;_losses&#39;: [],
 &#39;_metrics&#39;: [],
 &#39;_metrics_lock&#39;: &lt;unlocked _thread.lock object at 0x7f8180570bd0&gt;,
 &#39;_name&#39;: &#39;ssd_meta_arch&#39;,
 &#39;_non_max_suppression_fn&#39;: functools.partial(&lt;function batch_multiclass_non_max_suppression at 0x7f818e416dd0&gt;, score_thresh=9.99999993922529e-09, iou_thresh=0.6000000238418579, max_size_per_class=100, max_total_size=100, use_static_shapes=False, use_class_agnostic_nms=False, max_classes_per_detection=1, soft_nms_sigma=0.0, use_partitioned_nms=False, use_combined_nms=False, change_coordinate_frame=True, use_hard_nms=False, use_cpu_nms=False),
 &#39;_non_trainable_weights&#39;: [],
 &#39;_normalize_loc_loss_by_codesize&#39;: True,
 &#39;_normalize_loss_by_num_matches&#39;: True,
 &#39;_num_classes&#39;: 1,
 &#39;_obj_reference_counts_dict&#39;: ObjectIdentityDictionary(&#123;&lt;_ObjectIdentityWrapper wrapping 1&gt;: 1, &lt;_ObjectIdentityWrapper wrapping DictWrapper(&#123;&#125;)&gt;: 1, &lt;_ObjectIdentityWrapper wrapping True&gt;: 7, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.anchor_generators.multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator object at 0x7f8180623fd0&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor object at 0x7f8180430810&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.box_coders.faster_rcnn_box_coder.FasterRcnnBoxCoder object at 0x7f81805901d0&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor object at 0x7f81805753d0&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping False&gt;: 3, &lt;_ObjectIdentityWrapper wrapping &#39;ResNet50V1_FPN&#39;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.core.target_assigner.TargetAssigner object at 0x7f81803f3410&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.core.losses.SigmoidFocalClassificationLoss object at 0x7f81803f3350&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.core.losses.WeightedSmoothL1LocalizationLoss object at 0x7f81803f3390&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping 1.0&gt;: 1, &lt;_ObjectIdentityWrapper wrapping 1.0&gt;: 1, &lt;_ObjectIdentityWrapper wrapping 16&gt;: 1, &lt;_ObjectIdentityWrapper wrapping functools.partial(&lt;function resize_image at 0x7f818e46b0e0&gt;, new_height=640, new_width=640, method=0)&gt;: 1, &lt;_ObjectIdentityWrapper wrapping functools.partial(&lt;function batch_multiclass_non_max_suppression at 0x7f818e416dd0&gt;, score_thresh=9.99999993922529e-09, iou_thresh=0.6000000238418579, max_size_per_class=100, max_total_size=100, use_static_shapes=False, use_class_agnostic_nms=False, max_classes_per_detection=1, soft_nms_sigma=0.0, use_partitioned_nms=False, use_combined_nms=False, change_coordinate_frame=True, use_hard_nms=False, use_cpu_nms=False)&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;function _score_converter_fn_with_logit_scale.&lt;locals&gt;.score_converter_fn at 0x7f81803f23b0&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping ListWrapper([])&gt;: 1, &lt;_ObjectIdentityWrapper wrapping 1.0&gt;: 1, &lt;_ObjectIdentityWrapper wrapping EqualizationLossConfig(weight=0.0, exclude_prefixes=[])&gt;: 1&#125;),
 &#39;_outbound_nodes_value&#39;: [],
 &#39;_parallel_iterations&#39;: 16,
 &#39;_preserve_input_structure_in_config&#39;: False,
 &#39;_random_example_sampler&#39;: None,
 &#39;_return_raw_detections_during_predict&#39;: False,
 &#39;_saved_model_inputs_spec&#39;: None,
 &#39;_score_conversion_fn&#39;: &lt;function object_detection.builders.post_processing_builder._score_converter_fn_with_logit_scale.&lt;locals&gt;.score_converter_fn&gt;,
 &#39;_self_name_based_restores&#39;: set(),
 &#39;_self_saveable_object_factories&#39;: &#123;&#125;,
 &#39;_self_setattr_tracking&#39;: True,
 &#39;_self_tracked_trackables&#39;: [DictWrapper(&#123;&#125;),
  &lt;object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f8180430810&gt;,
  &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f81805753d0&gt;,
  ListWrapper([])],
 &#39;_self_unconditional_checkpoint_dependencies&#39;: [TrackableReference(name=&#39;_groundtruth_lists&#39;, ref=DictWrapper(&#123;&#125;)),
  TrackableReference(name=&#39;_box_predictor&#39;, ref=&lt;object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor object at 0x7f8180430810&gt;),
  TrackableReference(name=&#39;_feature_extractor&#39;, ref=&lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor object at 0x7f81805753d0&gt;),
  TrackableReference(name=&#39;_batched_prediction_tensor_names&#39;, ref=ListWrapper([]))],
 &#39;_self_unconditional_deferred_dependencies&#39;: &#123;&#125;,
 &#39;_self_unconditional_dependency_names&#39;: &#123;&#39;_batched_prediction_tensor_names&#39;: ListWrapper([]),
  &#39;_box_predictor&#39;: &lt;object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f8180430810&gt;,
  &#39;_feature_extractor&#39;: &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f81805753d0&gt;,
  &#39;_groundtruth_lists&#39;: DictWrapper(&#123;&#125;)&#125;,
 &#39;_self_update_uid&#39;: -1,
 &#39;_stateful&#39;: False,
 &#39;_supports_masking&#39;: False,
 &#39;_target_assigner&#39;: &lt;object_detection.core.target_assigner.TargetAssigner at 0x7f81803f3410&gt;,
 &#39;_thread_local&#39;: &lt;_thread._local at 0x7f81803f8290&gt;,
 &#39;_trainable&#39;: True,
 &#39;_trainable_weights&#39;: [],
 &#39;_unmatched_class_label&#39;: &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;,
 &#39;_updates&#39;: [],
 &#39;_use_confidences_as_targets&#39;: False,
 &#39;built&#39;: False&#125;
</code></pre>
<p>You’ll see that detection_model contains several variables:</p>
<p>Two of these will be relevant to you:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">_box_predictor&#39;: &lt;object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f5205eeb1d0&gt;,</span><br><span class="line">...</span><br><span class="line">_feature_extractor&#39;: &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f52040f1ef0&gt;,</span><br></pre></td></tr></table></figure>

<h4 id="Inspect-feature-extractor"><a href="#Inspect-feature-extractor" class="headerlink" title="Inspect _feature_extractor"></a>Inspect <code>_feature_extractor</code></h4><p>Take a look at the <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py">ssd_meta_arch.py</a> code.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Line 302</span><br><span class="line">feature_extractor: a SSDFeatureExtractor object.</span><br></pre></td></tr></table></figure>
<p>Also</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Line 380</span><br><span class="line">self._feature_extractor &#x3D; feature_extractor</span><br></pre></td></tr></table></figure>
<p>So <code>detection_model._feature_extractor</code> is a feature extractor, which you will want to reuse for your zombie detector model.</p>
<h4 id="Inspect-box-predictor"><a href="#Inspect-box-predictor" class="headerlink" title="Inspect _box_predictor"></a>Inspect <code>_box_predictor</code></h4><ul>
<li>View the <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py">ssd_meta_arch.py</a> file (which is the source code for detection_model)</li>
<li>Notice that in the <strong>init</strong> constructor for class SSDMetaArch(model.DetectionModel), <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">box_predictor: a box_predictor.BoxPredictor object</span><br><span class="line">...</span><br><span class="line">self._box_predictor &#x3D; box_predictor</span><br></pre></td></tr></table></figure>
<h4 id="Inspect-box-predictor-1"><a href="#Inspect-box-predictor-1" class="headerlink" title="Inspect _box_predictor"></a>Inspect _box_predictor</h4>Please take a look at the class type of <code>detection_model._box_predictor</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># view the type of _box_predictor</span></span><br><span class="line">detection_model._box_predictor</span><br></pre></td></tr></table></figure>




<pre><code>&lt;object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f8180430810&gt;
</code></pre>
<p>You’ll see that the class type of _box_predictor is</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor</span><br></pre></td></tr></table></figure>
<p>You can navigate through the GitHub repository to this path:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/object_detection/predictors">objection_detection/predictors</a></li>
<li>Notice that there is a file named convolutional_keras_box_predictor.py.  Please open that file.</li>
</ul>
<h4 id="View-variables-in-box-predictor"><a href="#View-variables-in-box-predictor" class="headerlink" title="View variables in _box_predictor"></a>View variables in <code>_box_predictor</code></h4><p>Also view the variables contained in _box_predictor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vars</span>(detection_model._box_predictor)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;_activity_regularizer&#39;: None,
 &#39;_additional_projection_layers&#39;: ListWrapper([]),
 &#39;_apply_batch_norm&#39;: True,
 &#39;_apply_conv_hyperparams_pointwise&#39;: False,
 &#39;_auto_track_sub_layers&#39;: True,
 &#39;_autocast&#39;: True,
 &#39;_base_tower_layers_for_heads&#39;: DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;),
 &#39;_box_prediction_head&#39;: &lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7f818049e9d0&gt;,
 &#39;_build_input_shape&#39;: None,
 &#39;_callable_losses&#39;: [],
 &#39;_compute_dtype_object&#39;: tf.float32,
 &#39;_conv_hyperparams&#39;: &lt;object_detection.builders.hyperparams_builder.KerasLayerHyperparams at 0x7f818e1a4f10&gt;,
 &#39;_default_training_arg&#39;: None,
 &#39;_depth&#39;: 256,
 &#39;_dtype_policy&#39;: &lt;Policy &quot;float32&quot;&gt;,
 &#39;_dynamic&#39;: False,
 &#39;_expects_mask_arg&#39;: True,
 &#39;_expects_training_arg&#39;: True,
 &#39;_freeze_batchnorm&#39;: True,
 &#39;_head_scope_conv_layers&#39;: DictWrapper(&#123;&#125;),
 &#39;_inbound_nodes_value&#39;: [],
 &#39;_initial_weights&#39;: None,
 &#39;_inplace_batchnorm_update&#39;: False,
 &#39;_input_spec&#39;: None,
 &#39;_instrumented_keras_api&#39;: True,
 &#39;_instrumented_keras_layer_class&#39;: True,
 &#39;_instrumented_keras_model_class&#39;: False,
 &#39;_is_training&#39;: True,
 &#39;_kernel_size&#39;: 3,
 &#39;_losses&#39;: [],
 &#39;_metrics&#39;: [],
 &#39;_metrics_lock&#39;: &lt;unlocked _thread.lock object at 0x7f8180570600&gt;,
 &#39;_name&#39;: &#39;WeightSharedConvolutionalBoxPredictor&#39;,
 &#39;_non_trainable_weights&#39;: [],
 &#39;_num_classes&#39;: 1,
 &#39;_num_layers_before_predictor&#39;: 4,
 &#39;_obj_reference_counts_dict&#39;: ObjectIdentityDictionary(&#123;&lt;_ObjectIdentityWrapper wrapping True&gt;: 3, &lt;_ObjectIdentityWrapper wrapping 1&gt;: 1, &lt;_ObjectIdentityWrapper wrapping False&gt;: 4, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead object at 0x7f818049e9d0&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping DictWrapper(&#123;&#39;class_predictions_with_background&#39;: &lt;object_detection.predictors.heads.keras_class_head.WeightSharedConvolutionalClassHead object at 0x7f81805c6550&gt;&#125;)&gt;: 1, &lt;_ObjectIdentityWrapper wrapping ListWrapper([&#39;class_predictions_with_background&#39;])&gt;: 1, &lt;_ObjectIdentityWrapper wrapping &lt;object_detection.builders.hyperparams_builder.KerasLayerHyperparams object at 0x7f818e1a4f10&gt;&gt;: 1, &lt;_ObjectIdentityWrapper wrapping 256&gt;: 1, &lt;_ObjectIdentityWrapper wrapping 4&gt;: 1, &lt;_ObjectIdentityWrapper wrapping 3&gt;: 1, &lt;_ObjectIdentityWrapper wrapping ListWrapper([])&gt;: 1, &lt;_ObjectIdentityWrapper wrapping DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;)&gt;: 1, &lt;_ObjectIdentityWrapper wrapping DictWrapper(&#123;&#125;)&gt;: 1&#125;),
 &#39;_outbound_nodes_value&#39;: [],
 &#39;_prediction_heads&#39;: DictWrapper(&#123;&#39;class_predictions_with_background&#39;: &lt;object_detection.predictors.heads.keras_class_head.WeightSharedConvolutionalClassHead object at 0x7f81805c6550&gt;&#125;),
 &#39;_preserve_input_structure_in_config&#39;: False,
 &#39;_saved_model_inputs_spec&#39;: None,
 &#39;_self_name_based_restores&#39;: set(),
 &#39;_self_saveable_object_factories&#39;: &#123;&#125;,
 &#39;_self_setattr_tracking&#39;: True,
 &#39;_self_tracked_trackables&#39;: [&lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7f818049e9d0&gt;,
  DictWrapper(&#123;&#39;class_predictions_with_background&#39;: &lt;object_detection.predictors.heads.keras_class_head.WeightSharedConvolutionalClassHead object at 0x7f81805c6550&gt;&#125;),
  ListWrapper([&#39;class_predictions_with_background&#39;]),
  ListWrapper([]),
  DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;),
  DictWrapper(&#123;&#125;)],
 &#39;_self_unconditional_checkpoint_dependencies&#39;: [TrackableReference(name=&#39;_box_prediction_head&#39;, ref=&lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead object at 0x7f818049e9d0&gt;),
  TrackableReference(name=&#39;_prediction_heads&#39;, ref=DictWrapper(&#123;&#39;class_predictions_with_background&#39;: &lt;object_detection.predictors.heads.keras_class_head.WeightSharedConvolutionalClassHead object at 0x7f81805c6550&gt;&#125;)),
  TrackableReference(name=&#39;_sorted_head_names&#39;, ref=ListWrapper([&#39;class_predictions_with_background&#39;])),
  TrackableReference(name=&#39;_additional_projection_layers&#39;, ref=ListWrapper([])),
  TrackableReference(name=&#39;_base_tower_layers_for_heads&#39;, ref=DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;)),
  TrackableReference(name=&#39;_head_scope_conv_layers&#39;, ref=DictWrapper(&#123;&#125;))],
 &#39;_self_unconditional_deferred_dependencies&#39;: &#123;&#125;,
 &#39;_self_unconditional_dependency_names&#39;: &#123;&#39;_additional_projection_layers&#39;: ListWrapper([]),
  &#39;_base_tower_layers_for_heads&#39;: DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;),
  &#39;_box_prediction_head&#39;: &lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7f818049e9d0&gt;,
  &#39;_head_scope_conv_layers&#39;: DictWrapper(&#123;&#125;),
  &#39;_prediction_heads&#39;: DictWrapper(&#123;&#39;class_predictions_with_background&#39;: &lt;object_detection.predictors.heads.keras_class_head.WeightSharedConvolutionalClassHead object at 0x7f81805c6550&gt;&#125;),
  &#39;_sorted_head_names&#39;: ListWrapper([&#39;class_predictions_with_background&#39;])&#125;,
 &#39;_self_update_uid&#39;: -1,
 &#39;_share_prediction_tower&#39;: False,
 &#39;_sorted_head_names&#39;: ListWrapper([&#39;class_predictions_with_background&#39;]),
 &#39;_stateful&#39;: False,
 &#39;_supports_masking&#39;: False,
 &#39;_thread_local&#39;: &lt;_thread._local at 0x7f81803d0e90&gt;,
 &#39;_trainable&#39;: True,
 &#39;_trainable_weights&#39;: [],
 &#39;_updates&#39;: [],
 &#39;_use_depthwise&#39;: False,
 &#39;built&#39;: False&#125;
</code></pre>
<p>Among the variables listed, a few will be relevant to you:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">_base_tower_layers_for_heads</span><br><span class="line">...</span><br><span class="line">_box_prediction_head</span><br><span class="line">...</span><br><span class="line">_prediction_heads</span><br></pre></td></tr></table></figure>

<p>In the source code for <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py">convolutional_keras_box_predictor.py</a> that you just opened, look at the source code to get a sense for what these three variables represent.</p>
<h4 id="Inspect-base-tower-layers-for-heads"><a href="#Inspect-base-tower-layers-for-heads" class="headerlink" title="Inspect base_tower_layers_for_heads"></a>Inspect <code>base_tower_layers_for_heads</code></h4><p>If you look at the <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py">convolutional_keras_box_predictor.py</a> file, you’ll notice this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># line 302</span><br><span class="line">self._base_tower_layers_for_heads &#x3D; &#123;</span><br><span class="line">        BOX_ENCODINGS: [],</span><br><span class="line">        CLASS_PREDICTIONS_WITH_BACKGROUND: [],</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>base_tower_layers_for_heads</code> is a dictionary with two key-value pairs.<ul>
<li><code>BOX_ENCODINGS</code>: points to a list of layers</li>
<li><code>CLASS_PREDICTIONS_WITH_BACKGROUND</code>: points to a list of layers</li>
<li>If you scan the code, you’ll see that for both of these, the lists are filled with all layers that appear BEFORE the prediction layer.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Line 377</span><br><span class="line"># Stack the base_tower_layers in the order of conv_layer, batch_norm_layer</span><br><span class="line">    # and activation_layer</span><br><span class="line">    base_tower_layers &#x3D; []</span><br><span class="line">    for i in range(self._num_layers_before_predictor):</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>So <code>detection_model.box_predictor._base_tower_layers_for_heads</code> contains:</p>
<ul>
<li>The layers for the prediction before the final bounding box prediction</li>
<li>The layers for the prediction before the final class prediction.</li>
</ul>
<p>You will want to use these in your model.</p>
<h4 id="Inspect-box-prediction-head"><a href="#Inspect-box-prediction-head" class="headerlink" title="Inspect _box_prediction_head"></a>Inspect <code>_box_prediction_head</code></h4><p>If you again look at <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py">convolutional_keras_box_predictor.py</a> file, you’ll see this</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Line 248</span><br><span class="line">box_prediction_head: The head that predicts the boxes.</span><br></pre></td></tr></table></figure>
<p>So <code>detection_model.box_predictor._box_prediction_head</code> points to the bounding box prediction layer, which you’ll want to use for your model.</p>
<h4 id="Inspect-prediction-heads"><a href="#Inspect-prediction-heads" class="headerlink" title="Inspect _prediction_heads"></a>Inspect <code>_prediction_heads</code></h4><p>If you again look at <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py">convolutional_keras_box_predictor.py</a> file, you’ll see this</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Line 121</span><br><span class="line">self._prediction_heads &#x3D; &#123;</span><br><span class="line">        BOX_ENCODINGS: box_prediction_heads,</span><br><span class="line">        CLASS_PREDICTIONS_WITH_BACKGROUND: class_prediction_heads,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>You’ll also see this docstring</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Line 83</span><br><span class="line">class_prediction_heads: A list of heads that predict the classes.</span><br></pre></td></tr></table></figure>

<p>So <code>detection_model.box_predictor._prediction_heads</code> is a dictionary that points to both prediction layers:</p>
<ul>
<li>The layer that predicts the bounding boxes</li>
<li>The layer that predicts the class (category).</li>
</ul>
<h4 id="Which-layers-will-you-reuse"><a href="#Which-layers-will-you-reuse" class="headerlink" title="Which layers will you reuse?"></a>Which layers will you reuse?</h4><p>Remember that you are reusing the model for its feature extraction and bounding box detection.</p>
<ul>
<li>You will create your own classification layer and train it on zombie images.</li>
<li>So you won’t need to reuse the class prediction layer of <code>detection_model</code>.</li>
</ul>
<h2 id="Define-checkpoints-for-desired-layers"><a href="#Define-checkpoints-for-desired-layers" class="headerlink" title="Define checkpoints for desired layers"></a>Define checkpoints for desired layers</h2><p>You will now isolate the layers of <code>detection_model</code> that you wish to reuse so that you can restore the weights to just those layers.</p>
<ul>
<li>First, define checkpoints for the box predictor</li>
<li>Next, define checkpoints for the model, which will point to this box predictor checkpoint as well as the feature extraction layers.</li>
</ul>
<p>Please use <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint">tf.train.Checkpoint</a>.</p>
<p>As a reminder of how to use tf.train.Checkpoint:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.train.Checkpoint(</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Pretend that <code>detection_model</code> contains these variables for which you want to restore weights:</p>
<ul>
<li><code>detection_model._ice_cream_sundae</code></li>
<li>‘detection_model._pies._apple_pie`</li>
<li>‘detection_model._pies._pecan_pie`</li>
</ul>
<p>Notice that the pies are nested within <code>._pies</code>.</p>
<p>If you just want the ice cream sundae and apple pie variables (and not the pecan pie) then you can do the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tmp_pies_checkpoint &#x3D; tf.train.Checkpoint(</span><br><span class="line">  _apple_pie &#x3D; detection_model._pies._apple_pie</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Next, in order to connect these together in a node graph, do this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmp_model_checkpoint &#x3D; tf.train.Checkpoint(</span><br><span class="line">  _pies &#x3D; tmp_pies_checkpoint,</span><br><span class="line">  _ice_cream_sundae &#x3D; detection_model._ice_cream_sundae</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Finally, define a checkpoint that uses the key <code>model</code> and takes in the tmp_model_checkpoint.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">checkpoint &#x3D; tf.train.Checkpoint(</span><br><span class="line">  model &#x3D; tmp_model_checkpoint</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>You’ll then be ready to restore the weights from the checkpoint that you downloaded.</p>
<p>Try this out step by step!</p>
<p><a name='exercise-6-1'></a></p>
<h3 id="Exercise-6-1-Define-Checkpoints-for-the-box-predictor"><a href="#Exercise-6-1-Define-Checkpoints-for-the-box-predictor" class="headerlink" title="Exercise 6.1: Define Checkpoints for the box predictor"></a>Exercise 6.1: Define Checkpoints for the box predictor</h3><ul>
<li>Please define <code>box_predictor_checkpoint</code> to be checkpoint for these two layers of the <code>detection_model</code>‘s box predictor:<ul>
<li>The base tower layer (the layers the precede both the class prediction and bounding box prediction layers).</li>
<li>The box prediction head (the prediction layer for bounding boxes).</li>
</ul>
</li>
<li>Note, you won’t include the class prediction layer.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">tmp_box_predictor_checkpoint = tf.compat.v2.train.Checkpoint(</span><br><span class="line">    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,</span><br><span class="line">    <span class="comment"># _prediction_heads=detection_model._box_predictor._prediction_heads,</span></span><br><span class="line">    <span class="comment">#    (i.e., the classification head that we *will not* restore)</span></span><br><span class="line">    _box_prediction_head=detection_model._box_predictor._box_prediction_head,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the datatype of this checkpoint</span></span><br><span class="line"><span class="built_in">type</span>(tmp_box_predictor_checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Expected output:</span></span><br><span class="line"><span class="comment"># tensorflow.python.training.tracking.util.Checkpoint</span></span><br></pre></td></tr></table></figure>




<pre><code>tensorflow.python.training.tracking.util.Checkpoint
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the variables of this checkpoint</span></span><br><span class="line"><span class="built_in">vars</span>(tmp_box_predictor_checkpoint)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;_attached_dependencies&#39;: None,
 &#39;_base_tower_layers_for_heads&#39;: DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;),
 &#39;_box_prediction_head&#39;: &lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7f818049e9d0&gt;,
 &#39;_save_assign_op&#39;: None,
 &#39;_save_counter&#39;: None,
 &#39;_saver&#39;: &lt;tensorflow.python.training.tracking.util.TrackableSaver at 0x7f8180498750&gt;,
 &#39;_self_name_based_restores&#39;: set(),
 &#39;_self_saveable_object_factories&#39;: &#123;&#125;,
 &#39;_self_setattr_tracking&#39;: True,
 &#39;_self_unconditional_checkpoint_dependencies&#39;: [TrackableReference(name=&#39;_base_tower_layers_for_heads&#39;, ref=DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;)),
  TrackableReference(name=&#39;_box_prediction_head&#39;, ref=&lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead object at 0x7f818049e9d0&gt;)],
 &#39;_self_unconditional_deferred_dependencies&#39;: &#123;&#125;,
 &#39;_self_unconditional_dependency_names&#39;: &#123;&#39;_base_tower_layers_for_heads&#39;: DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;),
  &#39;_box_prediction_head&#39;: &lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7f818049e9d0&gt;&#125;,
 &#39;_self_update_uid&#39;: -1&#125;
</code></pre>
<h4 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h4><p>You should expect to see a list of variables that include the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#39;_base_tower_layers_for_heads&#39;: DictWrapper(&#123;&#39;box_encodings&#39;: ListWrapper([]), &#39;class_predictions_with_background&#39;: ListWrapper([])&#125;),</span><br><span class="line">&#39;_box_prediction_head&#39;: &lt;object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7fefac014710&gt;,</span><br><span class="line"> ... </span><br></pre></td></tr></table></figure>

<p><a name='exercise-6-2'></a></p>
<h3 id="Exercise-6-2-Define-the-temporary-model-checkpoint"><a href="#Exercise-6-2-Define-the-temporary-model-checkpoint" class="headerlink" title="Exercise 6.2: Define the temporary model checkpoint**"></a>Exercise 6.2: Define the temporary model checkpoint**</h3><p>Now define <code>tmp_model_checkpoint</code> so that it points to these two layers:</p>
<ul>
<li>The feature extractor of the detection model.</li>
<li>The temporary box predictor checkpoint that you just defined.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">tmp_model_checkpoint = tf.compat.v2.train.Checkpoint(</span><br><span class="line">          _feature_extractor=detection_model._feature_extractor,</span><br><span class="line">          _box_predictor=tmp_box_predictor_checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tmp_model_checkpoint = tf.compat.v2.train.Checkpoint(model=tmp_model_checkpoint)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the datatype of this checkpoint</span></span><br><span class="line"><span class="built_in">type</span>(tmp_model_checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Expected output</span></span><br><span class="line"><span class="comment"># tensorflow.python.training.tracking.util.Checkpoint</span></span><br></pre></td></tr></table></figure>




<pre><code>tensorflow.python.training.tracking.util.Checkpoint
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the vars of this checkpoint</span></span><br><span class="line"><span class="built_in">vars</span>(tmp_model_checkpoint)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;_attached_dependencies&#39;: None,
 &#39;_box_predictor&#39;: &lt;tensorflow.python.training.tracking.util.Checkpoint at 0x7f8180498cd0&gt;,
 &#39;_feature_extractor&#39;: &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f81805753d0&gt;,
 &#39;_save_assign_op&#39;: None,
 &#39;_save_counter&#39;: None,
 &#39;_saver&#39;: &lt;tensorflow.python.training.tracking.util.TrackableSaver at 0x7f8180374350&gt;,
 &#39;_self_name_based_restores&#39;: set(),
 &#39;_self_saveable_object_factories&#39;: &#123;&#125;,
 &#39;_self_setattr_tracking&#39;: True,
 &#39;_self_unconditional_checkpoint_dependencies&#39;: [TrackableReference(name=&#39;_box_predictor&#39;, ref=&lt;tensorflow.python.training.tracking.util.Checkpoint object at 0x7f8180498cd0&gt;),
  TrackableReference(name=&#39;_feature_extractor&#39;, ref=&lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor object at 0x7f81805753d0&gt;)],
 &#39;_self_unconditional_deferred_dependencies&#39;: &#123;&#125;,
 &#39;_self_unconditional_dependency_names&#39;: &#123;&#39;_box_predictor&#39;: &lt;tensorflow.python.training.tracking.util.Checkpoint at 0x7f8180498cd0&gt;,
  &#39;_feature_extractor&#39;: &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f81805753d0&gt;&#125;,
 &#39;_self_update_uid&#39;: -1&#125;
</code></pre>
<h4 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h4><p>Among the variables of this checkpoint, you should see:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#39;_box_predictor&#39;: &lt;tensorflow.python.training.tracking.util.Checkpoint at 0x7fefac044a20&gt;,</span><br><span class="line"> &#39;_feature_extractor&#39;: &lt;object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7fefac0240b8&gt;,</span><br></pre></td></tr></table></figure>

<p><a name='exercise-6-3'></a></p>
<h3 id="Exercise-6-3-Restore-the-checkpoint"><a href="#Exercise-6-3-Restore-the-checkpoint" class="headerlink" title="Exercise 6.3: Restore the checkpoint"></a>Exercise 6.3: Restore the checkpoint</h3><p>You can now restore the checkpoint.</p>
<p>First, find and set the <code>checkpoint_path</code></p>
<ul>
<li>checkpoint_path: <ul>
<li>Using the “files” browser in the left side of Colab, navigate to <code>models -&gt; research -&gt; object_detection -&gt; test_data</code>. </li>
<li>If you completed the previous code cell that downloads and moves the checkpoint, you’ll see a subfolder named “checkpoint”.  <ul>
<li>The ‘checkpoint’ folder contains three files:<ul>
<li>checkpoint</li>
<li>ckpt-0.data-00000-of-00001</li>
<li>ckpt-0.index</li>
</ul>
</li>
<li>Please set checkpoint_path to the path to the full path <code>models/.../ckpt-0</code> <ul>
<li>Notice that you don’t want to include a file extension after <code>ckpt-0</code>.</li>
</ul>
</li>
<li><strong>IMPORTANT</strong>: Please don’t set the path to include the <code>.index</code> extension in the checkpoint file name.  <ul>
<li>If you do set it to <code>ckpt-0.index</code>, there won’t be any immediate error message, but later during training, you’ll notice that your model’s loss doesn’t improve, which means that the pre-trained weights were not restored properly.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Next, define one last checkpoint using <code>tf.train.Checkpoint()</code>.</p>
<ul>
<li>For the single keyword argument, <ul>
<li>Set the key as <code>model=</code> </li>
<li>Set the value to your temporary model checkpoint that you just defined.</li>
</ul>
</li>
<li><strong>IMPORTANT</strong>: You’ll need to set the keyword argument as <code>model=</code> and not something else like <code>detection_model=</code>.</li>
<li>If you set this keyword argument to anything else, it won’t show an immmediate error, but when you train your model on the zombie images, your model loss will not decrease (your model will not learn).</li>
</ul>
<p>Finally, call this checkpoint’s <code>.restore()</code> function, passing in the path to the checkpoint.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">checkpoint_path = <span class="string">&#x27;/content/models/research/object_detection/test_data/checkpoint/ckpt-0&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a checkpoint that sets `model= None</span></span><br><span class="line">checkpoint = tf.compat.v2.train.Checkpoint(model=tmp_model_checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Restore the checkpoint to the checkpoint path</span></span><br><span class="line">checkpoint.restore(checkpoint_path).expect_partial()</span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8180388650&gt;
</code></pre>
<p><a name='exercise-7'></a></p>
<h3 id="Exercise-7-Run-a-dummy-image-to-generate-the-model-variables"><a href="#Exercise-7-Run-a-dummy-image-to-generate-the-model-variables" class="headerlink" title="Exercise 7: Run a dummy image to generate the model variables"></a><strong>Exercise 7</strong>: Run a dummy image to generate the model variables</h3><p>Run a dummy image through the model so that variables are created. We need to select the trainable variables later in Exercise 9 and right now, it is still empty. Try running <code>len(detection_model.trainable_variables)</code> in a code cell and you will get <code>0</code>. We will pass in a dummy image through the forward pass to create these variables.</p>
<p>Recall that <code>detection_model</code> is an object of type <a href="">object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch</a> </p>
<p>Important methods that are available in the <code>detection_model</code> object  are:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L459">preprocess()</a>: </p>
<ul>
<li>takes in a tensor representing an image and returns</li>
<li>returns <code>image, shapes</code></li>
<li>For the dummy image, you can declare a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/zeros">tensor of zeros</a> that has a shape that the <code>preprocess()</code> method can accept (i.e. [batch, height, width, channels]). </li>
<li>Remember that your images have dimensions 640 x 640 x 3. </li>
<li>You can pass in a batch of 1 when making the dummy image. </li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L525">predict()</a></p>
<ul>
<li>takes in <code>image, shapes</code> which are created by the <code>preprocess()</code> function call.</li>
<li>returns a prediction in a Python dictionary</li>
<li>this will pass the dummy image through the forward pass of the network and create the model variables</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L655">postprocess()</a></p>
<ul>
<li>Takes in the prediction_dict and shapes</li>
<li>returns a dictionary of post-processed predictions of detected objects (“detections”).</li>
</ul>
</li>
</ul>
<p><strong>Note</strong>: Please use the recommended variable names, which include the prefix <code>tmp_</code>, since these variables won’t be used later, but you’ll define similarly-named variables later for predicting on actual zombie images. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE (Replace instances of `None` with your code)###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use the detection model&#x27;s `preprocess()` method and pass a dummy image</span></span><br><span class="line">tmp_image, tmp_shapes = detection_model.preprocess(tf.zeros([<span class="number">1</span>, <span class="number">640</span>, <span class="number">640</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># run a prediction with the preprocessed image and shapes</span></span><br><span class="line">tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># postprocess the predictions into final detections</span></span><br><span class="line">tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)</span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Weights restored!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Weights restored!
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test Code:</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(detection_model.trainable_variables) &gt; <span class="number">0</span>, <span class="string">&quot;Please pass in a dummy image to create the trainable variables.&quot;</span></span><br><span class="line"></span><br><span class="line">print(detection_model.weights[<span class="number">0</span>].shape)</span><br><span class="line">print(detection_model.weights[<span class="number">231</span>].shape)</span><br><span class="line">print(detection_model.weights[<span class="number">462</span>].shape)</span><br></pre></td></tr></table></figure>

<pre><code>(3, 3, 256, 24)
(512,)
(256,)
</code></pre>
<p><strong>Expected Output</strong>:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(3, 3, 256, 24)</span><br><span class="line">(512,)</span><br><span class="line">(256,)</span><br></pre></td></tr></table></figure>

<h2 id="Eager-mode-custom-training-loop"><a href="#Eager-mode-custom-training-loop" class="headerlink" title="Eager mode custom training loop"></a>Eager mode custom training loop</h2><p>With the data and model now setup, you can now proceed to configure the training.</p>
<p><a name='exercise-8'></a></p>
<h3 id="Exercise-8-Set-training-hyperparameters"><a href="#Exercise-8-Set-training-hyperparameters" class="headerlink" title="Exercise 8: Set training hyperparameters"></a><strong>Exercise 8</strong>: Set training hyperparameters</h3><p>Set an appropriate learning rate and optimizer for the training. </p>
<ul>
<li>batch_size: you can use 4<ul>
<li>You can increase the batch size up to 5, since you have just 5 images for training.</li>
</ul>
</li>
<li>num_batches: You can use 100<ul>
<li>You can increase the number of batches but the training will take longer to complete. </li>
</ul>
</li>
<li>learning_rate: You can use 0.01<ul>
<li>When you run the training loop later, notice how the initial loss INCREASES` before decreasing. </li>
<li>You can try a lower learning rate to see if you can avoid this increased loss.</li>
</ul>
</li>
<li>optimizer: you can use <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD">tf.keras.optimizers.SGD</a><ul>
<li>Set the learning rate</li>
<li>Set the momentum to 0.9</li>
</ul>
</li>
</ul>
<p>Training will be fairly quick, so we do encourage you to experiment a bit with these hyperparameters!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.set_learning_phase(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of `None` with your code)###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set the batch_size</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set the number of batches</span></span><br><span class="line">num_batches = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the learning rate</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set the optimizer and pass in the learning_rate</span></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:435: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.
  warnings.warn(&#39;`tf.keras.backend.set_learning_phase` is deprecated and &#39;
</code></pre>
<h2 id="Choose-the-layers-to-fine-tune"><a href="#Choose-the-layers-to-fine-tune" class="headerlink" title="Choose the layers to fine-tune"></a>Choose the layers to fine-tune</h2><p>To make use of transfer learning and pre-trained weights, you will train just certain parts of the detection model, namely, the last prediction layers.</p>
<ul>
<li>Please take a minute to inspect the layers of <code>detection_model</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inspect the layers of detection_model</span></span><br><span class="line"><span class="keyword">for</span> i,v <span class="keyword">in</span> <span class="built_in">enumerate</span>(detection_model.trainable_variables):</span><br><span class="line">    print(<span class="string">f&quot;i: <span class="subst">&#123;i&#125;</span> \t name: <span class="subst">&#123;v.name&#125;</span> \t shape:<span class="subst">&#123;v.shape&#125;</span> \t dtype=<span class="subst">&#123;v.dtype&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>i: 0      name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0      shape:(3, 3, 256, 24)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 1      name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/bias:0      shape:(24,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 2      name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0      shape:(3, 3, 256, 12)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 3      name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/bias:0      shape:(12,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 4      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 5      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 6      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 7      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 8      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 9      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 10      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 11      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 12      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 13      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 14      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 15      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 16      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 17      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 18      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 19      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 20      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 21      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 22      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 23      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 24      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 25      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 26      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 27      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 28      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 29      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 30      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 31      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 32      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 33      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 34      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 35      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 36      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 37      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 38      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 39      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 40      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 41      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 42      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 43      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 44      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 45      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 46      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 47      name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 48      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 49      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 50      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 51      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 52      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 53      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 54      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 55      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 56      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 57      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 58      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 59      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 60      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 61      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 62      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 63      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 64      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 65      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 66      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 67      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 68      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 69      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 70      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 71      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 72      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 73      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 74      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 75      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 76      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 77      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 78      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 79      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 80      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 81      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 82      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 83      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 84      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 85      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 86      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 87      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 88      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 89      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 90      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 91      name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 92      name: ResNet50V1_FPN/bottom_up_block5_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 93      name: ResNet50V1_FPN/bottom_up_block5_batchnorm/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 94      name: ResNet50V1_FPN/bottom_up_block5_batchnorm/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 95      name: ResNet50V1_FPN/bottom_up_block6_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 96      name: ResNet50V1_FPN/bottom_up_block6_batchnorm/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 97      name: ResNet50V1_FPN/bottom_up_block6_batchnorm/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 98      name: conv1_conv/kernel:0      shape:(7, 7, 3, 64)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 99      name: conv1_bn/gamma:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 100      name: conv1_bn/beta:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 101      name: conv2_block1_1_conv/kernel:0      shape:(1, 1, 64, 64)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 102      name: conv2_block1_1_bn/gamma:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 103      name: conv2_block1_1_bn/beta:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 104      name: conv2_block1_2_conv/kernel:0      shape:(3, 3, 64, 64)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 105      name: conv2_block1_2_bn/gamma:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 106      name: conv2_block1_2_bn/beta:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 107      name: conv2_block1_0_conv/kernel:0      shape:(1, 1, 64, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 108      name: conv2_block1_3_conv/kernel:0      shape:(1, 1, 64, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 109      name: conv2_block1_0_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 110      name: conv2_block1_0_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 111      name: conv2_block1_3_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 112      name: conv2_block1_3_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 113      name: conv2_block2_1_conv/kernel:0      shape:(1, 1, 256, 64)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 114      name: conv2_block2_1_bn/gamma:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 115      name: conv2_block2_1_bn/beta:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 116      name: conv2_block2_2_conv/kernel:0      shape:(3, 3, 64, 64)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 117      name: conv2_block2_2_bn/gamma:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 118      name: conv2_block2_2_bn/beta:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 119      name: conv2_block2_3_conv/kernel:0      shape:(1, 1, 64, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 120      name: conv2_block2_3_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 121      name: conv2_block2_3_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 122      name: conv2_block3_1_conv/kernel:0      shape:(1, 1, 256, 64)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 123      name: conv2_block3_1_bn/gamma:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 124      name: conv2_block3_1_bn/beta:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 125      name: conv2_block3_2_conv/kernel:0      shape:(3, 3, 64, 64)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 126      name: conv2_block3_2_bn/gamma:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 127      name: conv2_block3_2_bn/beta:0      shape:(64,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 128      name: conv2_block3_3_conv/kernel:0      shape:(1, 1, 64, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 129      name: conv2_block3_3_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 130      name: conv2_block3_3_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 131      name: conv3_block1_1_conv/kernel:0      shape:(1, 1, 256, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 132      name: conv3_block1_1_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 133      name: conv3_block1_1_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 134      name: conv3_block1_2_conv/kernel:0      shape:(3, 3, 128, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 135      name: conv3_block1_2_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 136      name: conv3_block1_2_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 137      name: conv3_block1_0_conv/kernel:0      shape:(1, 1, 256, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 138      name: conv3_block1_3_conv/kernel:0      shape:(1, 1, 128, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 139      name: conv3_block1_0_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 140      name: conv3_block1_0_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 141      name: conv3_block1_3_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 142      name: conv3_block1_3_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 143      name: conv3_block2_1_conv/kernel:0      shape:(1, 1, 512, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 144      name: conv3_block2_1_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 145      name: conv3_block2_1_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 146      name: conv3_block2_2_conv/kernel:0      shape:(3, 3, 128, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 147      name: conv3_block2_2_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 148      name: conv3_block2_2_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 149      name: conv3_block2_3_conv/kernel:0      shape:(1, 1, 128, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 150      name: conv3_block2_3_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 151      name: conv3_block2_3_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 152      name: conv3_block3_1_conv/kernel:0      shape:(1, 1, 512, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 153      name: conv3_block3_1_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 154      name: conv3_block3_1_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 155      name: conv3_block3_2_conv/kernel:0      shape:(3, 3, 128, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 156      name: conv3_block3_2_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 157      name: conv3_block3_2_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 158      name: conv3_block3_3_conv/kernel:0      shape:(1, 1, 128, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 159      name: conv3_block3_3_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 160      name: conv3_block3_3_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 161      name: conv3_block4_1_conv/kernel:0      shape:(1, 1, 512, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 162      name: conv3_block4_1_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 163      name: conv3_block4_1_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 164      name: conv3_block4_2_conv/kernel:0      shape:(3, 3, 128, 128)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 165      name: conv3_block4_2_bn/gamma:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 166      name: conv3_block4_2_bn/beta:0      shape:(128,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 167      name: conv3_block4_3_conv/kernel:0      shape:(1, 1, 128, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 168      name: conv3_block4_3_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 169      name: conv3_block4_3_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 170      name: conv4_block1_1_conv/kernel:0      shape:(1, 1, 512, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 171      name: conv4_block1_1_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 172      name: conv4_block1_1_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 173      name: conv4_block1_2_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 174      name: conv4_block1_2_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 175      name: conv4_block1_2_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 176      name: conv4_block1_0_conv/kernel:0      shape:(1, 1, 512, 1024)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 177      name: conv4_block1_3_conv/kernel:0      shape:(1, 1, 256, 1024)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 178      name: conv4_block1_0_bn/gamma:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 179      name: conv4_block1_0_bn/beta:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 180      name: conv4_block1_3_bn/gamma:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 181      name: conv4_block1_3_bn/beta:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 182      name: conv4_block2_1_conv/kernel:0      shape:(1, 1, 1024, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 183      name: conv4_block2_1_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 184      name: conv4_block2_1_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 185      name: conv4_block2_2_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 186      name: conv4_block2_2_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 187      name: conv4_block2_2_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 188      name: conv4_block2_3_conv/kernel:0      shape:(1, 1, 256, 1024)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 189      name: conv4_block2_3_bn/gamma:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 190      name: conv4_block2_3_bn/beta:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 191      name: conv4_block3_1_conv/kernel:0      shape:(1, 1, 1024, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 192      name: conv4_block3_1_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 193      name: conv4_block3_1_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 194      name: conv4_block3_2_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 195      name: conv4_block3_2_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 196      name: conv4_block3_2_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 197      name: conv4_block3_3_conv/kernel:0      shape:(1, 1, 256, 1024)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 198      name: conv4_block3_3_bn/gamma:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 199      name: conv4_block3_3_bn/beta:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 200      name: conv4_block4_1_conv/kernel:0      shape:(1, 1, 1024, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 201      name: conv4_block4_1_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 202      name: conv4_block4_1_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 203      name: conv4_block4_2_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 204      name: conv4_block4_2_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 205      name: conv4_block4_2_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 206      name: conv4_block4_3_conv/kernel:0      shape:(1, 1, 256, 1024)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 207      name: conv4_block4_3_bn/gamma:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 208      name: conv4_block4_3_bn/beta:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 209      name: conv4_block5_1_conv/kernel:0      shape:(1, 1, 1024, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 210      name: conv4_block5_1_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 211      name: conv4_block5_1_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 212      name: conv4_block5_2_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 213      name: conv4_block5_2_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 214      name: conv4_block5_2_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 215      name: conv4_block5_3_conv/kernel:0      shape:(1, 1, 256, 1024)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 216      name: conv4_block5_3_bn/gamma:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 217      name: conv4_block5_3_bn/beta:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 218      name: conv4_block6_1_conv/kernel:0      shape:(1, 1, 1024, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 219      name: conv4_block6_1_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 220      name: conv4_block6_1_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 221      name: conv4_block6_2_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 222      name: conv4_block6_2_bn/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 223      name: conv4_block6_2_bn/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 224      name: conv4_block6_3_conv/kernel:0      shape:(1, 1, 256, 1024)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 225      name: conv4_block6_3_bn/gamma:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 226      name: conv4_block6_3_bn/beta:0      shape:(1024,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 227      name: conv5_block1_1_conv/kernel:0      shape:(1, 1, 1024, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 228      name: conv5_block1_1_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 229      name: conv5_block1_1_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 230      name: conv5_block1_2_conv/kernel:0      shape:(3, 3, 512, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 231      name: conv5_block1_2_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 232      name: conv5_block1_2_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 233      name: conv5_block1_0_conv/kernel:0      shape:(1, 1, 1024, 2048)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 234      name: conv5_block1_3_conv/kernel:0      shape:(1, 1, 512, 2048)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 235      name: conv5_block1_0_bn/gamma:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 236      name: conv5_block1_0_bn/beta:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 237      name: conv5_block1_3_bn/gamma:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 238      name: conv5_block1_3_bn/beta:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 239      name: conv5_block2_1_conv/kernel:0      shape:(1, 1, 2048, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 240      name: conv5_block2_1_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 241      name: conv5_block2_1_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 242      name: conv5_block2_2_conv/kernel:0      shape:(3, 3, 512, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 243      name: conv5_block2_2_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 244      name: conv5_block2_2_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 245      name: conv5_block2_3_conv/kernel:0      shape:(1, 1, 512, 2048)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 246      name: conv5_block2_3_bn/gamma:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 247      name: conv5_block2_3_bn/beta:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 248      name: conv5_block3_1_conv/kernel:0      shape:(1, 1, 2048, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 249      name: conv5_block3_1_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 250      name: conv5_block3_1_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 251      name: conv5_block3_2_conv/kernel:0      shape:(3, 3, 512, 512)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 252      name: conv5_block3_2_bn/gamma:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 253      name: conv5_block3_2_bn/beta:0      shape:(512,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 254      name: conv5_block3_3_conv/kernel:0      shape:(1, 1, 512, 2048)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 255      name: conv5_block3_3_bn/gamma:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 256      name: conv5_block3_3_bn/beta:0      shape:(2048,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 257      name: ResNet50V1_FPN/FeatureMaps/top_down/projection_3/kernel:0      shape:(1, 1, 2048, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 258      name: ResNet50V1_FPN/FeatureMaps/top_down/projection_3/bias:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 259      name: ResNet50V1_FPN/FeatureMaps/top_down/projection_2/kernel:0      shape:(1, 1, 1024, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 260      name: ResNet50V1_FPN/FeatureMaps/top_down/projection_2/bias:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 261      name: ResNet50V1_FPN/FeatureMaps/top_down/projection_1/kernel:0      shape:(1, 1, 512, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 262      name: ResNet50V1_FPN/FeatureMaps/top_down/projection_1/bias:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 263      name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_2_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 264      name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_2_batchnorm/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 265      name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_2_batchnorm/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 266      name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_1_conv/kernel:0      shape:(3, 3, 256, 256)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 267      name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_1_batchnorm/gamma:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
i: 268      name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_1_batchnorm/beta:0      shape:(256,)      dtype=&lt;dtype: &#39;float32&#39;&gt;
</code></pre>
<p>Notice that there are some layers whose names are prefixed with the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">WeightSharedConvolutionalBoxPredictor&#x2F;WeightSharedConvolutionalBoxHead</span><br><span class="line">...</span><br><span class="line">WeightSharedConvolutionalBoxPredictor&#x2F;WeightSharedConvolutionalClassHead</span><br><span class="line">...</span><br><span class="line">WeightSharedConvolutionalBoxPredictor&#x2F;BoxPredictionTower</span><br><span class="line">...</span><br><span class="line">WeightSharedConvolutionalBoxPredictor&#x2F;ClassPredictionTower</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>Among these, which do you think are the prediction layers at the “end” of the model?</p>
<ul>
<li>Recall that when inspecting the source code to restore the checkpoints (<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py">convolutional_keras_box_predictor.py</a>) you noticed that:<ul>
<li><code>_base_tower_layers_for_heads</code>: refers to the layers that are placed right before the prediction layer</li>
<li><code>_box_prediction_head</code> refers to the prediction layer for the bounding boxes</li>
<li><code>_prediction_heads</code>: refers to the set of prediction layers (both for classification and for bounding boxes)</li>
</ul>
</li>
</ul>
<p>So you can see that in the source code for this model, “tower” refers to layers that are before the prediction layer, and “head” refers to the prediction layers.</p>
<p><a name='exercise-9'></a></p>
<h3 id="Exercise-9-Select-the-prediction-layer-variables"><a href="#Exercise-9-Select-the-prediction-layer-variables" class="headerlink" title="Exercise 9: Select the prediction layer variables"></a><strong>Exercise 9</strong>: Select the prediction layer variables</h3><p>Based on inspecting the <code>detection_model.trainable_variables</code>, please select the prediction layer variables that you will fine tune:</p>
<ul>
<li>The bounding box head variables (which predict bounding box coordinates)</li>
<li>The class head variables (which predict the class/category)</li>
</ul>
<p>You have a few options for doing this:</p>
<ul>
<li><p>You can access them by their list index:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">detection_model.trainable_variables[92]</span><br></pre></td></tr></table></figure></li>
<li><p>Alternatively, you can use string matching to select the variables:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmp_list &#x3D; []</span><br><span class="line">for v in detection_model.trainable_variables:</span><br><span class="line">  if v.name.startswith(&#39;ResNet50V1_FPN&#x2F;bottom_up_block5&#39;):</span><br><span class="line">    tmp_list.append(v)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>Hint</strong>: There are a total of four variables that you want to fine tune.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE (Replace instances of `None` with your code) ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># define a list that contains the layers that you wish to fine tune</span></span><br><span class="line">to_fine_tune = []</span><br><span class="line"></span><br><span class="line">prefixes_to_train = [</span><br><span class="line">  <span class="string">&#x27;WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> detection_model.trainable_variables:</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">any</span>([var.name.startswith(prefix) <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes_to_train]):</span><br><span class="line">    to_fine_tune.append(var)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test Code:</span></span><br><span class="line"></span><br><span class="line">print(to_fine_tune[<span class="number">0</span>].name)</span><br><span class="line">print(to_fine_tune[<span class="number">2</span>].name)</span><br></pre></td></tr></table></figure>

<pre><code>WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0
WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0
</code></pre>
<p><strong>Expected Output</strong>:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0</span><br><span class="line">WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0</span><br></pre></td></tr></table></figure>

<h2 id="Train-your-model"><a href="#Train-your-model" class="headerlink" title="Train your model"></a>Train your model</h2><p>You’ll define a function that handles training for one batch, which you’ll later use in your training loop.</p>
<p>First, walk through these code cells to learn how you’ll perform training using this model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get a batch of your training images</span></span><br><span class="line">g_images_list = train_image_tensors[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p>The <code>detection_model</code> is of class <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L655">SSDMetaArch</a>, and its source code shows that is has this function <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L459">preprocess</a>.</p>
<ul>
<li>This preprocesses the images so that they can be passed into the model (for training or prediction):<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def preprocess(self, inputs):</span><br><span class="line">  &quot;&quot;&quot;Feature-extractor specific preprocessing.</span><br><span class="line">  ...</span><br><span class="line">  Args:</span><br><span class="line">    inputs: a [batch, height_in, width_in, channels] float tensor representing</span><br><span class="line">      a batch of images with values between 0 and 255.0.</span><br><span class="line">  Returns:</span><br><span class="line">    preprocessed_inputs: a [batch, height_out, width_out, channels] float</span><br><span class="line">      tensor representing a batch of images.</span><br><span class="line">      </span><br><span class="line">    true_image_shapes: int32 tensor of shape [batch, 3] where each row is</span><br><span class="line">      of the form [height, width, channels] indicating the shapes</span><br><span class="line">      of true images in the resized images, as resized images can be padded</span><br><span class="line">      with zeros.</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use .preprocess to preprocess an image</span></span><br><span class="line">g_preprocessed_image = detection_model.preprocess(g_images_list[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">f&quot;g_preprocessed_image type: <span class="subst">&#123;<span class="built_in">type</span>(g_preprocessed_image)&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;g_preprocessed_image length: <span class="subst">&#123;<span class="built_in">len</span>(g_preprocessed_image)&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;index 0 has the preprocessed image of shape <span class="subst">&#123;g_preprocessed_image[<span class="number">0</span>].shape&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;index 1 has information about the image&#x27;s true shape excluding padding: <span class="subst">&#123;g_preprocessed_image[<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>g_preprocessed_image type: &lt;class &#39;tuple&#39;&gt;
g_preprocessed_image length: 2
index 0 has the preprocessed image of shape (1, 640, 640, 3)
index 1 has information about the image&#39;s true shape excluding padding: [[640 640   3]]
</code></pre>
<p>You can pre-process each image and save their outputs into two separate lists</p>
<ul>
<li>One list of the preprocessed images</li>
<li>One list of the true shape for each preprocessed image</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">preprocessed_image_list = []</span><br><span class="line">true_shape_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> img <span class="keyword">in</span> g_images_list:</span><br><span class="line">    processed_img, true_shape = detection_model.preprocess(img)</span><br><span class="line">    preprocessed_image_list.append(processed_img)</span><br><span class="line">    true_shape_list.append(true_shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&quot;preprocessed_image_list is of type <span class="subst">&#123;<span class="built_in">type</span>(preprocessed_image_list)&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;preprocessed_image_list has length <span class="subst">&#123;<span class="built_in">len</span>(preprocessed_image_list)&#125;</span>&quot;</span>)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">f&quot;true_shape_list is of type <span class="subst">&#123;<span class="built_in">type</span>(true_shape_list)&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;true_shape_list has length <span class="subst">&#123;<span class="built_in">len</span>(true_shape_list)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>preprocessed_image_list is of type &lt;class &#39;list&#39;&gt;
preprocessed_image_list has length 2

true_shape_list is of type &lt;class &#39;list&#39;&gt;
true_shape_list has length 2
</code></pre>
<h2 id="Make-a-prediction"><a href="#Make-a-prediction" class="headerlink" title="Make a prediction"></a>Make a prediction</h2><p>The <code>detection_model</code> also has a <code>.predict</code> function.  According to the source code for <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L525">predict</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">  def predict(self, preprocessed_inputs, true_image_shapes):</span><br><span class="line">    &quot;&quot;&quot;Predicts unpostprocessed tensors from input tensor.</span><br><span class="line">    This function takes an input batch of images and runs it through the forward</span><br><span class="line">    pass of the network to yield unpostprocessesed predictions.</span><br><span class="line">...</span><br><span class="line">    Args:</span><br><span class="line">      preprocessed_inputs: a [batch, height, width, channels] image tensor.</span><br><span class="line">      </span><br><span class="line">      true_image_shapes: int32 tensor of shape [batch, 3] where each row is</span><br><span class="line">        of the form [height, width, channels] indicating the shapes</span><br><span class="line">        of true images in the resized images, as resized images can be padded</span><br><span class="line">        with zeros.</span><br><span class="line">        </span><br><span class="line">    Returns:</span><br><span class="line">      prediction_dict: a dictionary holding &quot;raw&quot; prediction tensors:</span><br><span class="line">        1) preprocessed_inputs: the [batch, height, width, channels] image</span><br><span class="line">          tensor.</span><br><span class="line">        2) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,</span><br><span class="line">          box_code_dimension] containing predicted boxes.</span><br><span class="line">        3) class_predictions_with_background: 3-D float tensor of shape</span><br><span class="line">          [batch_size, num_anchors, num_classes+1] containing class predictions</span><br><span class="line">          (logits) for each of the anchors.  Note that this tensor *includes*</span><br><span class="line">          background class predictions (at class index 0).</span><br><span class="line">        4) feature_maps: a list of tensors where the ith tensor has shape</span><br><span class="line">          [batch, height_i, width_i, depth_i].</span><br><span class="line">        5) anchors: 2-D float tensor of shape [num_anchors, 4] containing</span><br><span class="line">          the generated anchors in normalized coordinates.</span><br><span class="line">        6) final_anchors: 3-D float tensor of shape [batch_size, num_anchors, 4]</span><br><span class="line">          containing the generated anchors in normalized coordinates.</span><br><span class="line">        If self._return_raw_detections_during_predict is True, the dictionary</span><br><span class="line">        will also contain:</span><br><span class="line">        7) raw_detection_boxes: a 4-D float32 tensor with shape</span><br><span class="line">          [batch_size, self.max_num_proposals, 4] in normalized coordinates.</span><br><span class="line">        8) raw_detection_feature_map_indices: a 3-D int32 tensor with shape</span><br><span class="line">          [batch_size, self.max_num_proposals].</span><br><span class="line">    &quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<p>Notice that <code>.predict</code> takes its inputs as tensors.  If you tried to pass in the preprocessed images and true shapes, you’ll get an error.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Try to call `predict` and pass in lists; look at the error message</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    detection_model.predict(preprocessed_image_list, true_shape_list)</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">&quot;Error message:&quot;</span>, e)</span><br></pre></td></tr></table></figure>

<pre><code>Error message: &#39;list&#39; object has no attribute &#39;get_shape&#39;
</code></pre>
<p>But don’t worry! You can check how to properly use <code>predict</code>:</p>
<ul>
<li>Notice that the source code documentation says that <code>preprocessed_inputs</code> and <code>true_image_shapes</code> are expected to be tensors and not lists of tensors.</li>
<li>One way to turn a list of tensors into a tensor is to use <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/concat">tf.concat</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(</span><br><span class="line">    values, axis, name&#x3D;&#39;concat&#39;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Turn a list of tensors into a tensor</span></span><br><span class="line">preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=<span class="number">0</span>)</span><br><span class="line">true_shape_tensor = tf.concat(true_shape_list, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&quot;preprocessed_image_tensor shape: <span class="subst">&#123;preprocessed_image_tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;true_shape_tensor shape: <span class="subst">&#123;true_shape_tensor.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>preprocessed_image_tensor shape: (2, 640, 640, 3)
true_shape_tensor shape: (2, 3)
</code></pre>
<p>Now you can make predictions for the images.<br>According to the source code, <code>predict</code> returns a dictionary containing the prediction information, including:</p>
<ul>
<li>The bounding box predictions</li>
<li>The class predictions</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make predictions on the images</span></span><br><span class="line">prediction_dict = detection_model.predict(preprocessed_image_tensor, true_shape_tensor)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;keys in prediction_dict:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> prediction_dict.keys():</span><br><span class="line">    print(key)</span><br></pre></td></tr></table></figure>

<pre><code>keys in prediction_dict:
preprocessed_inputs
feature_maps
anchors
final_anchors
box_encodings
class_predictions_with_background
</code></pre>
<h4 id="Calculate-loss"><a href="#Calculate-loss" class="headerlink" title="Calculate loss"></a>Calculate loss</h4><p>Now that your model has made its prediction, you want to compare it to the ground truth in order to calculate a loss.</p>
<ul>
<li>The <code>detection_model</code> has a <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L807">loss</a> function.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, prediction_dict, true_image_shapes, scope=<span class="literal">None</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Compute scalar loss tensors with respect to provided groundtruth.</span></span><br><span class="line"><span class="string">  Calling this function requires that groundtruth tensors have been</span></span><br><span class="line"><span class="string">  provided via the provide_groundtruth function.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    prediction_dict: a dictionary holding prediction tensors with</span></span><br><span class="line"><span class="string">      1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,</span></span><br><span class="line"><span class="string">        box_code_dimension] containing predicted boxes.</span></span><br><span class="line"><span class="string">      2) class_predictions_with_background: 3-D float tensor of shape</span></span><br><span class="line"><span class="string">        [batch_size, num_anchors, num_classes+1] containing class predictions</span></span><br><span class="line"><span class="string">        (logits) for each of the anchors. Note that this tensor *includes*</span></span><br><span class="line"><span class="string">        background class predictions.</span></span><br><span class="line"><span class="string">    true_image_shapes: int32 tensor of shape [batch, 3] where each row is</span></span><br><span class="line"><span class="string">      of the form [height, width, channels] indicating the shapes</span></span><br><span class="line"><span class="string">      of true images in the resized images, as resized images can be padded</span></span><br><span class="line"><span class="string">      with zeros.</span></span><br><span class="line"><span class="string">    scope: Optional scope name.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    a dictionary mapping loss keys (`localization_loss` and</span></span><br><span class="line"><span class="string">      `classification_loss`) to scalar tensors representing corresponding loss</span></span><br><span class="line"><span class="string">      values.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>It takes in:</p>
<ul>
<li>The prediction dictionary that comes from your call to <code>.predict()</code>.</li>
<li>the true images shape that comes from your call to <code>.preprocess()</code> followed by the conversion from a list to a tensor.</li>
</ul>
<p>Try calling <code>.loss</code>.  You’ll see an error message that you’ll addres in order to run the <code>.loss</code> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)</span><br><span class="line"><span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure>

<pre><code>Groundtruth tensor boxes has not been provided
</code></pre>
<p>This is giving an error about groundtruth_classes_list: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The graph tensor has name: groundtruth_classes_list:0</span><br></pre></td></tr></table></figure>

<p>Notice in the docstring for <code>loss</code> (shown above), it says:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Calling this function requires that groundtruth tensors have been</span><br><span class="line">    provided via the provide_groundtruth function.</span><br></pre></td></tr></table></figure>

<p>So you’ll first want to set the ground truth (true labels and true bounding boxes) before you calculate the loss.</p>
<ul>
<li>This makes sense, since the loss is comparing the prediction to the ground truth, and so the loss function needs to know the ground truth.<h4 id="Provide-the-ground-truth"><a href="#Provide-the-ground-truth" class="headerlink" title="Provide the ground truth"></a>Provide the ground truth</h4>The source code for providing the ground truth is located in the parent class of <code>SSDMetaArch</code>, <code>model.DetectionModel</code>.</li>
<li>Here is the link to the code for <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/fd6b24c19c68af026bb0a9efc9f7b1719c231d3d/research/object_detection/core/model.py#L297">provide_ground_truth</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">provide_groundtruth</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">      self,</span></span></span><br><span class="line"><span class="function"><span class="params">      groundtruth_boxes_list,</span></span></span><br><span class="line"><span class="function"><span class="params">      groundtruth_classes_list,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="meta">... </span><span class="comment"># more parameters not show here</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">&quot;&quot;&quot;</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">    Args:</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">      groundtruth_boxes_list: a list of 2-D tf.float32 tensors of shape</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">        [num_boxes, 4] containing coordinates of the groundtruth boxes.</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">          Groundtruth boxes are provided in [y_min, x_min, y_max, x_max]</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">          format and assumed to be normalized and clipped</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">          relative to the image window with y_min &lt;= y_max and x_min &lt;= x_max.</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">      groundtruth_classes_list: a list of 2-D tf.float32 one-hot (or k-hot)</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">        tensors of shape [num_boxes, num_classes] containing the class targets</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">        with the 0th index assumed to map to the first non-background class.</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">&quot;&quot;&quot;</span></span></span></span><br></pre></td></tr></table></figure>
<p>You’ll set two parameters in <code>provide_ground_truth</code>:</p>
<ul>
<li>The true bounding boxes</li>
<li>The true classes</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the ground truth bounding boxes</span></span><br><span class="line">gt_boxes_list = gt_box_tensors[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the ground truth class labels</span></span><br><span class="line">gt_classes_list = gt_classes_one_hot_tensors[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Provide the ground truth to the model</span></span><br><span class="line">detection_model.provide_groundtruth(</span><br><span class="line">            groundtruth_boxes_list=gt_boxes_list,</span><br><span class="line">            groundtruth_classes_list=gt_classes_list)</span><br></pre></td></tr></table></figure>

<p>Now you can calculate the loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate the loss after you&#x27;ve provided the ground truth </span></span><br><span class="line">losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the loss dictionary</span></span><br><span class="line">losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)</span><br><span class="line">print(<span class="string">f&quot;loss dictionary keys: <span class="subst">&#123;losses_dict.keys()&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;localization loss <span class="subst">&#123;losses_dict[<span class="string">&#x27;Loss/localization_loss&#x27;</span>]:<span class="number">.8</span>f&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;classification loss <span class="subst">&#123;losses_dict[<span class="string">&#x27;Loss/classification_loss&#x27;</span>]:<span class="number">.8</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>loss dictionary keys: dict_keys([&#39;Loss/localization_loss&#39;, &#39;Loss/classification_loss&#39;])
localization loss 0.08500115
classification loss 1.10470212
</code></pre>
<p>You can now calculate the gradient and optimize the variables that you selected to fine tune.</p>
<ul>
<li>Use tf.GradientTape</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># Make the prediction </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate the loss</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># calculate the gradient of each model variable with respect to each loss</span></span><br><span class="line">    gradients = tape.gradient([some loss], variables to fine tune)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># apply the gradients to update these model variables</span></span><br><span class="line">    optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, variables to fine tune))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s just reset the model so that you can practice setting it up yourself!</span></span><br><span class="line">detection_model.provide_groundtruth(groundtruth_boxes_list=[], groundtruth_classes_list=[])</span><br></pre></td></tr></table></figure>

<p><a name='exercise-10'></a></p>
<h3 id="Exercise-10-Define-the-training-step"><a href="#Exercise-10-Define-the-training-step" class="headerlink" title="Exercise 10: Define the training step"></a><strong>Exercise 10</strong>: Define the training step</h3><p>Please complete the function below to set up one training step.</p>
<ul>
<li>Preprocess the images</li>
<li>Make a prediction</li>
<li>Calculate the loss (and make sure the loss function has the ground truth to compare with the prediction)</li>
<li>Calculate the total loss:<ul>
<li><code>total_loss</code> = <code>localization_loss + classification_loss</code></li>
<li>Note: this is different than the example code that you saw above</li>
</ul>
</li>
<li>Calculate gradients with respect to the variables you selected to train.</li>
<li>Optimize the model’s variables</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># decorate with @tf.function for faster training (remember, graph mode!)</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step_fn</span>(<span class="params">image_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                groundtruth_boxes_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                groundtruth_classes_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                model,</span></span></span><br><span class="line"><span class="function"><span class="params">                optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">                vars_to_fine_tune</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A single training iteration.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      image_list: A list of [1, height, width, 3] Tensor of type tf.float32.</span></span><br><span class="line"><span class="string">        Note that the height and width can vary across images, as they are</span></span><br><span class="line"><span class="string">        reshaped within this function to be 640x640.</span></span><br><span class="line"><span class="string">      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type</span></span><br><span class="line"><span class="string">        tf.float32 representing groundtruth boxes for each image in the batch.</span></span><br><span class="line"><span class="string">      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]</span></span><br><span class="line"><span class="string">        with type tf.float32 representing groundtruth boxes for each image in</span></span><br><span class="line"><span class="string">        the batch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A scalar tensor representing the total loss for the input batch.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of `None` with your code) ###</span></span><br><span class="line"></span><br><span class="line">    shapes = tf.constant(batch_size * [[<span class="number">640</span>, <span class="number">640</span>, <span class="number">3</span>]], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        <span class="comment"># Preprocess the images</span></span><br><span class="line">      model.provide_groundtruth(</span><br><span class="line">          groundtruth_boxes_list=groundtruth_boxes_list,</span><br><span class="line">          groundtruth_classes_list=groundtruth_classes_list)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        preprocessed_images = tf.concat(</span><br><span class="line">            [detection_model.preprocess(image_tensor)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> image_tensor <span class="keyword">in</span> image_list], axis=<span class="number">0</span>)</span><br><span class="line">        prediction_dict = model.predict(preprocessed_images, shapes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># processed_img, true_shape = model.preprocess(image_list)</span></span><br><span class="line">        <span class="comment"># preprocessed_image_tensor = tf.concat(processed_img, axis=0)</span></span><br><span class="line">        <span class="comment"># true_shape_tensor = tf.concat(true_shape, axis=0)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make a prediction</span></span><br><span class="line">        <span class="comment"># prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the total loss (sum of both losses)</span></span><br><span class="line">        losses_dict = model.loss(prediction_dict, shapes)</span><br><span class="line">        </span><br><span class="line">        total_loss = losses_dict[<span class="string">&#x27;Loss/localization_loss&#x27;</span>] + losses_dict[<span class="string">&#x27;Loss/classification_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the gradients</span></span><br><span class="line">        gradients = tape.gradient(total_loss, vars_to_fine_tune)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Optimize the model&#x27;s selected variables</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, vars_to_fine_tune))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>

<h2 id="Run-the-training-loop"><a href="#Run-the-training-loop" class="headerlink" title="Run the training loop"></a>Run the training loop</h2><p>Run the training loop using the training step function that you just defined.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;Start fine-tuning!&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">    <span class="comment"># Grab keys for a random subset of examples</span></span><br><span class="line">    all_keys = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(train_images_np)))</span><br><span class="line">    random.shuffle(all_keys)</span><br><span class="line">    example_keys = all_keys[:batch_size]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the ground truth</span></span><br><span class="line">    gt_boxes_list = [gt_box_tensors[key] <span class="keyword">for</span> key <span class="keyword">in</span> example_keys]</span><br><span class="line">    gt_classes_list = [gt_classes_one_hot_tensors[key] <span class="keyword">for</span> key <span class="keyword">in</span> example_keys]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get the images</span></span><br><span class="line">    image_tensors = [train_image_tensors[key] <span class="keyword">for</span> key <span class="keyword">in</span> example_keys]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training step (forward pass + backwards pass)</span></span><br><span class="line">    total_loss = train_step_fn(image_tensors, </span><br><span class="line">                               gt_boxes_list, </span><br><span class="line">                               gt_classes_list,</span><br><span class="line">                               detection_model,</span><br><span class="line">                               optimizer,</span><br><span class="line">                               to_fine_tune</span><br><span class="line">                              )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> idx % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">&#x27;batch &#x27;</span> + <span class="built_in">str</span>(idx) + <span class="string">&#x27; of &#x27;</span> + <span class="built_in">str</span>(num_batches)</span><br><span class="line">        + <span class="string">&#x27;, loss=&#x27;</span> +  <span class="built_in">str</span>(total_loss.numpy()), flush=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Done fine-tuning!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Start fine-tuning!
batch 0 of 100, loss=1.1260462
batch 10 of 100, loss=11.773195
batch 20 of 100, loss=4.234128
batch 30 of 100, loss=0.07759774
batch 40 of 100, loss=0.021622008
batch 50 of 100, loss=0.0018396748
batch 60 of 100, loss=0.0012148139
batch 70 of 100, loss=0.0010113249
batch 80 of 100, loss=0.00088542164
batch 90 of 100, loss=0.0006638657
Done fine-tuning!
</code></pre>
<p><strong>Expected Output:</strong></p>
<p>Total loss should be decreasing and should be less than 1 after fine tuning. For example:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Start fine-tuning!</span><br><span class="line">batch 0 of 100, loss=1.2559178</span><br><span class="line">batch 10 of 100, loss=16.067217</span><br><span class="line">batch 20 of 100, loss=8.094654</span><br><span class="line">batch 30 of 100, loss=0.34514275</span><br><span class="line">batch 40 of 100, loss=0.033170983</span><br><span class="line">batch 50 of 100, loss=0.0024622646</span><br><span class="line">batch 60 of 100, loss=0.00074224477</span><br><span class="line">batch 70 of 100, loss=0.0006149876</span><br><span class="line">batch 80 of 100, loss=0.00046916265</span><br><span class="line">batch 90 of 100, loss=0.0004159231</span><br><span class="line">Done fine-tuning!</span><br></pre></td></tr></table></figure>

<h2 id="Load-test-images-and-run-inference-with-new-model"><a href="#Load-test-images-and-run-inference-with-new-model" class="headerlink" title="Load test images and run inference with new model!"></a>Load test images and run inference with new model!</h2><p>You can now test your model on a new set of images. The cell below downloads 237 images of a walking zombie and stores them in a <code>results/</code> directory.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># uncomment if you want to delete existing files</span></span><br><span class="line">!rm zombie-walk-frames.<span class="built_in">zip</span></span><br><span class="line">!rm -rf ./zombie-walk</span><br><span class="line">!rm -rf ./results</span><br><span class="line"></span><br><span class="line"><span class="comment"># download test images</span></span><br><span class="line">!wget --no-check-certificate \</span><br><span class="line">    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/zombie-walk-frames.<span class="built_in">zip</span> \</span><br><span class="line">    -O zombie-walk-frames.<span class="built_in">zip</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># unzip test images</span></span><br><span class="line">local_zip = <span class="string">&#x27;./zombie-walk-frames.zip&#x27;</span></span><br><span class="line">zip_ref = zipfile.ZipFile(local_zip, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;./results&#x27;</span>)</span><br><span class="line">zip_ref.close()</span><br></pre></td></tr></table></figure>

<pre><code>rm: cannot remove &#39;zombie-walk-frames.zip&#39;: No such file or directory
--2021-06-20 08:02:41--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/zombie-walk-frames.zip
Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.62.128, 172.253.115.128, 172.253.122.128, ...
Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.62.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 94778747 (90M) [application/zip]
Saving to: ‘zombie-walk-frames.zip’

zombie-walk-frames. 100%[===================&gt;]  90.39M   174MB/s    in 0.5s    

2021-06-20 08:02:42 (174 MB/s) - ‘zombie-walk-frames.zip’ saved [94778747/94778747]
</code></pre>
<p>You will load these images into numpy arrays to prepare it for inference.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">test_image_dir = <span class="string">&#x27;./results/&#x27;</span></span><br><span class="line">test_images_np = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># load images into a numpy array. this will take a few minutes to complete.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">237</span>):</span><br><span class="line">    image_path = os.path.join(test_image_dir, <span class="string">&#x27;zombie-walk&#x27;</span> + <span class="string">&quot;&#123;0:04&#125;&quot;</span>.<span class="built_in">format</span>(i) + <span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line">    print(image_path)</span><br><span class="line">    test_images_np.append(np.expand_dims(</span><br><span class="line">      load_image_into_numpy_array(image_path), axis=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<pre><code>./results/zombie-walk0000.jpg
./results/zombie-walk0001.jpg
./results/zombie-walk0002.jpg
./results/zombie-walk0003.jpg
./results/zombie-walk0004.jpg
./results/zombie-walk0005.jpg
./results/zombie-walk0006.jpg
./results/zombie-walk0007.jpg
./results/zombie-walk0008.jpg
./results/zombie-walk0009.jpg
./results/zombie-walk0010.jpg
./results/zombie-walk0011.jpg
./results/zombie-walk0012.jpg
./results/zombie-walk0013.jpg
./results/zombie-walk0014.jpg
./results/zombie-walk0015.jpg
./results/zombie-walk0016.jpg
./results/zombie-walk0017.jpg
./results/zombie-walk0018.jpg
./results/zombie-walk0019.jpg
./results/zombie-walk0020.jpg
./results/zombie-walk0021.jpg
./results/zombie-walk0022.jpg
./results/zombie-walk0023.jpg
./results/zombie-walk0024.jpg
./results/zombie-walk0025.jpg
./results/zombie-walk0026.jpg
./results/zombie-walk0027.jpg
./results/zombie-walk0028.jpg
./results/zombie-walk0029.jpg
./results/zombie-walk0030.jpg
./results/zombie-walk0031.jpg
./results/zombie-walk0032.jpg
./results/zombie-walk0033.jpg
./results/zombie-walk0034.jpg
./results/zombie-walk0035.jpg
./results/zombie-walk0036.jpg
./results/zombie-walk0037.jpg
./results/zombie-walk0038.jpg
./results/zombie-walk0039.jpg
./results/zombie-walk0040.jpg
./results/zombie-walk0041.jpg
./results/zombie-walk0042.jpg
./results/zombie-walk0043.jpg
./results/zombie-walk0044.jpg
./results/zombie-walk0045.jpg
./results/zombie-walk0046.jpg
./results/zombie-walk0047.jpg
./results/zombie-walk0048.jpg
./results/zombie-walk0049.jpg
./results/zombie-walk0050.jpg
./results/zombie-walk0051.jpg
./results/zombie-walk0052.jpg
./results/zombie-walk0053.jpg
./results/zombie-walk0054.jpg
./results/zombie-walk0055.jpg
./results/zombie-walk0056.jpg
./results/zombie-walk0057.jpg
./results/zombie-walk0058.jpg
./results/zombie-walk0059.jpg
./results/zombie-walk0060.jpg
./results/zombie-walk0061.jpg
./results/zombie-walk0062.jpg
./results/zombie-walk0063.jpg
./results/zombie-walk0064.jpg
./results/zombie-walk0065.jpg
./results/zombie-walk0066.jpg
./results/zombie-walk0067.jpg
./results/zombie-walk0068.jpg
./results/zombie-walk0069.jpg
./results/zombie-walk0070.jpg
./results/zombie-walk0071.jpg
./results/zombie-walk0072.jpg
./results/zombie-walk0073.jpg
./results/zombie-walk0074.jpg
./results/zombie-walk0075.jpg
./results/zombie-walk0076.jpg
./results/zombie-walk0077.jpg
./results/zombie-walk0078.jpg
./results/zombie-walk0079.jpg
./results/zombie-walk0080.jpg
./results/zombie-walk0081.jpg
./results/zombie-walk0082.jpg
./results/zombie-walk0083.jpg
./results/zombie-walk0084.jpg
./results/zombie-walk0085.jpg
./results/zombie-walk0086.jpg
./results/zombie-walk0087.jpg
./results/zombie-walk0088.jpg
./results/zombie-walk0089.jpg
./results/zombie-walk0090.jpg
./results/zombie-walk0091.jpg
./results/zombie-walk0092.jpg
./results/zombie-walk0093.jpg
./results/zombie-walk0094.jpg
./results/zombie-walk0095.jpg
./results/zombie-walk0096.jpg
./results/zombie-walk0097.jpg
./results/zombie-walk0098.jpg
./results/zombie-walk0099.jpg
./results/zombie-walk0100.jpg
./results/zombie-walk0101.jpg
./results/zombie-walk0102.jpg
./results/zombie-walk0103.jpg
./results/zombie-walk0104.jpg
./results/zombie-walk0105.jpg
./results/zombie-walk0106.jpg
./results/zombie-walk0107.jpg
./results/zombie-walk0108.jpg
./results/zombie-walk0109.jpg
./results/zombie-walk0110.jpg
./results/zombie-walk0111.jpg
./results/zombie-walk0112.jpg
./results/zombie-walk0113.jpg
./results/zombie-walk0114.jpg
./results/zombie-walk0115.jpg
./results/zombie-walk0116.jpg
./results/zombie-walk0117.jpg
./results/zombie-walk0118.jpg
./results/zombie-walk0119.jpg
./results/zombie-walk0120.jpg
./results/zombie-walk0121.jpg
./results/zombie-walk0122.jpg
./results/zombie-walk0123.jpg
./results/zombie-walk0124.jpg
./results/zombie-walk0125.jpg
./results/zombie-walk0126.jpg
./results/zombie-walk0127.jpg
./results/zombie-walk0128.jpg
./results/zombie-walk0129.jpg
./results/zombie-walk0130.jpg
./results/zombie-walk0131.jpg
./results/zombie-walk0132.jpg
./results/zombie-walk0133.jpg
./results/zombie-walk0134.jpg
./results/zombie-walk0135.jpg
./results/zombie-walk0136.jpg
./results/zombie-walk0137.jpg
./results/zombie-walk0138.jpg
./results/zombie-walk0139.jpg
./results/zombie-walk0140.jpg
./results/zombie-walk0141.jpg
./results/zombie-walk0142.jpg
./results/zombie-walk0143.jpg
./results/zombie-walk0144.jpg
./results/zombie-walk0145.jpg
./results/zombie-walk0146.jpg
./results/zombie-walk0147.jpg
./results/zombie-walk0148.jpg
./results/zombie-walk0149.jpg
./results/zombie-walk0150.jpg
./results/zombie-walk0151.jpg
./results/zombie-walk0152.jpg
./results/zombie-walk0153.jpg
./results/zombie-walk0154.jpg
./results/zombie-walk0155.jpg
./results/zombie-walk0156.jpg
./results/zombie-walk0157.jpg
./results/zombie-walk0158.jpg
./results/zombie-walk0159.jpg
./results/zombie-walk0160.jpg
./results/zombie-walk0161.jpg
./results/zombie-walk0162.jpg
./results/zombie-walk0163.jpg
./results/zombie-walk0164.jpg
./results/zombie-walk0165.jpg
./results/zombie-walk0166.jpg
./results/zombie-walk0167.jpg
./results/zombie-walk0168.jpg
./results/zombie-walk0169.jpg
./results/zombie-walk0170.jpg
./results/zombie-walk0171.jpg
./results/zombie-walk0172.jpg
./results/zombie-walk0173.jpg
./results/zombie-walk0174.jpg
./results/zombie-walk0175.jpg
./results/zombie-walk0176.jpg
./results/zombie-walk0177.jpg
./results/zombie-walk0178.jpg
./results/zombie-walk0179.jpg
./results/zombie-walk0180.jpg
./results/zombie-walk0181.jpg
./results/zombie-walk0182.jpg
./results/zombie-walk0183.jpg
./results/zombie-walk0184.jpg
./results/zombie-walk0185.jpg
./results/zombie-walk0186.jpg
./results/zombie-walk0187.jpg
./results/zombie-walk0188.jpg
./results/zombie-walk0189.jpg
./results/zombie-walk0190.jpg
./results/zombie-walk0191.jpg
./results/zombie-walk0192.jpg
./results/zombie-walk0193.jpg
./results/zombie-walk0194.jpg
./results/zombie-walk0195.jpg
./results/zombie-walk0196.jpg
./results/zombie-walk0197.jpg
./results/zombie-walk0198.jpg
./results/zombie-walk0199.jpg
./results/zombie-walk0200.jpg
./results/zombie-walk0201.jpg
./results/zombie-walk0202.jpg
./results/zombie-walk0203.jpg
./results/zombie-walk0204.jpg
./results/zombie-walk0205.jpg
./results/zombie-walk0206.jpg
./results/zombie-walk0207.jpg
./results/zombie-walk0208.jpg
./results/zombie-walk0209.jpg
./results/zombie-walk0210.jpg
./results/zombie-walk0211.jpg
./results/zombie-walk0212.jpg
./results/zombie-walk0213.jpg
./results/zombie-walk0214.jpg
./results/zombie-walk0215.jpg
./results/zombie-walk0216.jpg
./results/zombie-walk0217.jpg
./results/zombie-walk0218.jpg
./results/zombie-walk0219.jpg
./results/zombie-walk0220.jpg
./results/zombie-walk0221.jpg
./results/zombie-walk0222.jpg
./results/zombie-walk0223.jpg
./results/zombie-walk0224.jpg
./results/zombie-walk0225.jpg
./results/zombie-walk0226.jpg
./results/zombie-walk0227.jpg
./results/zombie-walk0228.jpg
./results/zombie-walk0229.jpg
./results/zombie-walk0230.jpg
./results/zombie-walk0231.jpg
./results/zombie-walk0232.jpg
./results/zombie-walk0233.jpg
./results/zombie-walk0234.jpg
./results/zombie-walk0235.jpg
./results/zombie-walk0236.jpg
</code></pre>
<p><a name='exercise-11'></a></p>
<h3 id="Exercise-11-Preprocess-predict-and-post-process-an-image"><a href="#Exercise-11-Preprocess-predict-and-post-process-an-image" class="headerlink" title="Exercise 11: Preprocess, predict, and post process an image"></a><strong>Exercise 11</strong>: Preprocess, predict, and post process an image</h3><p>Define a function that returns the detection boxes, classes, and scores.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Again, uncomment this decorator if you want to run inference eagerly</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect</span>(<span class="params">input_tensor</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run detection on an input image.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.</span></span><br><span class="line"><span class="string">      Note that height and width can be anything since the image will be</span></span><br><span class="line"><span class="string">      immediately resized according to the needs of the model within this</span></span><br><span class="line"><span class="string">      function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,</span></span><br><span class="line"><span class="string">      and `detection_scores`).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    preprocessed_image, shapes = detection_model.preprocess(input_tensor)</span><br><span class="line">    prediction_dict = detection_model.predict(preprocessed_image, shapes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of `None` with your code) ###</span></span><br><span class="line">    <span class="comment"># use the detection model&#x27;s postprocess() method to get the the final detections</span></span><br><span class="line">    detections = detection_model.postprocess(prediction_dict, shapes)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> detections</span><br></pre></td></tr></table></figure>

<p>You can now loop through the test images and get the detection scores and bounding boxes to overlay in the original image. We will save each result in a <code>results</code> dictionary and the autograder will use this to evaluate your results.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note that the first frame will trigger tracing of the tf.function, which will</span></span><br><span class="line"><span class="comment"># take some time, after which inference should be fast.</span></span><br><span class="line"></span><br><span class="line">label_id_offset = <span class="number">1</span></span><br><span class="line">results = &#123;<span class="string">&#x27;boxes&#x27;</span>: [], <span class="string">&#x27;scores&#x27;</span>: []&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_images_np)):</span><br><span class="line">    input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)</span><br><span class="line">    detections = detect(input_tensor)</span><br><span class="line">    plot_detections(</span><br><span class="line">      test_images_np[i][<span class="number">0</span>],</span><br><span class="line">      detections[<span class="string">&#x27;detection_boxes&#x27;</span>][<span class="number">0</span>].numpy(),</span><br><span class="line">      detections[<span class="string">&#x27;detection_classes&#x27;</span>][<span class="number">0</span>].numpy().astype(np.uint32)</span><br><span class="line">      + label_id_offset,</span><br><span class="line">      detections[<span class="string">&#x27;detection_scores&#x27;</span>][<span class="number">0</span>].numpy(),</span><br><span class="line">      category_index, figsize=(<span class="number">15</span>, <span class="number">20</span>), image_name=<span class="string">&quot;./results/gif_frame_&quot;</span> + (<span class="string">&#x27;%03d&#x27;</span> % i) + <span class="string">&quot;.jpg&quot;</span>)</span><br><span class="line">    results[<span class="string">&#x27;boxes&#x27;</span>].append(detections[<span class="string">&#x27;detection_boxes&#x27;</span>][<span class="number">0</span>][<span class="number">0</span>].numpy())</span><br><span class="line">    results[<span class="string">&#x27;scores&#x27;</span>].append(detections[<span class="string">&#x27;detection_scores&#x27;</span>][<span class="number">0</span>][<span class="number">0</span>].numpy())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEST CODE</span></span><br><span class="line"></span><br><span class="line">print(<span class="built_in">len</span>(results[<span class="string">&#x27;boxes&#x27;</span>]))</span><br><span class="line">print(results[<span class="string">&#x27;boxes&#x27;</span>][<span class="number">0</span>].shape)</span><br><span class="line">print(results[<span class="string">&#x27;boxes&#x27;</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compare with expected bounding boxes</span></span><br><span class="line">print(np.allclose(results[<span class="string">&#x27;boxes&#x27;</span>][<span class="number">0</span>], [<span class="number">0.28838485</span>, <span class="number">0.06830047</span>, <span class="number">0.7213766</span> , <span class="number">0.19833465</span>], rtol=<span class="number">0.18</span>))</span><br><span class="line">print(np.allclose(results[<span class="string">&#x27;boxes&#x27;</span>][<span class="number">5</span>], [<span class="number">0.29168868</span>, <span class="number">0.07529271</span>, <span class="number">0.72504973</span>, <span class="number">0.20099735</span>], rtol=<span class="number">0.18</span>))</span><br><span class="line">print(np.allclose(results[<span class="string">&#x27;boxes&#x27;</span>][<span class="number">10</span>], [<span class="number">0.29548776</span>, <span class="number">0.07994056</span>, <span class="number">0.7238164</span> , <span class="number">0.20778716</span>], rtol=<span class="number">0.18</span>))</span><br></pre></td></tr></table></figure>

<pre><code>237
(4,)
[0.28853202 0.06840048 0.72127116 0.19831184]
True
True
True
</code></pre>
<p><strong>Expected Output:</strong> Ideally the three boolean values at the bottom should be <code>True</code>. But if you only get two, you can still try submitting. This compares your resulting bounding boxes for each zombie image to some preloaded coordinates (i.e. the hardcoded values in the test cell above). Depending on how you annotated the training images,it’s possible that some of your results differ for these three frames but still get good results overall when all images are examined by the grader. If two or all are False, please try annotating the images again with a tighter bounding box or use the <a href="#gt-boxes">predefined <code>gt_boxes</code> list</a>.</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">237</span><br><span class="line">(4,)</span><br><span class="line"></span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br></pre></td></tr></table></figure>

<p>You can also check if the model detects a zombie class in the images by examining the <code>scores</code> key of the <code>results</code> dictionary. You should get higher than 88.0 here.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array(results[<span class="string">&#x27;scores&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># percent of frames where a zombie is detected</span></span><br><span class="line">zombie_detected = (np.where(x &gt; <span class="number">0.9</span>, <span class="number">1</span>, <span class="number">0</span>).<span class="built_in">sum</span>())/<span class="number">237</span>*<span class="number">100</span></span><br><span class="line">print(zombie_detected)</span><br></pre></td></tr></table></figure>

<pre><code>95.35864978902954
</code></pre>
<p>You can also display some still frames and inspect visually. If you don’t see a bounding box around the zombie, please consider re-annotating the ground truth or use the predefined <code>gt_boxes</code> <a href="#gt-boxes">here</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;Frame 0&#x27;</span>)</span><br><span class="line">display(IPyImage(<span class="string">&#x27;./results/gif_frame_000.jpg&#x27;</span>))</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">&#x27;Frame 5&#x27;</span>)</span><br><span class="line">display(IPyImage(<span class="string">&#x27;./results/gif_frame_005.jpg&#x27;</span>))</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">&#x27;Frame 10&#x27;</span>)</span><br><span class="line">display(IPyImage(<span class="string">&#x27;./results/gif_frame_010.jpg&#x27;</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Frame 0
</code></pre>
<p><img src="/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_131_1.jpg" alt="jpeg"></p>
<pre><code>Frame 5
</code></pre>
<p><img src="/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_131_3.jpg" alt="jpeg"></p>
<pre><code>Frame 10
</code></pre>
<p><img src="/Copy_of_C3W2_Assignment_files/Copy_of_C3W2_Assignment_131_5.jpg" alt="jpeg"></p>
<h2 id="Create-a-zip-of-the-zombie-walk-images"><a href="#Create-a-zip-of-the-zombie-walk-images" class="headerlink" title="Create a zip of the zombie-walk images."></a>Create a zip of the zombie-walk images.</h2><p>You can download this if you like to create your own animations</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">zipf = zipfile.ZipFile(<span class="string">&#x27;./zombie.zip&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, zipfile.ZIP_DEFLATED)</span><br><span class="line"></span><br><span class="line">filenames = glob.glob(<span class="string">&#x27;./results/gif_frame_*.jpg&#x27;</span>)</span><br><span class="line">filenames = <span class="built_in">sorted</span>(filenames)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">    zipf.write(filename)</span><br><span class="line"></span><br><span class="line">zipf.close()</span><br></pre></td></tr></table></figure>

<h2 id="Create-Zombie-animation"><a href="#Create-Zombie-animation" class="headerlink" title="Create Zombie animation"></a>Create Zombie animation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">imageio.plugins.freeimage.download()</span><br><span class="line"></span><br><span class="line">!rm -rf ./results/zombie-anim.gif</span><br><span class="line"></span><br><span class="line">anim_file = <span class="string">&#x27;./zombie-anim.gif&#x27;</span></span><br><span class="line"></span><br><span class="line">filenames = glob.glob(<span class="string">&#x27;./results/gif_frame_*.jpg&#x27;</span>)</span><br><span class="line">filenames = <span class="built_in">sorted</span>(filenames)</span><br><span class="line">last = -<span class="number">1</span></span><br><span class="line">images = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">    image = imageio.imread(filename)</span><br><span class="line">    images.append(image)</span><br><span class="line"></span><br><span class="line">imageio.mimsave(anim_file, images, <span class="string">&#x27;GIF-FI&#x27;</span>, fps=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Imageio: &#39;libfreeimage-3.16.0-linux64.so&#39; was not found on your computer; downloading it now.
Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/freeimage/libfreeimage-3.16.0-linux64.so (4.6 MB)
Downloading: 8192/4830080 bytes (0.2%)3825664/4830080 bytes (79.2%)4830080/4830080 bytes (100.0%)
  Done
File saved as /root/.imageio/freeimage/libfreeimage-3.16.0-linux64.so.
</code></pre>
<p>Unfortunately, using <code>IPyImage</code> in the notebook (as you’ve done in the rubber ducky detection tutorial) for the large <code>gif</code> generated will disconnect the runtime. To view the animation, you can instead use the <code>Files</code> pane on the left and double-click on <code>zombie-anim.gif</code>. That will open a preview page on the right. It will take 2 to 3 minutes to load and see the walking zombie.</p>
<h2 id="Save-results-file-for-grading"><a href="#Save-results-file-for-grading" class="headerlink" title="Save results file for grading"></a>Save results file for grading</h2><p>Run the cell below to save your results. Download the <code>results.data</code> file and upload it to the grader in the classroom.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove file if it exists</span></span><br><span class="line">!rm results.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># write results to binary file. upload for grading.</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;results.data&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> filehandle:</span><br><span class="line">    pickle.dump(results[<span class="string">&#x27;boxes&#x27;</span>], filehandle)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Done saving! Please download `results.data` from the Files tab\n&#x27;</span> \</span><br><span class="line">      <span class="string">&#x27;on the left and submit for grading.\nYou can also use the next cell as a shortcut for downloading.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>rm: cannot remove &#39;results.data&#39;: No such file or directory
Done saving! Please download `results.data` from the Files tab
on the left and submit for grading.
You can also use the next cell as a shortcut for downloading.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line"></span><br><span class="line">files.download(<span class="string">&#x27;results.data&#x27;</span>)</span><br></pre></td></tr></table></figure>


<pre><code>&lt;IPython.core.display.Javascript object&gt;



&lt;IPython.core.display.Javascript object&gt;
</code></pre>
<p><strong>Congratulations on completing this assignment! Please go back to the Coursera classroom and upload <code>results.data</code> to the Graded Lab item for Week 2.</strong></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Young Times
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2021/06/20/fine_tune_tf_2.0/" title="Fine Tune in Tensorflow 2.x">http://example.com/2021/06/20/fine_tune_tf_2.0/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" rel="tag"># 自动驾驶</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/19/fine_tune_in_tf2x/" rel="prev" title="Fine Tune in Tensorflow 2.x">
      <i class="fa fa-chevron-left"></i> Fine Tune in Tensorflow 2.x
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-2-Assignment-Zombie-Detection"><span class="nav-number">1.</span> <span class="nav-text">Week 2 Assignment: Zombie Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Exercises"><span class="nav-number">1.1.</span> <span class="nav-text">Exercises</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Installation"><span class="nav-number">1.2.</span> <span class="nav-text">Installation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Imports"><span class="nav-number">1.3.</span> <span class="nav-text">Imports</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-1-Import-Object-Detection-API-packages"><span class="nav-number">1.3.1.</span> <span class="nav-text">Exercise 1: Import Object Detection API packages</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Utilities"><span class="nav-number">1.4.</span> <span class="nav-text">Utilities</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Download-the-Zombie-data"><span class="nav-number">1.5.</span> <span class="nav-text">Download the Zombie data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-2-Visualize-the-training-images"><span class="nav-number">1.5.1.</span> <span class="nav-text">Exercise 2: Visualize the training images</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prepare-data-for-training-Optional"><span class="nav-number">1.6.</span> <span class="nav-text">Prepare data for training (Optional)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Option-1-draw-your-own-ground-truth-boxes"><span class="nav-number">1.6.0.1.</span> <span class="nav-text">Option 1: draw your own ground truth boxes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Option-2-use-the-given-ground-truth-boxes"><span class="nav-number">1.6.0.2.</span> <span class="nav-text">Option 2: use the given ground truth boxes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#View-your-ground-truth-box-coordinates"><span class="nav-number">1.6.0.3.</span> <span class="nav-text">View your ground truth box coordinates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-3-Define-the-category-index-dictionary"><span class="nav-number">1.6.1.</span> <span class="nav-text">Exercise 3: Define the category index dictionary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-preprocessing"><span class="nav-number">1.6.2.</span> <span class="nav-text">Data preprocessing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualize-the-zombies-with-their-ground-truth-bounding-boxes"><span class="nav-number">1.7.</span> <span class="nav-text">Visualize the zombies with their ground truth bounding boxes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Download-the-checkpoint-containing-the-pre-trained-weights"><span class="nav-number">1.8.</span> <span class="nav-text">Download the checkpoint containing the pre-trained weights</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-4-Download-checkpoints"><span class="nav-number">1.8.1.</span> <span class="nav-text">Exercise 4: Download checkpoints</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Configure-the-model"><span class="nav-number">1.9.</span> <span class="nav-text">Configure the model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-5-1-Locate-and-read-from-the-configuration-file"><span class="nav-number">1.9.1.</span> <span class="nav-text">Exercise 5.1: Locate and read from the configuration file</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pipeline-config"><span class="nav-number">1.9.1.1.</span> <span class="nav-text">pipeline_config</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#configs"><span class="nav-number">1.9.1.2.</span> <span class="nav-text">configs</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-5-2-Get-the-model-configuration"><span class="nav-number">1.9.2.</span> <span class="nav-text">Exercise 5.2: Get the model configuration</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#model-config"><span class="nav-number">1.9.2.1.</span> <span class="nav-text">model_config</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-5-3-Modify-model-config"><span class="nav-number">1.9.3.</span> <span class="nav-text">Exercise 5.3: Modify model_config</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Build-the-model"><span class="nav-number">1.10.</span> <span class="nav-text">Build the model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-5-4-Build-the-custom-model"><span class="nav-number">1.10.1.</span> <span class="nav-text">Exercise 5.4: Build the custom model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#model-builder"><span class="nav-number">1.10.1.1.</span> <span class="nav-text">model_builder</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Restore-weights-from-your-checkpoint"><span class="nav-number">1.11.</span> <span class="nav-text">Restore weights from your checkpoint</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspect-the-detection-model"><span class="nav-number">1.11.0.1.</span> <span class="nav-text">Inspect the detection_model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Find-the-source-code-for-detection-model"><span class="nav-number">1.11.0.2.</span> <span class="nav-text">Find the source code for detection_model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#View-the-variables-in-detection-model"><span class="nav-number">1.11.0.3.</span> <span class="nav-text">View the variables in detection_model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspect-feature-extractor"><span class="nav-number">1.11.0.4.</span> <span class="nav-text">Inspect _feature_extractor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspect-box-predictor"><span class="nav-number">1.11.0.5.</span> <span class="nav-text">Inspect _box_predictor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspect-box-predictor-1"><span class="nav-number">1.11.0.6.</span> <span class="nav-text">Inspect _box_predictor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#View-variables-in-box-predictor"><span class="nav-number">1.11.0.7.</span> <span class="nav-text">View variables in _box_predictor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspect-base-tower-layers-for-heads"><span class="nav-number">1.11.0.8.</span> <span class="nav-text">Inspect base_tower_layers_for_heads</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspect-box-prediction-head"><span class="nav-number">1.11.0.9.</span> <span class="nav-text">Inspect _box_prediction_head</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspect-prediction-heads"><span class="nav-number">1.11.0.10.</span> <span class="nav-text">Inspect _prediction_heads</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Which-layers-will-you-reuse"><span class="nav-number">1.11.0.11.</span> <span class="nav-text">Which layers will you reuse?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Define-checkpoints-for-desired-layers"><span class="nav-number">1.12.</span> <span class="nav-text">Define checkpoints for desired layers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-6-1-Define-Checkpoints-for-the-box-predictor"><span class="nav-number">1.12.1.</span> <span class="nav-text">Exercise 6.1: Define Checkpoints for the box predictor</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Expected-output"><span class="nav-number">1.12.1.1.</span> <span class="nav-text">Expected output</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-6-2-Define-the-temporary-model-checkpoint"><span class="nav-number">1.12.2.</span> <span class="nav-text">Exercise 6.2: Define the temporary model checkpoint**</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Expected-output-1"><span class="nav-number">1.12.2.1.</span> <span class="nav-text">Expected output</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-6-3-Restore-the-checkpoint"><span class="nav-number">1.12.3.</span> <span class="nav-text">Exercise 6.3: Restore the checkpoint</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-7-Run-a-dummy-image-to-generate-the-model-variables"><span class="nav-number">1.12.4.</span> <span class="nav-text">Exercise 7: Run a dummy image to generate the model variables</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Eager-mode-custom-training-loop"><span class="nav-number">1.13.</span> <span class="nav-text">Eager mode custom training loop</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-8-Set-training-hyperparameters"><span class="nav-number">1.13.1.</span> <span class="nav-text">Exercise 8: Set training hyperparameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Choose-the-layers-to-fine-tune"><span class="nav-number">1.14.</span> <span class="nav-text">Choose the layers to fine-tune</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-9-Select-the-prediction-layer-variables"><span class="nav-number">1.14.1.</span> <span class="nav-text">Exercise 9: Select the prediction layer variables</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-your-model"><span class="nav-number">1.15.</span> <span class="nav-text">Train your model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Make-a-prediction"><span class="nav-number">1.16.</span> <span class="nav-text">Make a prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Calculate-loss"><span class="nav-number">1.16.0.1.</span> <span class="nav-text">Calculate loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Provide-the-ground-truth"><span class="nav-number">1.16.0.2.</span> <span class="nav-text">Provide the ground truth</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-10-Define-the-training-step"><span class="nav-number">1.16.1.</span> <span class="nav-text">Exercise 10: Define the training step</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Run-the-training-loop"><span class="nav-number">1.17.</span> <span class="nav-text">Run the training loop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Load-test-images-and-run-inference-with-new-model"><span class="nav-number">1.18.</span> <span class="nav-text">Load test images and run inference with new model!</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exercise-11-Preprocess-predict-and-post-process-an-image"><span class="nav-number">1.18.1.</span> <span class="nav-text">Exercise 11: Preprocess, predict, and post process an image</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Create-a-zip-of-the-zombie-walk-images"><span class="nav-number">1.19.</span> <span class="nav-text">Create a zip of the zombie-walk images.</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Create-Zombie-animation"><span class="nav-number">1.20.</span> <span class="nav-text">Create Zombie animation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Save-results-file-for-grading"><span class="nav-number">1.21.</span> <span class="nav-text">Save results file for grading</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Young Times"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">Young Times</p>
  <div class="site-description" itemprop="description">路要一步一步的走</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">66</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Young Times</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">388k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:53</span>
</div>

<span id="busuanzi_container_site_uv">
  本站访问次数：<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
</span>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'hEvBR6pmxtoYkRhC3fnMB8Yq-gzGzoHsz',
      appKey     : 'nIkzvtLYb1avq32c9GYfgOOW',
      placeholder: "Please Leave Your Message Here...",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
